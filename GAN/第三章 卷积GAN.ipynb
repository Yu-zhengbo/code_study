{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96f2dbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97ceee35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "device = torch.device('cuda'if torch.cuda.is_available() else 'cpu')\n",
    "print(torch.cuda.memory_allocated(device)/1024/1024/1024)\n",
    "print(torch.cuda.memory_summary(device,abbreviated=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "340e7fce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 10])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class View(nn.Module):\n",
    "    def __init__(self,shape):\n",
    "        super(View,self).__init__()\n",
    "        self.shape = shape\n",
    "    def forward(self,x):\n",
    "        return x.reshape(*self.shape)\n",
    "    \n",
    "img = torch.rand(100)\n",
    "View([10,10])(img).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34602a5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 128, 3])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self,img_file):\n",
    "        self.img_file = os.listdir(img_file)\n",
    "        self.img_path = img_file\n",
    "    def __len__(self):\n",
    "        return len(self.img_file)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        if index>len(self.img_file):\n",
    "            raise IndexError()\n",
    "        img = plt.imread(self.img_path+'\\\\'+self.img_file[index])\n",
    "        img = self.crop_center(img)\n",
    "        return torch.cuda.FloatTensor(img)/255.\n",
    "    def crop_center(self,img,new_height=128,new_width=128):\n",
    "        height,width,_ = img.shape\n",
    "        dis_height = (height-new_height)//2\n",
    "        dis_width = (width-new_width)//2\n",
    "        return img[dis_height:dis_height+new_height,dis_width:dis_width+new_width,:]\n",
    "    \n",
    "    def plot_img(self,index):\n",
    "        if index>len(self.img_file):\n",
    "            raise IndexError()\n",
    "        img = plt.imread(self.img_path+'\\\\'+self.img_file[index])\n",
    "        img = self.crop_center(img)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "dataset = Dataset(r'E:\\\\img_align_celeba')\n",
    "dataset.plot_img(10000)\n",
    "dataset.__getitem__(10000).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c4ce519a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5694]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.5631455183029175\n",
      "tensor([[0.6731]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.1181104183197021\n",
      "tensor([[0.5621]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.5760483741760254\n",
      "tensor([[0.6493]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.0479117631912231\n",
      "tensor([[0.6067]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.4997318387031555\n",
      "tensor([[0.5160]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.7256025671958923\n",
      "tensor([[0.7052]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.3492523431777954\n",
      "tensor([[0.5713]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.8470149040222168\n",
      "tensor([[0.5930]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.5226002931594849\n",
      "tensor([[0.5421]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.7811988592147827\n",
      "tensor([[0.7307]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.3138048052787781\n",
      "tensor([[0.5168]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.727308452129364\n",
      "tensor([[0.8203]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.19811314344406128\n",
      "tensor([[0.5619]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.8252025842666626\n",
      "tensor([[0.7798]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.24872040748596191\n",
      "tensor([[0.4925]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.6782413125038147\n",
      "tensor([[0.8519]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.1602930724620819\n",
      "tensor([[0.4814]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.6567126512527466\n",
      "tensor([[0.8021]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.22046016156673431\n",
      "tensor([[0.5020]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.697068989276886\n",
      "tensor([[0.5641]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.5725531578063965\n",
      "tensor([[0.4905]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.6744107604026794\n",
      "tensor([[0.8758]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.13265113532543182\n",
      "tensor([[0.4804]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.6547642946243286\n",
      "tensor([[0.9134]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.09053818881511688\n",
      "tensor([[0.4411]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.581820011138916\n",
      "tensor([[0.8636]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.14666813611984253\n",
      "tensor([[0.3902]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.494637668132782\n",
      "tensor([[0.9313]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.07114510238170624\n",
      "tensor([[0.4399]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.5797051191329956\n",
      "tensor([[0.8390]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.1754983365535736\n",
      "tensor([[0.3632]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.4512273073196411\n",
      "tensor([[0.9114]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.09278508275747299\n",
      "tensor([[0.3937]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.5003497004508972\n",
      "tensor([[0.9698]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.030667385086417198\n",
      "tensor([[0.2591]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.29993656277656555\n",
      "tensor([[0.6156]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.48516079783439636\n",
      "tensor([[0.3473]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.42659610509872437\n",
      "tensor([[0.7716]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.25922927260398865\n",
      "tensor([[0.3536]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.4363262951374054\n",
      "tensor([[0.8253]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.1919959932565689\n",
      "tensor([[0.2470]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.2836855947971344\n",
      "tensor([[0.8574]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.15390509366989136\n",
      "tensor([[0.2800]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.32844609022140503\n",
      "tensor([[0.9327]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.06969623267650604\n",
      "tensor([[0.2105]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.23631399869918823\n",
      "tensor([[0.6601]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.4153594672679901\n",
      "tensor([[0.3034]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.36155006289482117\n",
      "tensor([[0.9340]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.06831423938274384\n",
      "tensor([[0.1984]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.22111226618289948\n",
      "tensor([[0.8918]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.1144876703619957\n",
      "tensor([[0.1940]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.21573100984096527\n",
      "tensor([[0.9502]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.051083896309137344\n",
      "tensor([[0.1726]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.189437597990036\n",
      "tensor([[0.8679]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.1416751891374588\n",
      "tensor([[0.2314]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.26316818594932556\n",
      "tensor([[0.9447]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.056848932057619095\n",
      "tensor([[0.1249]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.13342812657356262\n",
      "tensor([[0.9140]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0898929089307785\n",
      "tensor([[0.1244]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.13283027708530426\n",
      "tensor([[0.9516]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.049616873264312744\n",
      "tensor([[0.1271]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.13588367402553558\n",
      "tensor([[0.9596]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.04119648039340973\n",
      "tensor([[0.1186]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.12621323764324188\n",
      "tensor([[0.9561]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0448557510972023\n",
      "tensor([[0.0647]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.06691312789916992\n",
      "tensor([[0.9524]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.04878139868378639\n",
      "tensor([[0.1182]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.12576094269752502\n",
      "tensor([[0.9396]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.06224857643246651\n",
      "tensor([[0.0785]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.081730417907238\n",
      "tensor([[0.9764]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.02392205223441124\n",
      "tensor([[0.0565]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.058148257434368134\n",
      "tensor([[0.9940]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.006004742346704006\n",
      "tensor([[0.0573]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0589982345700264\n",
      "tensor([[0.2015]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.601859450340271\n",
      "tensor([[0.2718]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.31719040870666504\n",
      "tensor([[0.9976]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.002368033165112138\n",
      "tensor([[0.1853]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.2049427181482315\n",
      "tensor([[0.9918]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.008240285329520702\n",
      "tensor([[0.1465]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.15836572647094727\n",
      "tensor([[0.9935]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.006544141098856926\n",
      "tensor([[0.1130]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.1199435293674469\n",
      "tensor([[0.9934]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.006577019114047289\n",
      "tensor([[0.1113]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.11795523762702942\n",
      "tensor([[0.9829]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.017280980944633484\n",
      "tensor([[0.0809]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.08441107720136642\n",
      "tensor([[0.8508]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.16161149740219116\n",
      "tensor([[0.1351]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.14518660306930542\n",
      "tensor([[0.8537]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.15816351771354675\n",
      "tensor([[0.1096]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.11606494337320328\n",
      "tensor([[0.9591]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.041780371218919754\n",
      "tensor([[0.0752]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.07820075005292892\n",
      "tensor([[0.9934]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.006661380175501108\n",
      "tensor([[0.0735]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.07631489634513855\n",
      "tensor([[0.7741]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.2560993432998657\n",
      "tensor([[0.0654]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.06764908879995346\n",
      "tensor([[0.9854]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.014668109826743603\n",
      "tensor([[0.1093]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.11570727825164795\n",
      "tensor([[0.9290]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.07366102188825607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0807]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.08410345762968063\n",
      "tensor([[0.8604]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.1503276526927948\n",
      "tensor([[0.0854]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.08925222605466843\n",
      "tensor([[0.9971]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0029141406994313\n",
      "tensor([[0.0640]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.06612039357423782\n",
      "tensor([[0.9927]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.007332678884267807\n",
      "tensor([[0.0640]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.06612472236156464\n",
      "tensor([[0.9746]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.02577626332640648\n",
      "tensor([[0.0439]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.044931747019290924\n",
      "tensor([[0.9978]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.002249802928417921\n",
      "tensor([[0.0438]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.04475906863808632\n",
      "tensor([[0.0578]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.850184202194214\n",
      "tensor([[0.4542]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.6054500937461853\n",
      "tensor([[0.3608]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.0195529460906982\n",
      "tensor([[0.5022]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.6976219415664673\n",
      "tensor([[0.9703]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.03013429418206215\n",
      "tensor([[0.2694]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.31392213702201843\n",
      "tensor([[0.9985]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0015382671263068914\n",
      "tensor([[0.2355]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.26850423216819763\n",
      "tensor([[0.9895]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.010581671260297298\n",
      "tensor([[0.2192]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.2474193125963211\n",
      "tensor([[0.9866]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.013466806150972843\n",
      "tensor([[0.1528]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.16584785282611847\n",
      "tensor([[0.9522]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.049014803022146225\n",
      "tensor([[0.1401]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.15093591809272766\n",
      "tensor([[0.9551]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.045946285128593445\n",
      "tensor([[0.0660]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.06824462115764618\n",
      "tensor([[0.9867]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.013412012718617916\n",
      "tensor([[0.0853]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0891735702753067\n",
      "tensor([[0.9803]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.019893642514944077\n",
      "tensor([[0.0472]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.048361171036958694\n",
      "tensor([[0.9856]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.014509650878608227\n",
      "tensor([[0.0483]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.04951421543955803\n",
      "tensor([[0.9870]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0130594652146101\n",
      "tensor([[0.0642]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.06635749340057373\n",
      "tensor([[0.9992]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0008320692577399313\n",
      "tensor([[0.0367]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.037365712225437164\n",
      "tensor([[0.9981]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.001896151341497898\n",
      "tensor([[0.0433]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.044303830713033676\n",
      "tensor([[0.9975]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0025295394007116556\n",
      "tensor([[0.0518]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.053204864263534546\n",
      "tensor([[0.9954]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.004646350163966417\n",
      "tensor([[0.0321]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.03259850665926933\n",
      "tensor([[0.9590]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.04191629961133003\n",
      "tensor([[0.0354]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.03599200397729874\n",
      "tensor([[0.9985]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0015443561132997274\n",
      "tensor([[0.0324]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.032968975603580475\n",
      "tensor([[0.9864]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.013700026087462902\n",
      "tensor([[0.0285]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.028924399986863136\n",
      "tensor([[0.9265]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.07637833058834076\n",
      "tensor([[0.0264]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.026805276051163673\n",
      "tensor([[0.8842]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.12305773049592972\n",
      "tensor([[0.0318]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.032329194247722626\n",
      "tensor([[0.9796]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.020562205463647842\n",
      "tensor([[0.0345]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.03507380560040474\n",
      "tensor([[0.9624]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.03831636160612106\n",
      "tensor([[0.0362]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.03692173957824707\n",
      "tensor([[0.9992]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0007775467820465565\n",
      "tensor([[0.0259]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.026220208033919334\n",
      "tensor([[0.9909]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.009120926260948181\n",
      "tensor([[0.0365]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.03718029335141182\n",
      "tensor([[0.9993]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0007101434166543186\n",
      "tensor([[0.0340]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.03464214503765106\n",
      "tensor([[0.9994]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0006488878279924393\n",
      "tensor([[0.0231]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.02332945540547371\n",
      "tensor([[0.9636]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.037100307643413544\n",
      "tensor([[0.0263]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.02662394754588604\n",
      "tensor([[0.9997]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.00032489807927049696\n",
      "tensor([[0.0247]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.024959424510598183\n",
      "tensor([[0.9926]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.007426050957292318\n",
      "tensor([[0.0246]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.024944575503468513\n",
      "tensor([[0.9998]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.00021108232613187283\n",
      "tensor([[0.0179]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.018058067187666893\n",
      "tensor([[0.9935]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.006557040382176638\n",
      "tensor([[0.0184]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.01860097050666809\n",
      "tensor([[0.9958]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.004168364685028791\n",
      "tensor([[0.0244]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.024749042466282845\n",
      "tensor([[0.9995]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0004872078134212643\n",
      "tensor([[0.0181]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.018225161358714104\n",
      "tensor([[0.9945]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.005543548613786697\n",
      "tensor([[0.0194]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.01964019052684307\n",
      "tensor([[0.9990]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.001020773546770215\n",
      "tensor([[0.0188]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.019025519490242004\n",
      "tensor([[0.9693]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.03122345358133316\n",
      "tensor([[0.0157]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.015810664743185043\n",
      "tensor([[0.9956]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.004415889736264944\n",
      "tensor([[0.0185]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.018714774399995804\n",
      "tensor([[0.9996]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0004293051897548139\n",
      "tensor([[0.0163]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.016483228653669357\n",
      "tensor([[0.9913]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.008764355443418026\n",
      "tensor([[0.0160]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.01615200936794281\n",
      "tensor([[0.9879]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.01213474664837122\n",
      "tensor([[0.0118]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.011846761219203472\n",
      "tensor([[0.9782]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.02208242565393448\n",
      "tensor([[0.0157]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.015802066773176193\n",
      "tensor([[0.9965]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.003554335329681635\n",
      "tensor([[0.0198]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.020003093406558037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9373]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.06479159742593765\n",
      "tensor([[0.0157]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.015868617221713066\n",
      "tensor([[0.9806]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.019568463787436485\n",
      "tensor([[0.0168]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.01698235422372818\n",
      "tensor([[0.9995]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0004905473324470222\n",
      "tensor([[0.0164]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.016521403566002846\n",
      "tensor([[0.9878]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.012318233959376812\n",
      "tensor([[0.0135]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.01354413665831089\n",
      "tensor([[0.9784]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.021843647584319115\n",
      "tensor([[0.0200]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.02021114155650139\n",
      "tensor([[0.7510]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.28638210892677307\n",
      "tensor([[0.0510]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.05232160910964012\n",
      "tensor([[0.9994]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0006213926244527102\n",
      "tensor([[0.0271]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.027436694130301476\n",
      "tensor([[0.9987]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0012837540125474334\n",
      "tensor([[0.0429]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.04386785253882408\n",
      "tensor([[0.9997]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0002571078948676586\n",
      "tensor([[0.0248]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.025073647499084473\n",
      "tensor([[0.9819]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.018277669325470924\n",
      "tensor([[0.0254]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.025746356695890427\n",
      "tensor([[0.9985]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0015106877544894814\n",
      "tensor([[0.0183]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.018499689176678658\n",
      "tensor([[0.9996]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.00043502970947884023\n",
      "tensor([[0.0188]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.01898348331451416\n",
      "tensor([[0.9982]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0018400779226794839\n",
      "tensor([[0.0203]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.02053099311888218\n",
      "tensor([[0.9994]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0006061244639568031\n",
      "tensor([[0.0201]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.020287232473492622\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.837221290974412e-05\n",
      "tensor([[0.0268]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.02711579203605652\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.527268952690065e-05\n",
      "tensor([[0.0159]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.016026686877012253\n",
      "tensor([[0.9952]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.004791215527802706\n",
      "tensor([[0.0140]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.014066158793866634\n",
      "tensor([[0.9998]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.00023814891756046563\n",
      "tensor([[0.0150]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.015079494565725327\n",
      "tensor([[0.9900]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.010053938254714012\n",
      "tensor([[0.0179]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.018013034015893936\n",
      "tensor([[0.9995]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0005412970203906298\n",
      "tensor([[0.0132]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.01332877203822136\n",
      "tensor([[0.9989]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0010518000926822424\n",
      "tensor([[0.0146]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.014737307094037533\n",
      "tensor([[0.9987]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.001280232798308134\n",
      "tensor([[0.0143]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.014409931376576424\n",
      "tensor([[0.9982]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0018221040954813361\n",
      "tensor([[0.0135]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.013604918494820595\n",
      "tensor([[0.9704]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.030063346028327942\n",
      "tensor([[0.0126]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.012660801410675049\n",
      "tensor([[0.9897]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.010333457961678505\n",
      "tensor([[0.0104]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.010504869744181633\n",
      "tensor([[0.7769]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.25240445137023926\n",
      "tensor([[0.0232]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.023437757045030594\n",
      "tensor([[0.9998]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.00019069343397859484\n",
      "tensor([[0.0205]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.020747002214193344\n",
      "tensor([[0.9958]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.004179796669632196\n",
      "tensor([[0.0169]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.01702703721821308\n",
      "tensor([[0.9977]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0022669481113553047\n",
      "tensor([[0.0249]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.02523183450102806\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.22009798057843e-05\n",
      "tensor([[0.0260]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.026364866644144058\n",
      "tensor([[0.9990]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0009827673202380538\n",
      "tensor([[0.0158]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.01591361314058304\n",
      "tensor([[0.9960]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.003962907940149307\n",
      "tensor([[0.0154]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.015497345477342606\n",
      "tensor([[0.9982]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0018071160884574056\n",
      "tensor([[0.0136]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.013724801130592823\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.7060874344897456e-05\n",
      "tensor([[0.0134]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.013499791733920574\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.3351529560168274e-05\n",
      "tensor([[0.0123]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.012398493476212025\n",
      "tensor([[0.9654]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0352361761033535\n",
      "tensor([[0.0166]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.01669394038617611\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.4570753086882178e-06\n",
      "tensor([[0.0097]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.009744041599333286\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 9.549120295559987e-05\n",
      "tensor([[0.0122]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.012292527593672276\n",
      "tensor([[0.9998]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.00024017595569603145\n",
      "tensor([[0.0126]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.012655549682676792\n",
      "tensor([[0.9943]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.005723972339183092\n",
      "tensor([[0.0132]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.013295852579176426\n",
      "tensor([[0.9998]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.00022240966791287065\n",
      "tensor([[0.0147]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.014788362197577953\n",
      "tensor([[0.9985]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0014646044000983238\n",
      "tensor([[0.0167]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.016840016469359398\n",
      "tensor([[0.8829]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.12448865175247192\n",
      "tensor([[0.0217]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.02197926677763462\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.290230597485788e-05\n",
      "tensor([[0.0169]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.017024248838424683\n",
      "tensor([[0.9998]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0001964165858225897\n",
      "tensor([[0.0146]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.014679481275379658\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 9.155692532658577e-05\n",
      "tensor([[0.0207]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.020914791151881218\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.83861297450494e-05\n",
      "tensor([[0.0147]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.01485968567430973\n",
      "tensor([[0.9981]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0018977040890604258\n",
      "tensor([[0.0138]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.013859989121556282\n",
      "tensor([[0.6579]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.41875582933425903\n",
      "tensor([[0.0682]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.07062050700187683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.622607780722319e-06\n",
      "tensor([[0.0351]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.03568495810031891\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 5.92487704125233e-05\n",
      "tensor([[0.0261]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.026475945487618446\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.3589951777248643e-05\n",
      "tensor([[0.0300]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.030481114983558655\n",
      "tensor([[0.9350]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.06726045161485672\n",
      "tensor([[0.0336]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.03419654443860054\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.099489549640566e-05\n",
      "tensor([[0.0241]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.024412386119365692\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 7.438936154358089e-05\n",
      "tensor([[0.0196]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.019779037684202194\n",
      "tensor([[0.2465]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.4002164602279663\n",
      "tensor([[0.2544]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.2936255931854248\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 6.0679369198624045e-05\n",
      "tensor([[0.1271]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.13589780032634735\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.0132840543519706e-05\n",
      "tensor([[0.0701]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.07272079586982727\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.1604907892178744e-05\n",
      "tensor([[0.0613]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.06324325501918793\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.00011575892131077126\n",
      "tensor([[0.0564]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.05807042866945267\n",
      "tensor([[0.9998]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.00015152648848015815\n",
      "tensor([[0.0552]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.05676786229014397\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 5.4480129620060325e-05\n",
      "tensor([[0.0370]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.037752870470285416\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.0013630344474223e-05\n",
      "tensor([[0.0321]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.03261285275220871\n",
      "tensor([[0.8291]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.18741931021213531\n",
      "tensor([[0.0484]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.049657464027404785\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 9.763717389432713e-05\n",
      "tensor([[0.0395]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.04034721851348877\n",
      "tensor([[0.9850]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.015102004632353783\n",
      "tensor([[0.0316]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.032059889286756516\n",
      "tensor([[0.9989]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.001124954898841679\n",
      "tensor([[0.0212]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.021377651020884514\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0001149243616964668\n",
      "tensor([[0.0374]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.038147732615470886\n",
      "tensor([[0.9965]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0034634775947779417\n",
      "tensor([[0.0305]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.030973752960562706\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.694166323635727e-05\n",
      "tensor([[0.0202]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.020444419234991074\n",
      "tensor([[0.9817]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.018518997356295586\n",
      "tensor([[0.0309]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.031418412923812866\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.5181343011790887e-05\n",
      "tensor([[0.0158]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.01596357673406601\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.887592695013154e-06\n",
      "tensor([[0.0131]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.013184476643800735\n",
      "tensor([[0.9991]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0008855806081555784\n",
      "tensor([[0.0168]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0169379785656929\n",
      "tensor([[0.9995]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.00048005179269239306\n",
      "tensor([[0.0171]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.01723167859017849\n",
      "tensor([[0.9998]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.00018043954332824796\n",
      "tensor([[0.0092]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.009251401759684086\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 9.310679160989821e-05\n",
      "tensor([[0.0176]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.017712371423840523\n",
      "tensor([[0.9966]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0033592914696782827\n",
      "tensor([[0.0235]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.023827677592635155\n",
      "tensor([[0.9986]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.001364267198368907\n",
      "tensor([[0.0089]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.008923530578613281\n",
      "tensor([[0.9731]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.027284836396574974\n",
      "tensor([[0.0142]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.014296858571469784\n",
      "tensor([[0.9990]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0009640332427807152\n",
      "tensor([[0.0120]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.012114233337342739\n",
      "tensor([[0.9996]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0004033067380078137\n",
      "tensor([[0.0109]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.010942927561700344\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.826691317954101e-05\n",
      "tensor([[0.0160]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.01614546775817871\n",
      "tensor([[0.9998]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.00015021498256828636\n",
      "tensor([[0.0159]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.016001611948013306\n",
      "tensor([[0.9660]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.03464183583855629\n",
      "tensor([[0.0175]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.017650913447141647\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.041276406496763e-05\n",
      "tensor([[0.0168]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.016914336010813713\n",
      "tensor([[0.1692]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.7766181230545044\n",
      "tensor([[0.5950]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.9037689566612244\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920935776288388e-06\n",
      "tensor([[0.0626]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0646614283323288\n",
      "tensor([[0.6547]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.423648476600647\n",
      "tensor([[0.1768]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.19458803534507751\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.515347659937106e-05\n",
      "tensor([[0.1100]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.11651072651147842\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 5.364432581700385e-06\n",
      "tensor([[0.0975]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.10257809609174728\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 5.519542537513189e-05\n",
      "tensor([[0.0422]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.04306909069418907\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.337865791763761e-06\n",
      "tensor([[0.0449]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.045945536345243454\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.6689314179529902e-06\n",
      "tensor([[0.0664]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.06865624338388443\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 5.483642325998517e-06\n",
      "tensor([[0.0358]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.03649997338652611\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 8.344653679159819e-07\n",
      "tensor([[0.0537]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.05522432550787926\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.671713420771994e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0300]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.03050944395363331\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.6703237381298095e-05\n",
      "tensor([[0.0387]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.039467550814151764\n",
      "tensor([[0.9998]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0001918857597047463\n",
      "tensor([[0.0202]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.02043030597269535\n",
      "tensor([[0.9833]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.01680067740380764\n",
      "tensor([[0.0297]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.030175205320119858\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.622607780722319e-06\n",
      "tensor([[0.0240]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.02432120218873024\n",
      "tensor([[0.7955]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.22876107692718506\n",
      "tensor([[0.0479]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.04909593611955643\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.099446303167497e-06\n",
      "tensor([[0.0384]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.03917251527309418\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.4305124977909145e-06\n",
      "tensor([[0.0520]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.053374793380498886\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 7.152560215217818e-07\n",
      "tensor([[0.0274]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.02780335769057274\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.0490472959645558e-05\n",
      "tensor([[0.0328]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.03335714712738991\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 8.249623351730406e-05\n",
      "tensor([[0.0375]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.038257960230112076\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0153]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.015459935180842876\n",
      "tensor([[0.9992]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0007606060244143009\n",
      "tensor([[0.0165]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.016675759106874466\n",
      "tensor([[0.9998]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.00016523772501386702\n",
      "tensor([[0.0154]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.015483543276786804\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.360371763643343e-05\n",
      "tensor([[0.0238]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.024101855233311653\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 5.984485324006528e-05\n",
      "tensor([[0.0210]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.021214228123426437\n",
      "tensor([[0.9997]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0002893031924031675\n",
      "tensor([[0.0143]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.014451837167143822\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.00010884400398936123\n",
      "tensor([[0.0205]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.020674405619502068\n",
      "tensor([[0.9980]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.001982268178835511\n",
      "tensor([[0.0170]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.017101310193538666\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.9073504518019035e-06\n",
      "tensor([[0.0148]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.014889148063957691\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.9669725588755682e-05\n",
      "tensor([[0.0179]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.018097274005413055\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 7.677372923353687e-05\n",
      "tensor([[0.0125]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.01261057984083891\n",
      "tensor([[0.9998]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.00016488003893755376\n",
      "tensor([[0.0103]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.010311777703464031\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.0728893357736524e-05\n",
      "tensor([[0.0087]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.008703446015715599\n",
      "tensor([[0.9944]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.005658692214637995\n",
      "tensor([[0.0093]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.009341161698102951\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.695494797284482e-06\n",
      "tensor([[0.0095]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.009560062550008297\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.576279254957626e-07\n",
      "tensor([[0.0086]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.008597810752689838\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.2649790025752736e-06\n",
      "tensor([[0.0113]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.011375329457223415\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.768372718899627e-07\n",
      "tensor([[0.0079]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.007980095222592354\n",
      "tensor([[0.9998]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0002178787108277902\n",
      "tensor([[0.0084]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.008409835398197174\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 5.960466182841628e-07\n",
      "tensor([[0.0095]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0095613868907094\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.9073504518019035e-06\n",
      "tensor([[0.0052]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005164824891835451\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 9.417578439752106e-06\n",
      "tensor([[0.0076]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.007597384974360466\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.145769485650817e-06\n",
      "tensor([[0.0051]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005066572222858667\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 8.619203435955569e-05\n",
      "tensor([[0.0134]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.013476411812007427\n",
      "tensor([[0.9932]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.006781273055821657\n",
      "tensor([[0.0092]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.009255371987819672\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.9445127438521013e-05\n",
      "tensor([[0.0064]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.006405739579349756\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.00012064707698300481\n",
      "tensor([[0.0078]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.007812419906258583\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 6.830925849499181e-05\n",
      "tensor([[0.0038]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003842395031824708\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.180178969661938e-05\n",
      "tensor([[0.0039]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003948845434933901\n",
      "tensor([[0.9995]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0005358104826882482\n",
      "tensor([[0.0053]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005333854816854\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0077]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.007770070340484381\n",
      "tensor([[0.9079]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.09658849984407425\n",
      "tensor([[0.0123]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.012363794259727001\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.895445711852517e-05\n",
      "tensor([[0.0094]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.009487010538578033\n",
      "tensor([[0.9823]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.017890755087137222\n",
      "tensor([[0.0057]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005759401246905327\n",
      "tensor([[0.9943]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.005717617925256491\n",
      "tensor([[0.0072]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.007226107642054558\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.5167358873877674e-05\n",
      "tensor([[0.0073]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.007283264771103859\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.2649790025752736e-06\n",
      "tensor([[0.0116]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.011647138744592667\n",
      "tensor([[0.9991]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0009109353413805366\n",
      "tensor([[0.0096]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.009671156294643879\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.00010800945165101439\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0058]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005784100852906704\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0107]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.010759692639112473\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 7.152560215217818e-07\n",
      "tensor([[0.0113]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.011369542218744755\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.5644214221974835e-05\n",
      "tensor([[0.0106]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.010686310939490795\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.3113030945532955e-06\n",
      "tensor([[0.0080]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.007996438071131706\n",
      "tensor([[0.9996]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0003533987619448453\n",
      "tensor([[0.0146]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.014692909084260464\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 9.536747711536009e-07\n",
      "tensor([[0.0052]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0051811812445521355\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 9.89442014542874e-06\n",
      "tensor([[0.0067]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.006719644647091627\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.00011969328625127673\n",
      "tensor([[0.0048]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004815112333744764\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.576279254957626e-07\n",
      "tensor([[0.0051]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0051155174151062965\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.180178969661938e-05\n",
      "tensor([[0.0041]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0040999543853104115\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.923464803141542e-05\n",
      "tensor([[0.0080]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.008065596222877502\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.00013483480142895132\n",
      "tensor([[0.0053]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005330139305442572\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.768372718899627e-07\n",
      "tensor([[0.0066]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.006654179655015469\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.3470740668708459e-05\n",
      "tensor([[0.0037]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003719204105436802\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0001100362278521061\n",
      "tensor([[0.0036]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003626118181273341\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.157711423933506e-05\n",
      "tensor([[0.0076]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.007645013276487589\n",
      "tensor([[0.9917]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.008331517688930035\n",
      "tensor([[0.0059]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005962229333817959\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.5497220147153712e-06\n",
      "tensor([[0.0044]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0043768566101789474\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 7.152560215217818e-07\n",
      "tensor([[0.0042]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004191707819700241\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 6.890534132253379e-05\n",
      "tensor([[0.0073]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.007363601587712765\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.00013400021998677403\n",
      "tensor([[0.0031]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003109874902293086\n",
      "tensor([[0.9988]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0012216278119012713\n",
      "tensor([[0.0044]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004427564330399036\n",
      "tensor([[0.9990]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0009917764691635966\n",
      "tensor([[0.0026]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0026103921700268984\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920935776288388e-06\n",
      "tensor([[0.0044]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004369253292679787\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 9.655998837843072e-06\n",
      "tensor([[0.0060]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.006008340511471033\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 6.318112355074845e-06\n",
      "tensor([[0.0049]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004864165559411049\n",
      "tensor([[0.9971]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0029278898146003485\n",
      "tensor([[0.0103]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.010325147770345211\n",
      "tensor([[0.9994]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0005760659696534276\n",
      "tensor([[0.0042]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004167885519564152\n",
      "tensor([[0.9995]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0004956758348271251\n",
      "tensor([[0.0063]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00631672190502286\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.768372718899627e-07\n",
      "tensor([[0.0036]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0035682725720107555\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 5.006802894058637e-06\n",
      "tensor([[0.0045]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004493302665650845\n",
      "tensor([[0.9960]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.00400336179882288\n",
      "tensor([[0.0036]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0035911831073462963\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.5378116586362012e-05\n",
      "tensor([[0.0032]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0032387312967330217\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 6.318112355074845e-06\n",
      "tensor([[0.0042]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004201643634587526\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.4305124977909145e-06\n",
      "tensor([[0.0040]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00398648576810956\n",
      "tensor([[0.9989]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0011425582924857736\n",
      "tensor([[0.0029]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0028741497080773115\n",
      "tensor([[0.9950]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.004978511948138475\n",
      "tensor([[0.0034]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0034052827395498753\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.00010681722778826952\n",
      "tensor([[0.0033]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003299727337434888\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.768382950715022e-06\n",
      "tensor([[0.0032]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0031589041464030743\n",
      "tensor([[0.0605]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.805077075958252\n",
      "tensor([[0.2841]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.33423733711242676\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 7.391003236989491e-06\n",
      "tensor([[0.1756]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.19306877255439758\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0960]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.10095586627721786\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.9073504518019035e-06\n",
      "tensor([[0.0981]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.10324060171842575\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 8.702316335984506e-06\n",
      "tensor([[0.0535]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.05497189983725548\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.768372718899627e-07\n",
      "tensor([[0.0816]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.08511337637901306\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.0728841743912199e-06\n",
      "tensor([[0.0652]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0674276202917099\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.180178969661938e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0262]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.02651585079729557\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.0609683158691041e-05\n",
      "tensor([[0.0417]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.042586006224155426\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0373]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0379708968102932\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 9.155692532658577e-05\n",
      "tensor([[0.0346]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.035209257155656815\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.707477662828751e-05\n",
      "tensor([[0.0313]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.031754910945892334\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.00013113881868775934\n",
      "tensor([[0.0181]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.018216785043478012\n",
      "tensor([[0.9931]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.006937611848115921\n",
      "tensor([[0.0254]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.025737732648849487\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.8477610865375027e-05\n",
      "tensor([[0.0300]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.03042316995561123\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.708877895609476e-05\n",
      "tensor([[0.0188]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.018949586898088455\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0156]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.015673821792006493\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841887468734058e-06\n",
      "tensor([[0.0158]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.01595110073685646\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.4782061043661088e-05\n",
      "tensor([[0.0182]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.01838870346546173\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.887592695013154e-06\n",
      "tensor([[0.0289]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.029274363070726395\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.1352534278994426e-05\n",
      "tensor([[0.0125]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.012563137337565422\n",
      "tensor([[0.9978]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0021673664450645447\n",
      "tensor([[0.0204]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.02058873325586319\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.7285496141994372e-05\n",
      "tensor([[0.0119]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.011994966305792332\n",
      "tensor([[0.9894]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.010617212392389774\n",
      "tensor([[0.0112]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.011270376853644848\n",
      "tensor([[0.9997]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.00027320539811626077\n",
      "tensor([[0.0150]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.015075440518558025\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.2649790025752736e-06\n",
      "tensor([[0.0083]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.008360848762094975\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 5.1260126383567695e-06\n",
      "tensor([[0.0126]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.012700160034000874\n",
      "tensor([[0.9976]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.002379325218498707\n",
      "tensor([[0.0091]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.009141677990555763\n",
      "tensor([[0.7908]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.23475205898284912\n",
      "tensor([[0.0273]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.027702854946255684\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.00010526733967708424\n",
      "tensor([[0.0255]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.025865867733955383\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 9.536747711536009e-07\n",
      "tensor([[0.0282]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.028569776564836502\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0417]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.042625878006219864\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 7.152560215217818e-07\n",
      "tensor([[0.0097]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.009711780585348606\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0120]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.01205891091376543\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0190]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.019171573221683502\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 5.173817044124007e-05\n",
      "tensor([[0.0201]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.020286928862333298\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 7.987054232216906e-06\n",
      "tensor([[0.0103]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.01034670788794756\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 7.867844033171423e-06\n",
      "tensor([[0.0111]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.011198103427886963\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0148]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.014906631782650948\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0098]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.009861896745860577\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 7.867844033171423e-06\n",
      "tensor([[0.0130]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.01305523794144392\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.576279254957626e-07\n",
      "tensor([[0.0103]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.010313523933291435\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0089]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.008904285728931427\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.099446303167497e-06\n",
      "tensor([[0.0124]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.01244272943586111\n",
      "tensor([[0.6481]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.43367254734039307\n",
      "tensor([[0.0346]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.03523401543498039\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 6.198902156029362e-06\n",
      "tensor([[0.0477]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.048914022743701935\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 9.536747711536009e-07\n",
      "tensor([[0.0289]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.02932947874069214\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 9.536747711536009e-07\n",
      "tensor([[0.0266]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.02691640332341194\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.9073504518019035e-06\n",
      "tensor([[0.0222]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.02245633490383625\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 7.152582838898525e-06\n",
      "tensor([[0.0181]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.018284104764461517\n",
      "tensor([[0.9996]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0003561415651347488\n",
      "tensor([[0.0359]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.036562785506248474\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0156]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.015680240467190742\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.585496102052275e-05\n",
      "tensor([[0.0219]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.022163351997733116\n",
      "tensor([[0.9997]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.000276424951152876\n",
      "tensor([[0.0213]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.021551696583628654\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 7.152560215217818e-07\n",
      "tensor([[0.0108]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.010872726328670979\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.768372718899627e-07\n",
      "tensor([[0.0216]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.021854126825928688\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.576279254957626e-07\n",
      "tensor([[0.0084]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00845792330801487\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0138]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.013920004479587078\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.0728841743912199e-06\n",
      "tensor([[0.0137]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.013816959224641323\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.2278632311790716e-05\n",
      "tensor([[0.0113]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.011333552189171314\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 8.821526535029989e-06\n",
      "tensor([[0.0103]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.010383746586740017\n",
      "tensor([[0.9975]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0024917148984968662\n",
      "tensor([[0.0123]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.012374958023428917\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0118]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.011919382959604263\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.0265599687263602e-06\n",
      "tensor([[0.0156]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.015689928084611893\n",
      "tensor([[0.9990]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0009689255384728312\n",
      "tensor([[0.0114]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.01149615179747343\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0040]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0039690718986094\n",
      "tensor([[0.9923]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.007735771127045155\n",
      "tensor([[0.0070]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.007032387424260378\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0044]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004446004051715136\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.337865791763761e-06\n",
      "tensor([[0.0069]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.006883596070110798\n",
      "tensor([[0.9998]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0001806780055630952\n",
      "tensor([[0.0069]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.006935811601579189\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.768372718899627e-07\n",
      "tensor([[0.0055]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005532100796699524\n",
      "tensor([[0.9826]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.017540568485856056\n",
      "tensor([[0.0071]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.007172436453402042\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 9.417578439752106e-06\n",
      "tensor([[0.0043]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004292808007448912\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.0728841743912199e-06\n",
      "tensor([[0.0117]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.011734521016478539\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0082]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00823121052235365\n",
      "tensor([[0.9995]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0005121350404806435\n",
      "tensor([[0.0068]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00685970950871706\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.120573597290786e-05\n",
      "tensor([[0.0042]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004230913706123829\n",
      "tensor([[0.9998]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.00015820324188098311\n",
      "tensor([[0.0085]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.008555127307772636\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 9.655998837843072e-06\n",
      "tensor([[0.0059]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005954854190349579\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.768372718899627e-07\n",
      "tensor([[0.0064]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.006466749124228954\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0055]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0054917652159929276\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.933984044124372e-05\n",
      "tensor([[0.0031]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003138933563604951\n",
      "tensor([[0.9849]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.015178857371211052\n",
      "tensor([[0.0074]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0074159023351967335\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.0160405003698543e-05\n",
      "tensor([[0.0050]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004991630557924509\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0080]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.008048291318118572\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0050]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005044646095484495\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.218656047465629e-06\n",
      "tensor([[0.0059]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005926613230258226\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.576279254957626e-07\n",
      "tensor([[0.0044]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004438281059265137\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0085]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.008559035137295723\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.0728841743912199e-06\n",
      "tensor([[0.0037]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003688154509291053\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 7.033372639853042e-06\n",
      "tensor([[0.0046]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004580661188811064\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 7.271793037944008e-06\n",
      "tensor([[0.0082]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.008223878219723701\n",
      "tensor([[0.9991]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0009329497115686536\n",
      "tensor([[0.0055]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005523830186575651\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0044]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004391104448586702\n",
      "tensor([[0.9995]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0005361682851798832\n",
      "tensor([[0.0039]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00386453396640718\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.00012386612070258707\n",
      "tensor([[0.0040]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00403878977522254\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.9550514480215497e-05\n",
      "tensor([[0.0044]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00445121293887496\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 8.344653679159819e-07\n",
      "tensor([[0.0032]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0031956180464476347\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 8.344653679159819e-07\n",
      "tensor([[0.0056]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005617392715066671\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0029]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0029169502668082714\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.0265599687263602e-06\n",
      "tensor([[0.0046]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004647607449442148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 8.309232362080365e-05\n",
      "tensor([[0.0033]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003290039487183094\n",
      "tensor([[0.9988]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0012242536759003997\n",
      "tensor([[0.0041]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004068773239850998\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.529963462118758e-06\n",
      "tensor([[0.0032]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003207517322152853\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0023]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002305481117218733\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.576279254957626e-07\n",
      "tensor([[0.0033]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0032734747510403395\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 6.437322554120328e-06\n",
      "tensor([[0.0034]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003431658260524273\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.337865791763761e-06\n",
      "tensor([[0.0023]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0023133072536438704\n",
      "tensor([[0.9976]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0024164284113794565\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00176072062458843\n",
      "tensor([[0.9998]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.000212513143196702\n",
      "tensor([[0.0025]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002540235873311758\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.4466054532676935e-05\n",
      "tensor([[0.0027]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002704818034544587\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.0728841743912199e-06\n",
      "tensor([[0.0026]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0026129616890102625\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.6464813345228322e-05\n",
      "tensor([[0.0041]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004114916548132896\n",
      "tensor([[0.9973]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0026558111421763897\n",
      "tensor([[0.0040]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003975115716457367\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.291543518775143e-06\n",
      "tensor([[0.0031]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0030833883211016655\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.576279254957626e-07\n",
      "tensor([[0.0032]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0032473423052579165\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0020]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002027957234531641\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.1100266167195514e-05\n",
      "tensor([[0.0045]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004515336360782385\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.0531240301788785e-06\n",
      "tensor([[0.0021]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002137320814654231\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.4570753086882178e-06\n",
      "tensor([[0.0040]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004053512122482061\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0032]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0032114041969180107\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 8.702316335984506e-06\n",
      "tensor([[0.0032]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003184615634381771\n",
      "tensor([[0.9957]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.004263596143573523\n",
      "tensor([[0.0027]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0027081051375716925\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.218656047465629e-06\n",
      "tensor([[0.0034]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0034478669986128807\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 8.344653679159819e-07\n",
      "tensor([[0.0027]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002695673843845725\n",
      "tensor([[0.9997]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0002866798313334584\n",
      "tensor([[0.0030]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0029824699740856886\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.768382950715022e-06\n",
      "tensor([[0.0022]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0021928134374320507\n",
      "tensor([[0.9950]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.00500636687502265\n",
      "tensor([[0.0023]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0022772829979658127\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.7881409348774469e-06\n",
      "tensor([[0.0024]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0024137396831065416\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 5.960466182841628e-07\n",
      "tensor([[0.0032]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003252963535487652\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0029]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0029088803566992283\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0023]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0023159957490861416\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.9073504518019035e-06\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0016348008066415787\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0019195610657334328\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0019338340498507023\n",
      "tensor([[0.8706]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.13852165639400482\n",
      "tensor([[0.0051]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005117254797369242\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 9.536747711536009e-07\n",
      "tensor([[0.0059]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005912343040108681\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.8239186829305254e-05\n",
      "tensor([[0.0060]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0059961676597595215\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.218656047465629e-06\n",
      "tensor([[0.0098]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.009896088391542435\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0032]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003177021862939\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0036]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003577604191377759\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.00012398534454405308\n",
      "tensor([[0.0031]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0030762734822928905\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.861027041944908e-06\n",
      "tensor([[0.0027]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002669078530743718\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0065]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.006510064005851746\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 9.584885992808267e-05\n",
      "tensor([[0.0039]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003939510323107243\n",
      "tensor([[0.9997]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0002631891402415931\n",
      "tensor([[0.0031]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003093313192948699\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.576279254957626e-07\n",
      "tensor([[0.0030]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0030097311828285456\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 9.179157132166438e-06\n",
      "tensor([[0.0024]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002432023175060749\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9926]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.007441543508321047\n",
      "tensor([[0.0023]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002310260431841016\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 8.940736734075472e-06\n",
      "tensor([[0.0028]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002823460614308715\n",
      "tensor([[0.9996]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.00035244476748630404\n",
      "tensor([[0.0038]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003836172167211771\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0043]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004265751224011183\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.00013865000801160932\n",
      "tensor([[0.0023]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002334336983039975\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.051804378628731e-05\n",
      "tensor([[0.0049]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004908489063382149\n",
      "tensor([[0.7790]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.24970851838588715\n",
      "tensor([[0.0096]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.009672420099377632\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.7881409348774469e-06\n",
      "tensor([[0.0087]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.008773735724389553\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 8.344653679159819e-07\n",
      "tensor([[0.0087]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.008782274089753628\n",
      "tensor([[0.9997]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0003067725629080087\n",
      "tensor([[0.0071]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.007099137641489506\n",
      "tensor([[0.9996]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.00039400471723638475\n",
      "tensor([[0.0071]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.007102379575371742\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.576279254957626e-07\n",
      "tensor([[0.0099]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.009982836432754993\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 5.960466182841628e-07\n",
      "tensor([[0.0054]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005416492465883493\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0058]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0058011868968605995\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 7.391003236989491e-06\n",
      "tensor([[0.0132]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.013336987234652042\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0095]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.009548087604343891\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0077]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.007706098258495331\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.768372718899627e-07\n",
      "tensor([[0.0060]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.006010259035974741\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.4662849935120903e-05\n",
      "tensor([[0.0089]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.008936039172112942\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0104]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.010412836447358131\n",
      "tensor([[0.9887]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.011358148418366909\n",
      "tensor([[0.0046]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004564194940030575\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.7881409348774469e-06\n",
      "tensor([[0.0078]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.007811999414116144\n",
      "tensor([[0.9998]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0001890241983346641\n",
      "tensor([[0.0055]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00547588337212801\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.2649790025752736e-06\n",
      "tensor([[0.0050]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005009841173887253\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0072]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.007247901521623135\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.576279254957626e-07\n",
      "tensor([[0.0047]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004719169344753027\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.657022767176386e-05\n",
      "tensor([[0.0055]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00552652683109045\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0079]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.007973546162247658\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.099446303167497e-06\n",
      "tensor([[0.0066]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.006585598923265934\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0047]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004664314910769463\n",
      "tensor([[0.9998]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.00016845691425260156\n",
      "tensor([[0.0038]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0037726308219134808\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.182938598911278e-05\n",
      "tensor([[0.0037]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003708435455337167\n",
      "tensor([[0.9986]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.001351017039269209\n",
      "tensor([[0.0035]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0034968529362231493\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 5.960466182841628e-07\n",
      "tensor([[0.0082]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.008187220431864262\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.576279254957626e-07\n",
      "tensor([[0.0043]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004291491117328405\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 9.536747711536009e-07\n",
      "tensor([[0.0037]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0037308107130229473\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.00010228680184809491\n",
      "tensor([[0.0038]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003845745697617531\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.7418175250204513e-06\n",
      "tensor([[0.0058]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005852446425706148\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0044]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004426187369972467\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.768372718899627e-07\n",
      "tensor([[0.0039]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003921139985322952\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920935776288388e-06\n",
      "tensor([[0.0027]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0027335062623023987\n",
      "tensor([[0.9991]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0008614195976406336\n",
      "tensor([[0.0057]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00575772300362587\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.588264007703401e-05\n",
      "tensor([[0.0028]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0028442619368433952\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0063]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0063424548134207726\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0044]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0043760184198617935\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.2530810383614153e-05\n",
      "tensor([[0.0022]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0021637228783220053\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0036]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003563307924196124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.768372718899627e-07\n",
      "tensor([[0.0043]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004343930631875992\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.741851312748622e-05\n",
      "tensor([[0.0041]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004115096293389797\n",
      "tensor([[0.7734]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.2569222152233124\n",
      "tensor([[0.0133]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.013341275975108147\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0139]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.013992656022310257\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0083]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.008292150683701038\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0089]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.008895746432244778\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0129]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.013012061826884747\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 5.841272468387615e-06\n",
      "tensor([[0.0089]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.008914750069379807\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0083]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.008286741562187672\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.4305124977909145e-06\n",
      "tensor([[0.0067]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0066765607334673405\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0094]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.009464445523917675\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.576279254957626e-07\n",
      "tensor([[0.0060]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.006049236748367548\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.00014055763313081115\n",
      "tensor([[0.0041]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004088403191417456\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0088]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00882821436971426\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0088]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.008811798878014088\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.576279254957626e-07\n",
      "tensor([[0.0060]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.006020273081958294\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0046]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004567069001495838\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 6.7949526965094265e-06\n",
      "tensor([[0.0073]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0072962334379553795\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.2663880119798705e-05\n",
      "tensor([[0.0033]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003282085992395878\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 6.604412919841707e-05\n",
      "tensor([[0.0043]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004354466684162617\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0070]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.006987789645791054\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 5.960466182841628e-07\n",
      "tensor([[0.0055]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005518016405403614\n",
      "counter: 1000\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.576279254957626e-07\n",
      "tensor([[0.0093]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.009383398108184338\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0046]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0046389843337237835\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0036]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00365046551451087\n",
      "tensor([[0.9985]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0015354016795754433\n",
      "tensor([[0.0044]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004371169023215771\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.0371261851105373e-05\n",
      "tensor([[0.0040]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00400587497279048\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0026]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0026020854711532593\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0048]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004768517334014177\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0028]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0028452184051275253\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0075]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.007528679445385933\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0058]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00580970011651516\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.0728841743912199e-06\n",
      "tensor([[0.0033]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003326339414343238\n",
      "tensor([[0.9971]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.00293243327178061\n",
      "tensor([[0.0022]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002236959058791399\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0030]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002986535197123885\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.576279254957626e-07\n",
      "tensor([[0.0027]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002718623960390687\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 5.364432581700385e-06\n",
      "tensor([[0.0033]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0032624714076519012\n",
      "tensor([[0.1012]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.291020154953003\n",
      "tensor([[0.4716]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.6379449367523193\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.933914285880746e-06\n",
      "tensor([[0.0588]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.06062183156609535\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0365]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.03715424984693527\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0442]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0452500618994236\n",
      "tensor([[0.9998]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.00016476081509608775\n",
      "tensor([[0.0325]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.03300495445728302\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0295]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.029980367049574852\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0266]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.026996374130249023\n",
      "tensor([[0.9512]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.05002039670944214\n",
      "tensor([[0.0203]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.020535191521048546\n",
      "tensor([[0.9941]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.005946819204837084\n",
      "tensor([[0.0406]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.04143979772925377\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.768372718899627e-07\n",
      "tensor([[0.0192]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.019362980499863625\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0178]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.017969584092497826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0474]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.048591285943984985\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0117]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.011790066957473755\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0110]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.011093168519437313\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.6689314179529902e-06\n",
      "tensor([[0.0145]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.014623956754803658\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.172333774477011e-06\n",
      "tensor([[0.0119]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.01200654823333025\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 5.960466182841628e-07\n",
      "tensor([[0.0138]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.013923872262239456\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0112]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.011261213570833206\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0128]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.012832853943109512\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.145769485650817e-06\n",
      "tensor([[0.0073]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.007371287792921066\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.23978434203309e-05\n",
      "tensor([[0.0169]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.016995692625641823\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.2649790025752736e-06\n",
      "tensor([[0.0115]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0115647679194808\n",
      "tensor([[0.9978]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0022276996169239283\n",
      "tensor([[0.0107]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.010750413872301579\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 8.344653679159819e-07\n",
      "tensor([[0.0108]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.010843020863831043\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0049]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004934664815664291\n",
      "tensor([[0.9895]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.010508423671126366\n",
      "tensor([[0.0198]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.019986066967248917\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0133]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.013386037200689316\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.4305124977909145e-06\n",
      "tensor([[0.0103]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.010345322079956532\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0058]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005814616102725267\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0188]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.01894606277346611\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 9.536747711536009e-07\n",
      "tensor([[0.0100]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.010099516250193119\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0065]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.006497045047581196\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0047]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004720247350633144\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0076]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.007657866459339857\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0043]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004279399290680885\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.768372718899627e-07\n",
      "tensor([[0.0072]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0072052753530442715\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0082]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.008203746750950813\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0062]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.006259800400584936\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0128]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.012849397026002407\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.768372718899627e-07\n",
      "tensor([[0.0075]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.007539549842476845\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 9.536747711536009e-07\n",
      "tensor([[0.0038]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0037577333860099316\n",
      "tensor([[0.9991]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0008849840378388762\n",
      "tensor([[0.0040]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003974577412009239\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0048]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0047864243388175964\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0054]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00540468655526638\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.7166285033454187e-05\n",
      "tensor([[0.0052]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005236603785306215\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.5497220147153712e-06\n",
      "tensor([[0.0080]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00801139883697033\n",
      "tensor([[0.9989]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.001056334818713367\n",
      "tensor([[0.0042]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004163576290011406\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0022]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002205716446042061\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0036]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0036202557384967804\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.8358399756834842e-05\n",
      "tensor([[0.0052]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005263627041131258\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.7418175250204513e-06\n",
      "tensor([[0.0024]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0023563827853649855\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 7.152560215217818e-07\n",
      "tensor([[0.0033]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0032943449914455414\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0034]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0034273520577698946\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0036]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0035999168176203966\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0025]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0025454345159232616\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.360371763643343e-05\n",
      "tensor([[0.0041]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004140712320804596\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.314073182991706e-05\n",
      "tensor([[0.0042]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004216308239847422\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.576279254957626e-07\n",
      "tensor([[0.0034]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00340839265845716\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.0490472959645558e-05\n",
      "tensor([[0.0042]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0041935034096241\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 5.960466182841628e-07\n",
      "tensor([[0.0065]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.006545400712639093\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.768372718899627e-07\n",
      "tensor([[0.0042]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004221037030220032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.576279254957626e-07\n",
      "tensor([[0.0085]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00855446606874466\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0034]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0033966104965656996\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0041]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004154059570282698\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.768372718899627e-07\n",
      "tensor([[0.0031]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0031532240100204945\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0027]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0027042203582823277\n",
      "tensor([[0.9974]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0025536809116601944\n",
      "tensor([[0.0037]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003680915804579854\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.576279254957626e-07\n",
      "tensor([[0.0051]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005081369075924158\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 9.536747711536009e-07\n",
      "tensor([[0.0031]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003109994577243924\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 8.583106136939023e-06\n",
      "tensor([[0.0044]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004424151964485645\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.506212644628249e-05\n",
      "tensor([[0.0048]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004806907381862402\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.00012386612070258707\n",
      "tensor([[0.0032]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003194601507857442\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.7656937163555995e-05\n",
      "tensor([[0.0022]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002239468041807413\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0047]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004700365476310253\n",
      "tensor([[0.9966]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.003365929936990142\n",
      "tensor([[0.0031]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0031024611089378595\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0032]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003165421774610877\n",
      "tensor([[0.9012]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.10400267690420151\n",
      "tensor([[0.0046]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004596529062837362\n",
      "tensor([[0.9989]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0010749512584879994\n",
      "tensor([[0.0043]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004304720554500818\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 7.152560215217818e-07\n",
      "tensor([[0.0026]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0025549358688294888\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3126869564293884e-05\n",
      "tensor([[0.0052]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00520844291895628\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0070]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.007000874727964401\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0059]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0058808051981031895\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0050]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004979829769581556\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0001382923364872113\n",
      "tensor([[0.0031]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003112147096544504\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0037]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0036712242290377617\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0029]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0028712803032249212\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0034]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0033979860600084066\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.814704541582614e-06\n",
      "tensor([[0.0026]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002567186253145337\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 7.891966379247606e-05\n",
      "tensor([[0.0029]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0029381122440099716\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0073]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.007338503375649452\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0069]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.006892838515341282\n",
      "tensor([[0.9779]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.022396354004740715\n",
      "tensor([[0.0030]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003008595434948802\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.1352534278994426e-05\n",
      "tensor([[0.0077]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.007751028053462505\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.768372718899627e-07\n",
      "tensor([[0.0050]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0050232601352036\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0031]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0031483806669712067\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 7.152560215217818e-07\n",
      "tensor([[0.0050]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005013854708522558\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0071]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0070997378788888454\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0024]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0023771144915372133\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0050]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0050471024587750435\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0040]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004032685421407223\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.768372718899627e-07\n",
      "tensor([[0.0027]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0026946577709168196\n",
      "tensor([[0.9865]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.013599843718111515\n",
      "tensor([[0.0025]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0024632129352539778\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.529963462118758e-06\n",
      "tensor([[0.0043]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004304361063987017\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 9.536747711536009e-07\n",
      "tensor([[0.0027]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0027431289199739695\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0035]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003498886479064822\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0039]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003893075743690133\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 6.318112355074845e-06\n",
      "tensor([[0.0026]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0026463684625923634\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920935776288388e-06\n",
      "tensor([[0.0037]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003714298363775015\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0027]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002685035578906536\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.6451016563223675e-05\n",
      "tensor([[0.0029]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002910972572863102\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 6.282526737777516e-05\n",
      "tensor([[0.0020]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0020488016307353973\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0046]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004597546998411417\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 8.344653679159819e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0031]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003078485606238246\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.9908149624825455e-05\n",
      "tensor([[0.0030]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0030390857718884945\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.57628505298635e-06\n",
      "tensor([[0.0041]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004103365819901228\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0032]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0032463856041431427\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0055]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005536715965718031\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.0848104466276709e-05\n",
      "tensor([[0.0028]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0027681123465299606\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015801151748746634\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841887468734058e-06\n",
      "tensor([[0.0024]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0024134409613907337\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0028]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002825313713401556\n",
      "tensor([[0.9834]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0167769156396389\n",
      "tensor([[0.0046]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004582637455314398\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 5.1260126383567695e-06\n",
      "tensor([[0.0031]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0031543001532554626\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0024]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0023557853419333696\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 9.536747711536009e-07\n",
      "tensor([[0.0026]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0025621666572988033\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0025]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002521054120734334\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0032]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0031983088701963425\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 5.960482212685747e-06\n",
      "tensor([[0.0024]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002385359490290284\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.576279254957626e-07\n",
      "tensor([[0.0030]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0029971767216920853\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0024]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002429872052744031\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 9.536747711536009e-07\n",
      "tensor([[0.0026]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0026263482868671417\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0039]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0038693207316100597\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.576279254957626e-07\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012906771153211594\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0033]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003349782433360815\n",
      "tensor([[0.9919]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.008100868202745914\n",
      "tensor([[0.0020]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002013862133026123\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.6689314179529902e-06\n",
      "tensor([[0.0034]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00337382429279387\n",
      "tensor([[0.9997]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.00034385870094411075\n",
      "tensor([[0.0024]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0023832684382796288\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920935776288388e-06\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0018279560608789325\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0026]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00260859914124012\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0025]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0025479444302618504\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015071657253429294\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.0728841743912199e-06\n",
      "tensor([[0.0024]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0023563827853649855\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.6808651707833633e-05\n",
      "tensor([[0.0023]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002331588650122285\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0020]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001962977694347501\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0018595450092107058\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.47958396357717e-05\n",
      "tensor([[0.0027]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0026773856952786446\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0023]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0023448518477380276\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0026]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0026388384867459536\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 7.987054232216906e-06\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014442496467381716\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0033]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003352413885295391\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.7418175250204513e-06\n",
      "tensor([[0.0021]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0021157576702535152\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0026]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0025995755568146706\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 8.142326259985566e-05\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017713489942252636\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0040]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003999890759587288\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0016412487020716071\n",
      "tensor([[0.9996]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.00041821401100605726\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011086646700277925\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 5.722062269342132e-06\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017714087152853608\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.0265599687263602e-06\n",
      "tensor([[0.0022]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0021737581118941307\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0025]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002455983078107238\n",
      "tensor([[0.8616]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.14900505542755127\n",
      "tensor([[0.0035]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003541115904226899\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0045]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004480370320379734\n",
      "tensor([[0.7332]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.3102731704711914\n",
      "tensor([[0.0289]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.029339298605918884\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0281]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.028497958555817604\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0218]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0220351405441761\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0126]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.012723702937364578\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0101]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.01018152479082346\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0162]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.01636652834713459\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0186]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.018786681815981865\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.4305124977909145e-06\n",
      "tensor([[0.0074]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0073837172240018845\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0120]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.012087507173418999\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0109]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.010990776121616364\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0072]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0072404565289616585\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.218656047465629e-06\n",
      "tensor([[0.0119]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.01197131909430027\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0067]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.006697502452880144\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0053]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005270038265734911\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 7.152582838898525e-06\n",
      "tensor([[0.0078]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00786612555384636\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0055]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005526707042008638\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0072]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.007216321770101786\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0082]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.008208133280277252\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0053]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0053394874557852745\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0058]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005784640088677406\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0044]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004447500687092543\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0061]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0061002690345048904\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0040]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004000489134341478\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.5497220147153712e-06\n",
      "tensor([[0.0052]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005222463048994541\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.4570753086882178e-06\n",
      "tensor([[0.0035]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0035499087534844875\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 7.152560215217818e-07\n",
      "tensor([[0.0087]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.008701041340827942\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0071]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.007085331249982119\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0052]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005242535378783941\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0058]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0058174338191747665\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.6689314179529902e-06\n",
      "tensor([[0.0058]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005818093195557594\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0083]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.008288304321467876\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0033]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003261155914515257\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0059]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005938125308603048\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0039]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0039438786916434765\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0037]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0036656009033322334\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0064]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.006390863098204136\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 6.07969241173123e-06\n",
      "tensor([[0.0033]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003264743834733963\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0028]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0028504785150289536\n",
      "tensor([[0.9994]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0005848925793543458\n",
      "tensor([[0.0029]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0029281887691468\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0023]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0022945483215153217\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.622607780722319e-06\n",
      "tensor([[0.0026]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0026390773709863424\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.7418175250204513e-06\n",
      "tensor([[0.0059]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005901790224015713\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0036]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0035995577927678823\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.4570753086882178e-06\n",
      "tensor([[0.0069]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0069011808373034\n",
      "tensor([[0.9993]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0007476620958186686\n",
      "tensor([[0.0046]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004623894579708576\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0029]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002925379201769829\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0039]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003877458395436406\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.576279254957626e-07\n",
      "tensor([[0.0038]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003823427716270089\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0036]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0036149316001683474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0034]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003408512333407998\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0020]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0020505336578935385\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0046]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004638265818357468\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001765855704434216\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0039]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0039541712030768394\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.9073504518019035e-06\n",
      "tensor([[0.0026]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0025928225368261337\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 5.960466182841628e-07\n",
      "tensor([[0.0021]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0021350509487092495\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0038]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0038549003656953573\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0027]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002738885348662734\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011880896054208279\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0036]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003625818993896246\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0020]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0019748625345528126\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.6689314179529902e-06\n",
      "tensor([[0.0024]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002408840460702777\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0020]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0020103382412344217\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0037]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003722195513546467\n",
      "tensor([[0.9987]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0012571963015943766\n",
      "tensor([[0.0021]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002062777755782008\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013911263085901737\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0030]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002970932051539421\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0020]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0019834027625620365\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0024]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002380340825766325\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0032]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0032493756152689457\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001691161305643618\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.768372718899627e-07\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0019218900706619024\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0024]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0023663602769374847\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 5.364432581700385e-06\n",
      "tensor([[0.0022]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002205537399277091\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0023]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00232710805721581\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.5735749911982566e-05\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012240149080753326\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0037]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003692222759127617\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 9.536747711536009e-07\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017318215686827898\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0022]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0022296709939837456\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0029]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0028560974169522524\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0023]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0022588830906897783\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0016853698762133718\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0016660256078466773\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012084392365068197\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008805097313597798\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012595237931236625\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.9073504518019035e-06\n",
      "tensor([[0.0039]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0038650725036859512\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001940044923685491\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015003009466454387\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013192052720114589\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 7.033372639853042e-06\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0016325322212651372\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0027]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0026615483220666647\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0022]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0022153339814394712\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.576279254957626e-07\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0019411796238273382\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014340425841510296\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.2439409298822284e-05\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001794576644897461\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0037]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003754023928195238\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.768372718899627e-07\n",
      "tensor([[0.0027]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0026800152845680714\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017476442735642195\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0016739661805331707\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0020]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0020204316824674606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 9.059946933120955e-06\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014315953012555838\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0029]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0029361394699662924\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015642950311303139\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013553143944591284\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 5.960466182841628e-07\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015917564742267132\n",
      "tensor([[0.9861]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.013993622735142708\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013904697261750698\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0030]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0030308952555060387\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013893357245251536\n",
      "tensor([[0.9780]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.02225598506629467\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013426014920696616\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 7.653529610252008e-05\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008031970355659723\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0026]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0025750144850462675\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001446876092813909\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001399005064740777\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.64917320641689e-06\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014794082380831242\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.0728841743912199e-06\n",
      "tensor([[0.0022]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002210973296314478\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0016833995468914509\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001590323750860989\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.145769485650817e-06\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001880923518911004\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0024]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0023781300988048315\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.00012732362665701658\n",
      "tensor([[0.0020]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0019574237521737814\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015981444157660007\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0016825039638206363\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.515347659937106e-05\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001542863785289228\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0023]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0022741765715181828\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.0728841743912199e-06\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001731403637677431\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009697011555545032\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 8.583106136939023e-06\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001019341521896422\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0024]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00244845449924469\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 5.602852525044e-06\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010996544733643532\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012456184485927224\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.4570753086882178e-06\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010642706183716655\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013836653670296073\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.576279254957626e-07\n",
      "tensor([[0.0023]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0022813454270362854\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006434006500057876\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.5987963454099372e-05\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009627803228795528\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 5.960466182841628e-07\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008709049434401095\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.3113030945532955e-06\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011663680197671056\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007026279345154762\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010815744753926992\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1324947081448045e-05\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001267341780476272\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001795890275388956\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015039423014968634\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 5.245222382654902e-06\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013934541493654251\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013340068981051445\n",
      "tensor([[0.9859]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.01418730616569519\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0019371185917407274\n",
      "tensor([[0.9998]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.00015832246572244912\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014718272723257542\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0018549469532445073\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0018246120307594538\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013091785367578268\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001550385495647788\n",
      "tensor([[0.9997]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.00025841951719485223\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009644508245401084\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012012183433398604\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000783213647082448\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 8.344653679159819e-07\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012898414861410856\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 5.006802894058637e-06\n",
      "tensor([[0.0022]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0022276996169239283\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001096790307201445\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012099908199161291\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012756971409544349\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007756379200145602\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010139716323465109\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 7.033372639853042e-06\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014168519992381334\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 8.225474630307872e-06\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001530625973828137\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.576279254957626e-07\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009828866459429264\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.625427391147241e-05\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013491071294993162\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009076541173271835\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920935776288388e-06\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001370235811918974\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007802907493896782\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011256113648414612\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014126140158623457\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017816789913922548\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000975727045442909\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009422567673027515\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013998406939208508\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011355169117450714\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013285159366205335\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 5.722062269342132e-06\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005509582697413862\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.768382950715022e-06\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001102697686292231\n",
      "tensor([[0.9862]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.013914625160396099\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0016651300247758627\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.576279254957626e-07\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008936343947425485\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001130504417233169\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012819039402529597\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 7.391003236989491e-06\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00162280083168298\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 5.459934618556872e-05\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011960264528170228\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014230000087991357\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017275225836783648\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.768372718899627e-07\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014366093091666698\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001450278447009623\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011688146041706204\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010441626654937863\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004972859751433134\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005124331801198423\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010804408229887486\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0020]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001986150164157152\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007547006825916469\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 8.940736734075472e-06\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008944696164689958\n",
      "tensor([[0.9998]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.00024399156973231584\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006222872762009501\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.4305124977909145e-06\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012678789207711816\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013838444137945771\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.576279254957626e-07\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012235374888405204\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 9.536747711536009e-07\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009642122313380241\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012831572676077485\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011355765163898468\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001418762025423348\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.5033982637978625e-06\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005998621927574277\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 7.152560215217818e-07\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000856766477227211\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.267783515388146e-05\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010637335944920778\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 7.117047789506614e-05\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017537346575409174\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.9816695789340883e-05\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009166626259684563\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006343349814414978\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0024]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002353933174163103\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008088640752248466\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000758220034185797\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 6.437322554120328e-06\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009096228168345988\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008602861780673265\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920999895664863e-05\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012691322481259704\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010703568113967776\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007780836313031614\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.768372718899627e-07\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013495845487341285\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.171017306158319e-05\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000693144160322845\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010862883646041155\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001049711718223989\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006599219050258398\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006576554151251912\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012157197343185544\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003372403443790972\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.002736073336564e-05\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010034706210717559\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.000104194346931763\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009129040990956128\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012279537040740252\n",
      "tensor([[0.9906]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.00942804291844368\n",
      "tensor([[0.0023]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00234730145893991\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005288926186040044\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008476391667500138\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007394901476800442\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008071341435424984\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008267004159279168\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007817820296622813\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007932351436465979\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011809882707893848\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 7.152560215217818e-07\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012151229893788695\n",
      "tensor([[0.9996]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.00036389296292327344\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000675966264680028\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007920420612208545\n",
      "tensor([[0.9997]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0003436201950535178\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006976175936870277\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009923134930431843\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.3113030945532955e-06\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007076978799887002\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006531821563839912\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 7.271793037944008e-06\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000789894605986774\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 8.344653679159819e-07\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005831033922731876\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006868217024020851\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010771590750664473\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.850534267257899e-05\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008800324867479503\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007419357425533235\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009597375756129622\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006543154013343155\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 6.198902156029362e-06\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010966112604364753\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.0728893357736524e-05\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005159516585990787\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000742770847864449\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00045816664351150393\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005820895312353969\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006893268437124789\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.9073504518019035e-06\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000567895476706326\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007646026206202805\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007618587114848197\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.8358399756834842e-05\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008715015137568116\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.5497220147153712e-06\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00036049424670636654\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.7881409348774469e-06\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005577569827437401\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00040688447188585997\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.8239186829305254e-05\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006475160480476916\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009797244565561414\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006995262810960412\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.576279254957626e-07\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005788093549199402\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00043467190698720515\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007459918851964176\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006980947800911963\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006382117280736566\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 9.536747711536009e-07\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009452398517169058\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008375574834644794\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004439743352122605\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010589004959911108\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013805616181343794\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000742770847864449\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.396135460003279e-05\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010947615373879671\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.768372718899627e-07\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000712767883669585\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005803003441542387\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008926201844587922\n",
      "tensor([[0.9998]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.00019677428645081818\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006601604982279241\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009045518236234784\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000984736136160791\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007538656354881823\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.768372718899627e-07\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005387922865338624\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012754583731293678\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004474925808608532\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001100012450478971\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005219152080826461\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006451899535022676\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003280581731814891\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008496078080497682\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000540641020052135\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.57628505298635e-06\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007661535055376589\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 7.152560215217818e-07\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00034356059040874243\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011846284614875913\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008312937570735812\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004793958505615592\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005701020709238946\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.9073504518019035e-06\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000530741352122277\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.00013972305168863386\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007089504506438971\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010899282060563564\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.576279254957626e-07\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007662727730348706\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005316955503076315\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000808744749519974\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.218656047465629e-06\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005318148178048432\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004993731854483485\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000727381557226181\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00025460385950282216\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.861027041944908e-06\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009009723435156047\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004490430001169443\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007692553335800767\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010204155696555972\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.6689440599293448e-05\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004956758348271251\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.2517053619376384e-05\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004293051897548139\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.9073504518019035e-06\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006816922104917467\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003845835162792355\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.00010526733967708424\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007841084152460098\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00033616708242334425\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010642706183716655\n",
      "counter: 2000\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00047575822100043297\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004578684747684747\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006447724299505353\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000629742513410747\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005769605631940067\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00035500866943039\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006342753185890615\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008843874675221741\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007707466138526797\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920935776288388e-06\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004888179246336222\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1324947081448045e-05\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005105845048092306\n",
      "tensor([[0.0919]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.387294292449951\n",
      "tensor([[0.1439]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.15532684326171875\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0761]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.07914601266384125\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0144]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.014541159383952618\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0732]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.07599007338285446\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0372]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.037931088358163834\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0205]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0206709373742342\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0076]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.007642610464245081\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0207]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.020872551947832108\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0270]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.02737390249967575\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0253]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.025658108294010162\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 6.437322554120328e-06\n",
      "tensor([[0.0081]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.008149301633238792\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0213]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.021537384018301964\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0132]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.01323720347136259\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.337865791763761e-06\n",
      "tensor([[0.0112]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.011308052577078342\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920935776288388e-06\n",
      "tensor([[0.0099]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.009961886331439018\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 9.059946933120955e-06\n",
      "tensor([[0.0036]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0035828084219247103\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0167]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.01681625470519066\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.7537724236026406e-05\n",
      "tensor([[0.0121]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.012155319564044476\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0057]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005759161897003651\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.9206701583461836e-05\n",
      "tensor([[0.0077]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.007746343035250902\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0068]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.006791114807128906\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0165]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.01667860709130764\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0127]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.01278099324554205\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0143]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0144117446616292\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0043]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004359315615147352\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 9.536747711536009e-07\n",
      "tensor([[0.0071]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.007155386731028557\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0040]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004021374974399805\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0078]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.007815243676304817\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0078]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.007820529863238335\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0043]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004267546813935041\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.816171349375509e-05\n",
      "tensor([[0.0047]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0047284518368542194\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0067]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.006732245907187462\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.576279254957626e-07\n",
      "tensor([[0.0027]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0027445631567388773\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0072]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.007267834153026342\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0087]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.008728879503905773\n",
      "tensor([[0.9994]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0006019496358931065\n",
      "tensor([[0.0047]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004665871616452932\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 7.152560215217818e-07\n",
      "tensor([[0.0061]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.006122398190200329\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0051]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005150146316736937\n",
      "tensor([[0.9949]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.005072323139756918\n",
      "tensor([[0.0056]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005591918248683214\n",
      "tensor([[0.9997]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0002777366025839001\n",
      "tensor([[0.0081]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.008108619600534439\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0118]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.011826193891465664\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.184333738521673e-05\n",
      "tensor([[0.0035]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00354937044903636\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0072]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00717940041795373\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0062]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.006185549311339855\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0036]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0035692895762622356\n",
      "tensor([[0.9981]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.001888626953586936\n",
      "tensor([[0.0042]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004254018887877464\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0045]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004531262908130884\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0049]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0049323285929858685\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 5.960466182841628e-07\n",
      "tensor([[0.0028]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00277821347117424\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0040]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003991153556853533\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0028]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0027987149078398943\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0019229053286835551\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0067]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.006681721191853285\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0042]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00419667549431324\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0038]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0038299495354294777\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0030]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0029828886035829782\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0062]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.006182670593261719\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0024]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0024500079452991486\n",
      "tensor([[0.9982]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0018385254079475999\n",
      "tensor([[0.0035]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003475320292636752\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.576279254957626e-07\n",
      "tensor([[0.0035]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0035092344041913748\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0026]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0025906714145094156\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.541977250482887e-05\n",
      "tensor([[0.0027]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002714380621910095\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.576279254957626e-07\n",
      "tensor([[0.0023]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0022577480413019657\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 5.1260126383567695e-06\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017548094037920237\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.00010121380910277367\n",
      "tensor([[0.0037]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003681813133880496\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0030]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0029959212988615036\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0029]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0028873004484921694\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.768372718899627e-07\n",
      "tensor([[0.0044]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004420439712703228\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001100430148653686\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001782753737643361\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920935776288388e-06\n",
      "tensor([[0.0050]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005045963916927576\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0025]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0025056374724954367\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.57628505298635e-06\n",
      "tensor([[0.0040]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003975714091211557\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.576279254957626e-07\n",
      "tensor([[0.0034]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003433931153267622\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0024]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00239629321731627\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0037]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003675292246043682\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0031]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0031527455430477858\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012771293986588717\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014289689715951681\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.349837061250582e-05\n",
      "tensor([[0.0025]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002465364057570696\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0032]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0031847951468080282\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0033]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003348646219819784\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0021]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0020979580003768206\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010598552180454135\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 6.07969241173123e-06\n",
      "tensor([[0.0024]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0024276613257825375\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0035]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003486804198473692\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.622607780722319e-06\n",
      "tensor([[0.0022]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0022275205701589584\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0026]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0025882211048156023\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0019493612926453352\n",
      "tensor([[0.9998]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0002201441820943728\n",
      "tensor([[0.0021]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0020910892635583878\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0035]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003508576424792409\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0035]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0035007407423108816\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0020]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002022641710937023\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.4782061043661088e-05\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0016657270025461912\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017431661253795028\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0018238357733935118\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010037689935415983\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0020]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00195873761549592\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0032]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0031759454868733883\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.0013630344474223e-05\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001390768215060234\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0029]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0029348840471357107\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0023]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002338817808777094\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0022]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002223936142399907\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017247761134058237\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0023]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0022982521913945675\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.172412081970833e-05\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001272056601010263\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.00013614627823699266\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001221329439431429\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0021]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002063315361738205\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012451410293579102\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.0531240301788785e-06\n",
      "tensor([[0.0028]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002773969667032361\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 8.201935270335525e-05\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0018518418073654175\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0021]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0021145630162209272\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000821868481580168\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.3379157684976235e-05\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015727125573903322\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.622607780722319e-06\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001605905476026237\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011625487823039293\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013841429026797414\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.4305124977909145e-06\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013047620886936784\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0025]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002542805392295122\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.576279254957626e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012623884249478579\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013513751327991486\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017817984335124493\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014069436583667994\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.528657180140726e-05\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010524564422667027\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 7.152560215217818e-07\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001192744355648756\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012252085143700242\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.622607780722319e-06\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011847477871924639\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.887592695013154e-06\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0016469204565510154\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007796345744282007\n",
      "tensor([[0.9974]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.002574237762019038\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007906701066531241\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0019126336555927992\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 6.07969241173123e-06\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014312969287857413\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015372522175312042\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001611159066669643\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.2649790025752736e-06\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0016926538664847612\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0018828940810635686\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014562476426362991\n",
      "tensor([[0.9987]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0013147291028872132\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009730422170832753\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.768372718899627e-07\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013719070702791214\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008458495140075684\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.4108408474130556e-05\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009688658756203949\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010182078694924712\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010234584333375096\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012520041782408953\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009035376715473831\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.576279254957626e-07\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011046667350456119\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013362151803448796\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920999895664863e-05\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0016559356590732932\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 9.727950964588672e-05\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009497143910266459\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011219116859138012\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0016992215532809496\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001368504948914051\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010301410220563412\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007853014394640923\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001691877725534141\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007812451221980155\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013028521789237857\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012512282701209188\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.2649790025752736e-06\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001142021152190864\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001261135097593069\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.622607780722319e-06\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015957563882693648\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 5.960482212685747e-06\n",
      "tensor([[0.0021]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0020853551104664803\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014124349690973759\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 6.9141628955549095e-06\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006522875046357512\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012307584984228015\n",
      "tensor([[0.2382]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.434850811958313\n",
      "tensor([[0.2961]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.3511711657047272\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0604]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.062303319573402405\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0208]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.021031595766544342\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0161]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.016196714714169502\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0116]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.011644545011222363\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0188]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.01900249719619751\n",
      "tensor([[0.9952]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.004806607495993376\n",
      "tensor([[0.0184]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.018566541373729706\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 9.536747711536009e-07\n",
      "tensor([[0.0052]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005175129976123571\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 5.960466182841628e-07\n",
      "tensor([[0.0132]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.013337288983166218\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0176]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.017788514494895935\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0189]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.019077036529779434\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0160]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.016129596158862114\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0107]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.010714204981923103\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0020]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0020068741869181395\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0072]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.007192967925220728\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0047]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0047179716639220715\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.576279254957626e-07\n",
      "tensor([[0.0192]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.019366808235645294\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0065]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.006472088396549225\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0028]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0028043335769325495\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.0728841743912199e-06\n",
      "tensor([[0.0043]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004319147206842899\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0040]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004057461861521006\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.576279254957626e-07\n",
      "tensor([[0.0050]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004983903374522924\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0045]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004546651151031256\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 8.344653679159819e-07\n",
      "tensor([[0.0153]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.015398374758660793\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0084]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.008411037735641003\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0039]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0038613027427345514\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0148]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.01487172394990921\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0109]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.01096323598176241\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017870529554784298\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0040]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003961112815886736\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920935776288388e-06\n",
      "tensor([[0.0047]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004758216440677643\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0033]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003302238881587982\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.4305124977909145e-06\n",
      "tensor([[0.0034]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0033779507502913475\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013475553132593632\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0055]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005517896264791489\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0051]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00506986677646637\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0016103829257190228\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0047]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00474779587239027\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0027]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002688621636480093\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0030]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0030099106952548027\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0053]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0053324163891375065\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0034]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003415928687900305\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0025]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0025105969980359077\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013167583383619785\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0068]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0068436856381595135\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0031]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0031233876943588257\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.576279254957626e-07\n",
      "tensor([[0.0022]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002248428761959076\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0043]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0042792195454239845\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0040]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003968772478401661\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011998458066955209\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0026]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0026143959257751703\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0020]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002009263262152672\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0105]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.01059426087886095\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0022]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0022284165024757385\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0031]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003080040216445923\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014626942574977875\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0026]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0026062685064971447\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.291543518775143e-06\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001104905386455357\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0021]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002115399343892932\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0023]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0022540444042533636\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.768372718899627e-07\n",
      "tensor([[0.0020]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0019700846169143915\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0022]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0021575104910880327\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0029]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0028834748081862926\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.2649790025752736e-06\n",
      "tensor([[0.0023]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002323164837434888\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001822820631787181\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0063]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.006352472119033337\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0029]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0028838932048529387\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0021]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0021003473084419966\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0028]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002786103170365095\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.768372718899627e-07\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001716536469757557\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008077903185039759\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010448190150782466\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.576279254957626e-07\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017445991979911923\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012156004086136818\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.337865791763761e-06\n",
      "tensor([[0.0033]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003347928635776043\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013831878313794732\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.768372718899627e-07\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017330157570540905\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001876623835414648\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0028]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002758369781076908\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0022]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0021815237123519182\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014508157037198544\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011512108612805605\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0016694286605343223\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010012034326791763\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0020]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00197749026119709\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015548031078651547\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001414941973052919\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015580267645418644\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009752497426234186\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0016968930140137672\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 6.6757424974639434e-06\n",
      "tensor([[0.0021]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002095867646858096\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.6689314179529902e-06\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012174503644928336\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.768372718899627e-07\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013574630720540881\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008021232788451016\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007349568768404424\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009501916938461363\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009986378718167543\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0018486768240109086\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0018802068661898375\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010729822097346187\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.254466719226912e-05\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001038733054883778\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.622607780722319e-06\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001385157578624785\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007236237288452685\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0019092296715825796\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012944369809702039\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0019381338497623801\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008067165617831051\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0030]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002970872214064002\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 9.179157132166438e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0020]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0020417538471519947\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0019214123021811247\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.6689314179529902e-06\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010161196114495397\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0018180435290560126\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001441444270312786\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011470337631180882\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001033721026033163\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013942897785454988\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013892760034650564\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0019255329389125109\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0024]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002416308969259262\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0021]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0020865497644990683\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0036]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0036341340746730566\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001368266181088984\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001268416061066091\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012564801145344973\n",
      "tensor([[0.9997]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.00033121826709248126\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010630176402628422\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008675045683048666\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011750208213925362\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011877912329509854\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 8.344653679159819e-07\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0019246968440711498\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001104487688280642\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006285496638156474\n",
      "tensor([[0.9989]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.00108455796726048\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009019864955917001\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011565814493224025\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0016488309483975172\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 5.960466182841628e-07\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009470893419347703\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014194186078384519\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 7.152560215217818e-07\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013697583926841617\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010125397238880396\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 8.344653679159819e-07\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014139271806925535\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0018139233579859138\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022825223277322948\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012885285541415215\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007561323000118136\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011627875501289964\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010972080053761601\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017361205536872149\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001249378197826445\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008763934019953012\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009238217608071864\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015293126925826073\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000804807641543448\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006253289757296443\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007379393209703267\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010503680678084493\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008000354864634573\n",
      "tensor([[0.9868]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.013335658237338066\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001845930004492402\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010799635201692581\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 7.152560215217818e-07\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000499969522934407\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006984526989981532\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012278342619538307\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015968907391652465\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010711324866861105\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009938647272065282\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004799921589437872\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011371280997991562\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 9.775639773579314e-05\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005576973198913038\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001031513325870037\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0016658464446663857\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009156484156847\n",
      "tensor([[0.9979]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0021155187860131264\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014383402885869145\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014879442751407623\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010081245563924313\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000990463886409998\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 7.152560215217818e-07\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008707260130904615\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0021]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002106320345774293\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00038064809632487595\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008717401651665568\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 6.19907514192164e-05\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010205348953604698\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0018195961602032185\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008834329200908542\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00039472023490816355\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00108455796726048\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007363287732005119\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013403930934146047\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.00011766648822231218\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008094009244814515\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00027338427025824785\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005745153757743537\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008669079979881644\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011742450296878815\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 6.19907514192164e-05\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006929055671207607\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005724279908463359\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008838505018502474\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.768372718899627e-07\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009201228385791183\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005612756358459592\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005464258138090372\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007650798070244491\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.2439409298822284e-05\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001038494287058711\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0021]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0020577607210725546\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009346798760816455\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015662054065614939\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 5.960466182841628e-07\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009204211528412998\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00035035787732340395\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003246596024837345\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007441427442245185\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.7418175250204513e-06\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010266207391396165\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008004530100151896\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007019718177616596\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006214522873051465\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.0728841743912199e-06\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005921089905314147\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010387926595285535\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011725741205736995\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010553800966590643\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010651060147210956\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00041290701483376324\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006395835080184042\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006440567085519433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007527919369749725\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 7.319718133658171e-05\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012215084861963987\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005737400497309864\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009585443185642362\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00034785360912792385\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920935776288388e-06\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006535996799357235\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004246540484018624\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00046854265383444726\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006262236274778843\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007428305107168853\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 7.152560215217818e-07\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006990491528995335\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010687457397580147\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012623287038877606\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004972263122908771\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010861094342544675\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004058111517224461\n",
      "tensor([[0.9966]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0033676044549793005\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004015775048173964\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010118833743035793\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004928730195388198\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008901742403395474\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005803003441542387\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.60018530045636e-05\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007388340309262276\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006233011954464018\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005101670394651592\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012382182758301497\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.576279254957626e-07\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00042030104668810964\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008649392984807491\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009862277656793594\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013630138710141182\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00033795583294704556\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004613271448761225\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007389532984234393\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00045107046025805175\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008110711933113635\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 8.344653679159819e-07\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00037045186036266387\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007820802857168019\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.5749537599040195e-05\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009015092509798706\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000476533459732309\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006168598774820566\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003710481396410614\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006522278417833149\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005862643010914326\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007986038108356297\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003668742720037699\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006288478616625071\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00031190013396553695\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010420146863907576\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00046562065836042166\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000547081814147532\n",
      "tensor([[0.9996]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0003876841510646045\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00045435020001605153\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.862455560010858e-05\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00032102252589538693\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003449319628998637\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009434499661438167\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004797536530531943\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021692483278457075\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005492884083651006\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006762048578821123\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 5.960466182841628e-07\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006618901388719678\n",
      "tensor([[0.0169]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.08303689956665\n",
      "tensor([[0.2903]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.34286725521087646\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0233]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.02358134277164936\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 6.9141628955549095e-06\n",
      "tensor([[0.0224]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.02263343334197998\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0265]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.026894545182585716\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.0384995877975598e-05\n",
      "tensor([[0.0198]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.01998661458492279\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 6.962064799154177e-05\n",
      "tensor([[0.0156]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.01570100709795952\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 7.271793037944008e-06\n",
      "tensor([[0.0185]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.01872163638472557\n",
      "tensor([[0.9994]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.000586681766435504\n",
      "tensor([[0.0102]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.010274199768900871\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 7.510213436034974e-06\n",
      "tensor([[0.0209]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.021114446222782135\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 6.47327397018671e-05\n",
      "tensor([[0.0076]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0076157632283866405\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 6.198902156029362e-06\n",
      "tensor([[0.0253]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.025637315586209297\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0111]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.011159648187458515\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.087568256887607e-05\n",
      "tensor([[0.0079]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.007943986915051937\n",
      "tensor([[0.9461]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.055402159690856934\n",
      "tensor([[0.0242]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.024483665823936462\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0249]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.025263193994760513\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0131]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.013232673518359661\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 5.960482212685747e-06\n",
      "tensor([[0.0140]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.014053283259272575\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0112]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.011243973858654499\n",
      "tensor([[0.9997]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0002822678361553699\n",
      "tensor([[0.0072]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.007272517308592796\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.98023678624304e-06\n",
      "tensor([[0.0071]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.007151304744184017\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.4080565708572976e-05\n",
      "tensor([[0.0060]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.006005521863698959\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0045]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004551920108497143\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.768382950715022e-06\n",
      "tensor([[0.0078]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.007812961004674435\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0001366231736028567\n",
      "tensor([[0.0093]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.009341161698102951\n",
      "tensor([[0.9997]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0002654546988196671\n",
      "tensor([[0.0086]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.008670017123222351\n",
      "tensor([[0.9948]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0051902285777032375\n",
      "tensor([[0.0131]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.013233458623290062\n",
      "tensor([[0.9995]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.000457749207271263\n",
      "tensor([[0.0072]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.007194288540631533\n",
      "tensor([[0.9998]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.00016249546024482697\n",
      "tensor([[0.0123]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.01239885576069355\n",
      "tensor([[0.9993]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.000719150120858103\n",
      "tensor([[0.0056]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005664327181875706\n",
      "tensor([[0.9948]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.005208502523601055\n",
      "tensor([[0.0029]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0029341070912778378\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0053]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005294365808367729\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0052]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005171055905520916\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0089]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.008959433995187283\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0070]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0070738657377660275\n",
      "tensor([[0.9998]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.00021024768648203462\n",
      "tensor([[0.0078]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.007872373796999454\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 8.344653679159819e-07\n",
      "tensor([[0.0106]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.010642212815582752\n",
      "tensor([[0.9812]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.018968963995575905\n",
      "tensor([[0.0078]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.007814522832632065\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.00010622111585689709\n",
      "tensor([[0.0025]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0025127483531832695\n",
      "tensor([[0.9974]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.002567903371527791\n",
      "tensor([[0.0126]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.012639674358069897\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.1962550312746316e-05\n",
      "tensor([[0.0063]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.006316902115941048\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0056]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005665465723723173\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 7.140891102608293e-05\n",
      "tensor([[0.0084]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.008430272340774536\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0026]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002617025515064597\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.588264007703401e-05\n",
      "tensor([[0.0073]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.007303258404135704\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.576279254957626e-07\n",
      "tensor([[0.0044]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004379670135676861\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.00013519247295334935\n",
      "tensor([[0.0049]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004909327253699303\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0050]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0050541115924716\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0052]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005232708994299173\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0049]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004896449390798807\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.00013423866766970605\n",
      "tensor([[0.0063]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.006358650512993336\n",
      "counter: 3000\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 5.364432581700385e-06\n",
      "tensor([[0.0037]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0037211186718195677\n",
      "tensor([[0.9664]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.03414418175816536\n",
      "tensor([[0.0036]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0035587018355727196\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0034]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0033732859883457422\n",
      "tensor([[0.9996]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.000390188506571576\n",
      "tensor([[0.0022]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0021667093969881535\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.9073504518019035e-06\n",
      "tensor([[0.0063]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.006352472119033337\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0044]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004377993755042553\n",
      "tensor([[0.9987]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0012659095227718353\n",
      "tensor([[0.0039]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0038979824166744947\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 6.6757424974639434e-06\n",
      "tensor([[0.0032]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0032233037054538727\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0070]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0070047760382294655\n",
      "tensor([[0.9981]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0019254134967923164\n",
      "tensor([[0.0072]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.007263391278684139\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0031]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0030889485497027636\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0029]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002933748299255967\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0074]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.007404673378914595\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0051]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0051624285988509655\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0026]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002648639492690563\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 8.213857654482126e-05\n",
      "tensor([[0.0028]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002798177069053054\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.98023678624304e-06\n",
      "tensor([[0.0046]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004587906878441572\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 8.344653679159819e-07\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0019358644494786859\n",
      "tensor([[0.9969]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0031054506544023752\n",
      "tensor([[0.0035]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003505406202748418\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015430428320541978\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.541977250482887e-05\n",
      "tensor([[0.0025]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00249691354110837\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.2278632311790716e-05\n",
      "tensor([[0.0055]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005555955693125725\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.576279254957626e-07\n",
      "tensor([[0.0024]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0023765768855810165\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0026]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002571608405560255\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0032]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0032181611750274897\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 8.344653679159819e-07\n",
      "tensor([[0.0040]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003963865805417299\n",
      "tensor([[0.8445]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.16906926035881042\n",
      "tensor([[0.0045]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0044904290698468685\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0068]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.006789734587073326\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.5497220147153712e-06\n",
      "tensor([[0.0054]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005384970456361771\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0089]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.008895746432244778\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0075]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.007525797002017498\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.2649790025752736e-06\n",
      "tensor([[0.0083]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.008368361741304398\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0051]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005121808033436537\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841887468734058e-06\n",
      "tensor([[0.0071]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.007169254589825869\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.57628505298635e-06\n",
      "tensor([[0.0029]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002888077637180686\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.00010109458526130766\n",
      "tensor([[0.0048]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004800738301128149\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0046]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0046234154142439365\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0075]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.007561590522527695\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 6.07969241173123e-06\n",
      "tensor([[0.0051]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005136666353791952\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0043]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004334831144660711\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 5.960466182841628e-07\n",
      "tensor([[0.0089]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.008989324793219566\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0040]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003996001090854406\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 8.344653679159819e-07\n",
      "tensor([[0.0056]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005644545890390873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0037]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003746724920347333\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0040]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003999890759587288\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0031]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0030898454133421183\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001491108094342053\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0029]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0029092987533658743\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 5.960466182841628e-07\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0019455988658592105\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.922062387573533e-05\n",
      "tensor([[0.0067]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0067032030783593655\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0104]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.010445721447467804\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 5.960466182841628e-07\n",
      "tensor([[0.0040]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003991153556853533\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 9.536747711536009e-07\n",
      "tensor([[0.0029]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0028886753134429455\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1086525773862377e-05\n",
      "tensor([[0.0044]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004427384585142136\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0041]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004077271558344364\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0050]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005040093325078487\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0031]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0031256000511348248\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.6689314179529902e-06\n",
      "tensor([[0.0053]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005319593008607626\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012314746854826808\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014627539785578847\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0028]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0027969814836978912\n",
      "tensor([[0.9830]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.017122045159339905\n",
      "tensor([[0.0065]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.006543721072375774\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.3232318451628089e-05\n",
      "tensor([[0.0034]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0034170649014413357\n",
      "tensor([[0.9977]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.002295862650498748\n",
      "tensor([[0.0026]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002598918043076992\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0064]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.006418937351554632\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017675872659310699\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0038]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0037744257133454084\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0028]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002801942639052868\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0026]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0025779425632208586\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00185781333129853\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0022]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0022309254854917526\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015588029054924846\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.8638572479831055e-05\n",
      "tensor([[0.0023]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0023400126956403255\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0030]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002954432275146246\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0038]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0037666477728635073\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 8.344653679159819e-07\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013518526684492826\n",
      "tensor([[0.9927]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0073311179876327515\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014161954168230295\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0032]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0031901765614748\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0020]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001976056955754757\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0021]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0020800393540412188\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017268061637878418\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.396135460003279e-05\n",
      "tensor([[0.0025]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0025454943533986807\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014336247695609927\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.5497327694902197e-05\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013036280870437622\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001758212805725634\n",
      "tensor([[0.9998]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.00016166086425073445\n",
      "tensor([[0.0059]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0058709727600216866\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0045]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004552039783447981\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014004972763359547\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0052]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005210360046476126\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001245976542122662\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 6.437322554120328e-06\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00129837600979954\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014510544715449214\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0023]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0022652156185358763\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.7881409348774469e-06\n",
      "tensor([[0.0021]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002097480231896043\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0019433893030509353\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0029]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002907864050939679\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 7.176656799856573e-05\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0016559356590732932\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015592804411426187\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0032]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0031663186382502317\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0026]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0026212686207145452\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015987413935363293\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.811997572076507e-05\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010072295553982258\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 7.271793037944008e-06\n",
      "tensor([[0.0022]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0021650369744747877\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012050375808030367\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001375368912704289\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 5.960466182841628e-07\n",
      "tensor([[0.0022]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0021764461416751146\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.576279254957626e-07\n",
      "tensor([[0.0025]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0025454943533986807\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.4305124977909145e-06\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013677887618541718\n",
      "tensor([[0.9989]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0010729224886745214\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017158199334517121\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0021]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0020566259045153856\n",
      "tensor([[0.9989]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.001090286299586296\n",
      "tensor([[0.0023]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0022705921437591314\n",
      "tensor([[0.9783]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.02191760763525963\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0019445238867774606\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0022]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0021736386697739363\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0019344312604516745\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0027]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0027125875931233168\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0025]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0024813776835799217\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0038]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0038064352702349424\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0018338080262765288\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0025]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0025374870747327805\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011021605459973216\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0042]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004180275369435549\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.576279254957626e-07\n",
      "tensor([[0.0027]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0026723656337708235\n",
      "tensor([[0.9512]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.05001481994986534\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0016707421746104956\n",
      "tensor([[0.9978]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.002246337942779064\n",
      "tensor([[0.0022]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002162408549338579\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001827538013458252\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.2649790025752736e-06\n",
      "tensor([[0.0040]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00401323614642024\n",
      "tensor([[0.9994]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0006399413687177002\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0016714585945010185\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0020]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001971995923668146\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0027]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002743308199569583\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 8.344653679159819e-07\n",
      "tensor([[0.0034]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0034392541274428368\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0023]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002319042570888996\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0025]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0025254760403186083\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0035]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0035387829411774874\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0035]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0034798062406480312\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017287764931097627\n",
      "tensor([[0.9939]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.006073343101888895\n",
      "tensor([[0.0074]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.007430794648826122\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0016753991367295384\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.576279254957626e-07\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013251736527308822\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009899268625304103\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015379686374217272\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.6689314179529902e-06\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010802021715790033\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015178511384874582\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.0728841743912199e-06\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012610753765329719\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0023]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0023090059403330088\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0021]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00207346910610795\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.111410842393525e-05\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007756975828669965\n",
      "tensor([[0.9942]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0057844603434205055\n",
      "tensor([[0.0025]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002501215785741806\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 9.203380614053458e-05\n",
      "tensor([[0.0037]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0037028715014457703\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 5.960482212685747e-06\n",
      "tensor([[0.0030]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00298085599206388\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.0728841743912199e-06\n",
      "tensor([[0.0033]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0032902786042541265\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012120198225602508\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.1338690203265287e-05\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009905236074700952\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007877471507526934\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 6.687864515697584e-05\n",
      "tensor([[0.0024]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0024212084244936705\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0020]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0020189385395497084\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.576279254957626e-07\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015547435032203794\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011853445321321487\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0021]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0020920448005199432\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.0531240301788785e-06\n",
      "tensor([[0.0028]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0027945907786488533\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001941119902767241\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0016814293339848518\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.887592695013154e-06\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0016888327663764358\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.3856013033073395e-05\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000878063787240535\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0038]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0037595282774418592\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013408109080046415\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 6.07969241173123e-06\n",
      "tensor([[0.0020]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0020369160920381546\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010205348953604698\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.3113030945532955e-06\n",
      "tensor([[0.0025]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0025181262753903866\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0028]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002790645696222782\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017174917738884687\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.57628505298635e-06\n",
      "tensor([[0.0021]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0020531616173684597\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008203771430999041\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017226863419637084\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001915559871122241\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015773690538480878\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00036603951593860984\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011361136566847563\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001634681480936706\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013372298562899232\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006787696038372815\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013740557478740811\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006867024349048734\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.849142583727371e-05\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012261633528396487\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.3589951777248643e-05\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010769203072413802\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007142590475268662\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009606921230442822\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0032]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00319687370210886\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011250146199017763\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001000547083094716\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.00012100474850740284\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015103892656043172\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1563368389033712e-05\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014534421497955918\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 6.830925849499181e-05\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011787803377956152\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 7.152560215217818e-07\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010272173676639795\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007165256538428366\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1086525773862377e-05\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007360305171459913\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007174800266511738\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001403959235176444\n",
      "tensor([[0.9819]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.01829138956964016\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010861094342544675\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.849142583727371e-05\n",
      "tensor([[0.0028]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0028169455472379923\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009109950042329729\n",
      "tensor([[0.9996]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0004251310892868787\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001037479960359633\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.768372718899627e-07\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009521605097688735\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011074712965637445\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0016841758042573929\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010802021715790033\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 5.960466182841628e-07\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014466970460489392\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.6689314179529902e-06\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00031106540700420737\n",
      "tensor([[0.9962]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.003794827964156866\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009903445607051253\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005761852371506393\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008886231225915253\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010804408229887486\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 9.310679160989821e-05\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007019121549092233\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 9.536788638797589e-06\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010699988342821598\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 9.059946933120955e-06\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006898039719089866\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011630859225988388\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017911132890731096\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017559438711032271\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011287739034742117\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001359671470709145\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.1457903130794875e-05\n",
      "tensor([[0.0027]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0026911317836493254\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.8716033082455397e-05\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005576377152465284\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001535103190690279\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0040]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004034002311527729\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006425656611099839\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007066242396831512\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014474133495241404\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 6.9141628955549095e-06\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0019287577597424388\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009626013343222439\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009908218635246158\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014735582517459989\n",
      "tensor([[0.9993]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.000718792260158807\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014398922212421894\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014527258463203907\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009151711128652096\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004947813577018678\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006370188784785569\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.9073668227065355e-05\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012349956668913364\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.768372718899627e-07\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013814569683745503\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0016641747206449509\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012756374198943377\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 7.033372639853042e-06\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008107729372568429\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007970528677105904\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.0728841743912199e-06\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008768706466071308\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.2874686035502236e-05\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014496815856546164\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.768372718899627e-07\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006148320389911532\n",
      "tensor([[0.9996]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.000382496538804844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010059765772894025\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005757081671617925\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.98023678624304e-06\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003433816891629249\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.768372718899627e-07\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008189454092644155\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.5020483260741457e-05\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011731708655133843\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 7.152560215217818e-07\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006503789336420596\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004155903297942132\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0001490227150497958\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006346331792883575\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0021]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0021398295648396015\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 6.401744030881673e-05\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005965223535895348\n",
      "tensor([[0.9997]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0002878722734749317\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004005041846539825\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0023]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00232065562158823\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007602481637150049\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003793362993746996\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005631840322166681\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013670724583789706\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011260886676609516\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0016050098929554224\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0023]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002262885682284832\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.768382950715022e-06\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013231444172561169\n",
      "tensor([[0.9233]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.07980291545391083\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010256661335006356\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.0728841743912199e-06\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010883171344175935\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015974279958754778\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.7299300199956633e-05\n",
      "tensor([[0.0034]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0034002589527517557\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 5.960482212685747e-06\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014646044000983238\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 9.536747711536009e-07\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006243150564841926\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011128416517749429\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015237609623000026\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0018044888274744153\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0020]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002011592499911785\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014450853923335671\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009903445607051253\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012958693550899625\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006704193074256182\n",
      "tensor([[0.9986]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0014397131744772196\n",
      "tensor([[0.0024]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0024164882488548756\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 7.987054232216906e-06\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005804792745038867\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 9.536747711536009e-07\n",
      "tensor([[0.0022]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0022028491366654634\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017871723975986242\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001304045901633799\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006252097082324326\n",
      "tensor([[0.9367]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.06535080820322037\n",
      "tensor([[0.0021]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002112114103510976\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0023]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0023282431066036224\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0045]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0045279101468622684\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017182083101943135\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 5.960466182841628e-07\n",
      "tensor([[0.0023]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0023085875436663628\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0021]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002062180545181036\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0018622919451445341\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0021]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0020810547284781933\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008851033053360879\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0044]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004402539227157831\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.3113030945532955e-06\n",
      "tensor([[0.0036]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003586876206099987\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0022]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002193351276218891\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000665170606225729\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0028]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0027585490606725216\n",
      "tensor([[0.9998]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.00021513630053959787\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0018258063355460763\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0045]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004556410945951939\n",
      "tensor([[0.9932]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.006810918916016817\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010754286777228117\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012274165637791157\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0039]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003942442592233419\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0016658464446663857\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0027]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002689637476578355\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001245081308297813\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0019274440128356218\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.000129827341879718\n",
      "tensor([[0.0034]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003432555589824915\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011806898983195424\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.5749537599040195e-05\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014363108202815056\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012524218764156103\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0039]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0038660296704620123\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0016252485802397132\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0031]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0031050918623805046\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017902774270623922\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015367746818810701\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008420912781730294\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 9.536747711536009e-07\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009773379424586892\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001125969341956079\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015442367875948548\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012561817420646548\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0016572491731494665\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0024]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002393066883087158\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.576279254957626e-07\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007448585820384324\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1563368389033712e-05\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015902043087407947\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0016241739504039288\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.194859891664237e-05\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012601802591234446\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005481553380377591\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001217569806613028\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001704296562820673\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008874299819581211\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0024]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002407944295555353\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001127401483245194\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.768372718899627e-07\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008601668523624539\n",
      "tensor([[0.7428]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.297390878200531\n",
      "tensor([[0.0071]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00715340580791235\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0071]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.007129272911697626\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0137]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.013765651732683182\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0145]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.014560633338987827\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0092]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00926969014108181\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.398919554660097e-05\n",
      "tensor([[0.0071]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.007172736339271069\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0038]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003776220604777336\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0042]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0041671073995530605\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0124]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.012436392717063427\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0023]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002328661270439625\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0078]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.007861260324716568\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001869995379820466\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0040]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004010124132037163\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0101]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.010105959139764309\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0025]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002481855684891343\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001658323802985251\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0025]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0025414905976504087\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0087]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.008751367218792439\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0023]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0022829584777355194\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0031]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0030806977301836014\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0046]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004629762843251228\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0029]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0028950716368854046\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0039]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003914318047463894\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0033]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003303255420178175\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0026]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00264618918299675\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.5497220147153712e-06\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014801245415583253\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0034]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0034403307363390923\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0040]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0040477667935192585\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0070]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.007040910888463259\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0069]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.006944754160940647\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0023]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002267067553475499\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001631994848139584\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001137903775088489\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0025]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0024717573542147875\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.576279254957626e-07\n",
      "tensor([[0.0024]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0023811175487935543\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0038]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0037692205514758825\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0026]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0026437987107783556\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0075]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.007522734347730875\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0038]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0037999735213816166\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.0728841743912199e-06\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015739065129309893\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013730410719290376\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00180335424374789\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0022]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0022182015236467123\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0022]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0022225622087717056\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0025]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0024909977801144123\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0018674274906516075\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 7.152560215217818e-07\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013991841115057468\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009873614180833101\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.2649790025752736e-06\n",
      "tensor([[0.0032]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0032152310013771057\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0045]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004475520458072424\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017777977045625448\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0027]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0027401403058320284\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0075]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.007516968995332718\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0062]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0061779324896633625\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0021]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0021040504798293114\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004268007178325206\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0018833718495443463\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017222086898982525\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.7418175250204513e-06\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010180885437875986\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010220265248790383\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000910816015675664\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0023]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002278119558468461\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014756475575268269\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0051]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005081848707050085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011228664079681039\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008643427863717079\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013984679244458675\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0090]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.009013501927256584\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007075786124914885\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009639735799282789\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.4305124977909145e-06\n",
      "tensor([[0.0025]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002491953782737255\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004534557228907943\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001310312538407743\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012990921968594193\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00026074470952153206\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007377007277682424\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014631121885031462\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008955434313975275\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012176891323179007\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007575639174319804\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.814704541582614e-06\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001180809224024415\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005752310389652848\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0020]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002022641710937023\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00046240052324719727\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00186091847717762\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009122478077188134\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007050138083286583\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015197613975033164\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014884218107908964\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0022]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002239169320091605\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013522703666239977\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001010451465845108\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007492129225283861\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001690862700343132\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008843278046697378\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.98023678624304e-06\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010694618104025722\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0016777276759967208\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 6.9141628955549095e-06\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012265214463695884\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0018508863868191838\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006422674632631242\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000867027323693037\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0021]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0021288986317813396\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004325848422013223\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0021]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002086131600663066\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001035093329846859\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009035972761921585\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005636014975607395\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000945657433476299\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.0728841743912199e-06\n",
      "tensor([[0.0020]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0019533028826117516\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.0609683158691041e-05\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007956211920827627\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006041563465259969\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00043389672646299005\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014866309938952327\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011844494147226214\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009101597825065255\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000610537885222584\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001327740028500557\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00047730866936035454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008367819827981293\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007108591962605715\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010937470942735672\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.576279254957626e-07\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006857480620965362\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0016074576415121555\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004409927933011204\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007875085575506091\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001578622730448842\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007831539842300117\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007604867569170892\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005418934160843492\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005226904759183526\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008857595385052264\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006708964356221259\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00030879973201081157\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006035599508322775\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008677431615069509\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006640373612754047\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005837594508193433\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0024]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002394620329141617\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00029717330471612513\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007088311831466854\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0021]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0021032739896327257\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0022]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002156972885131836\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008288479875773191\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.3113030945532955e-06\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003793362993746996\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007920420612208545\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006709560984745622\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006021881708875299\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009104580385610461\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005404024850577116\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009805597364902496\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019057421013712883\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.60018530045636e-05\n",
      "tensor([[0.0027]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0026708117220550776\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000749749771784991\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000617277342826128\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005364664830267429\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008330834098160267\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008105940069071949\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012213891604915261\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007838698220439255\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008650586241856217\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004835105501115322\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007290517096407712\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.2755474926962052e-05\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007564902189187706\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004421257763169706\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005304432124830782\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009161853231489658\n",
      "counter: 4000\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007011963753029704\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006994070135988295\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00027177450829185545\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013372894609346986\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.768372718899627e-07\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005998621927574277\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003098729357589036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009927310748025775\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008043901179917157\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00026611052453517914\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002643219195306301\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1324947081448045e-05\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009070574888028204\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.8835246009984985e-05\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008653569384478033\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004217321693431586\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007462304783985019\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006349910399876535\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.764313128660433e-05\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00027600760222412646\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1444157280493528e-05\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011946539161726832\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005992061924189329\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005754695739597082\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008781234500929713\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 5.1260126383567695e-06\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00024369347374886274\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009186313836835325\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008552750805392861\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012764133280143142\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003835101961158216\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007648412138223648\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004529190482571721\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00036305817775428295\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000331993360305205\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005859661032445729\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010997737990692258\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002399970981059596\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008005723357200623\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005513757350854576\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007631113403476775\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005633032997138798\n",
      "tensor([[0.7135]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.3375442326068878\n",
      "tensor([[0.0060]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.006035144440829754\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0063]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.006354691460728645\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0064]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.006459670141339302\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0054]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005439804866909981\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0047]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004695933777838945\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0056]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005608521401882172\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0092]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.009251341223716736\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0054]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0054307556711137295\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0055]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005515438970178366\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0088]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.008872052654623985\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0053]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005351053085178137\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0031]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00311471801251173\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0071]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.007127231918275356\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0031]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0030621634796261787\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0053]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00532744312658906\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0095]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.009581606835126877\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009095034911297262\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015953981783241034\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0016653090715408325\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0046]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0046523381024599075\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0044]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004395714495331049\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0036]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00362067436799407\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012063505128026009\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00151128473225981\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001235353760421276\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00146102299913764\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0020]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0019789235666394234\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0100]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.010003968141973019\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008683993946760893\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.0531240301788785e-06\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012002037838101387\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0022]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0022154536563903093\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009417198016308248\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0022]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0021836741361767054\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0035]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003505406202748418\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0047]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004720187745988369\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0056]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005622727330774069\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0040]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004032685421407223\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014933167258277535\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.768372718899627e-07\n",
      "tensor([[0.0029]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002933329902589321\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017480623209849\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0046]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004604313522577286\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007132450700737536\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015541465254500508\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0031]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0031168705318123102\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014341023052111268\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0021]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002076097298413515\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011648164363577962\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0059]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005963188596069813\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0016772500239312649\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015100310556590557\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0032]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003161654807627201\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001427357317879796\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0039]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003945614211261272\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012492588721215725\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0028]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0028268678579479456\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014543375000357628\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009862874867394567\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011476305080577731\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006460846052505076\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0020]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002016669139266014\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0018306431593373418\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0032]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003214334137737751\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007742659654468298\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0019387907814234495\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0023]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002316115191206336\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007766520138829947\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002931786293629557\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0023]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0022696363739669323\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009050887892954051\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009728632285259664\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013111480511724949\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0020]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002038110513240099\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005424301489256322\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001304642646573484\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014981519198045135\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.529963462118758e-06\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011989505728706717\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0018277768976986408\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005492288037203252\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005207821377553046\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005709966644644737\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008704873616807163\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0045]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004489710554480553\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015628025867044926\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0023]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0022652156185358763\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008240159950219095\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010839016176760197\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001070834114216268\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010901669738814235\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0032]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0032409438863396645\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009740564855746925\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0027]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002705654827877879\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010254870867356658\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0021]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002087266417220235\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00040980629273690283\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017703339690342546\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007117539062164724\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007122906972654164\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004673500079661608\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010108690476045012\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 5.602852525044e-06\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006160845514386892\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004891757271252573\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011121255811303854\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006691667949780822\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004148747830186039\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011876121861860156\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008589140488766134\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007318551652133465\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010874817380681634\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.0967314665322192e-05\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011781835928559303\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007424725918099284\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000718971190508455\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007832733099348843\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.933914285880746e-06\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001690623932518065\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0026]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002564138500019908\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005266860825940967\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004251310892868787\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001440966734662652\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00024649559054523706\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008026005234569311\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005786900874227285\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001175796496681869\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006967825465835631\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005499444087035954\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00043288301094435155\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005275806761346757\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015191046986728907\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000306653295410797\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001563877216540277\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006047527422197163\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00092567119281739\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004043204244226217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000855811988003552\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007423533243127167\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018956074200104922\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005458890809677541\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004684830200858414\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001353881903924048\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008688169764354825\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014714690623804927\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005950909690000117\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009235234465450048\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0021]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002096046693623066\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002865605929400772\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010544254910200834\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 5.1260126383567695e-06\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00034463382326066494\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013499426422640681\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003875648835673928\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006059455336071551\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009099807939492166\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009209580603055656\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007946070982143283\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008058813982643187\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000833322003018111\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0016682345885783434\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.633180545468349e-05\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007881647325120866\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012243730016052723\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00037701084511354566\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010483990190550685\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007517182384617627\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008224053308367729\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011231647804379463\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003507156216073781\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0016371889505535364\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004595381615217775\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007223710999824107\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006995859439484775\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00026658750721253455\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007063259836286306\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005689093377441168\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010182078694924712\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000949535402469337\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004536346241366118\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012162568746134639\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003640122013166547\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014533227076753974\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012853057123720646\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.9073504518019035e-06\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000709308369550854\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00029246314079500735\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00039817867218516767\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.315469413995743e-05\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004792765830643475\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004613867786247283\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004946024273522198\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006426253239624202\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001403242931701243\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004917400074191391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005753503064624965\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.695494797284482e-06\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006841376889497042\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003249577130191028\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005203647306188941\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00038577604573220015\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006105975480750203\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00044659810373559594\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003371807106304914\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003909636870957911\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008106536115519702\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004393231065478176\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003480920859146863\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00039376618224196136\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 5.006915671401657e-05\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000693502021022141\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.576279254957626e-07\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003390290657989681\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000327044544974342\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00035882473457604647\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016613194020465016\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005593075766228139\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020213978132233024\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007647218881174922\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003577514726202935\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004356856516096741\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.576279254957626e-07\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007424725918099284\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019468771643005311\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003783822467084974\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00040402228478342295\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003030163061339408\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.172333774477011e-06\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005252548144198954\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005925861187279224\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005706388619728386\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00023755272559355944\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003293102781753987\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00024250108981505036\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002795252366922796\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000357334065483883\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008266407530754805\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00035059635411016643\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008260442409664392\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.218656047465629e-06\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004026508249808103\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00024500509607605636\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00047707013436593115\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017400109209120274\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.768372718899627e-07\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00038857856998220086\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 5.173817044124007e-05\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00032847552211023867\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000848414667416364\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00040950815309770405\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004542905662674457\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004721206205431372\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006092258263379335\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00046067117364145815\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006164424121379852\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00031303297146223485\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.576279254957626e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007520760991610587\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00036323704989627004\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006231818697415292\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003795748052652925\n",
      "tensor([[0.0314]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.459642171859741\n",
      "tensor([[0.9542]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.0841331481933594\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014616198604926467\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0038]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0038046403788030148\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0053]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0053590829484164715\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001920934533700347\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0041]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00408666767179966\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017517642118036747\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0024]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0023588920012116432\n",
      "tensor([[0.9990]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0010038285981863737\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017708116210997105\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007532691233791411\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0021]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002054714597761631\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 5.364432581700385e-06\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017657959833741188\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0037]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003737391671165824\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0023]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002346942899748683\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015652502188459039\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001445503206923604\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0020]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001977131934836507\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0037]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00371130695566535\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001926130149513483\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 6.318112355074845e-06\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009564561769366264\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0062]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.006238388363271952\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000842568464577198\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0028]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0028320681303739548\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0036]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003615529742091894\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0025]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0025226676370948553\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0088]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.008830860257148743\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0024]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0024199537001550198\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0024]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002419654978439212\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008330834098160267\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0033]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0032563121058046818\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001114094746299088\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001668652519583702\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014778561890125275\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015213731676340103\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011181524023413658\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0018837301759049296\n",
      "tensor([[0.9997]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0003268060681875795\n",
      "tensor([[0.0030]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0029695569537580013\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012445442844182253\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0141]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.014183739200234413\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008776462054811418\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0021]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0020562077406793833\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 5.746052920585498e-05\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013686839956790209\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0028]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0027941723819822073\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0038]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003840839257463813\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005638400325551629\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001349524944089353\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0040]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004000070504844189\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0024]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002449828665703535\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0026]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0025742973666638136\n",
      "tensor([[0.9997]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.00029478842043317854\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001208677887916565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0024]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002378309378400445\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001052933745086193\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006168598774820566\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0023]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002340430859476328\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0018353008199483156\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.6689314179529902e-06\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001017969218082726\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0068]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0068540084175765514\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.5497220147153712e-06\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015266860136762261\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014568445039913058\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0027]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0026898167561739683\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010244727600365877\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0024]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0024481555446982384\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010686860186979175\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013710714410990477\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006758469971828163\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0032]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0032514685299247503\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.576279254957626e-07\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00029329786775633693\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0018441386055201292\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0021]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0020732302218675613\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014223434263840318\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007413988932967186\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005755888414569199\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007020314806140959\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010828275699168444\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007887015817686915\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015457292320206761\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0023]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0022747740149497986\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006451302906498313\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011284159263595939\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010439240140840411\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010004277573898435\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017157006077468395\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010126590495929122\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013033297145739198\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012019941350445151\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.576279254957626e-07\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000808744749519974\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0022]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0021918576676398516\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007133643375709653\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000256034720223397\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007363884360529482\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011613554088398814\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011537171667441726\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00045601988676935434\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013073880691081285\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00042495218804106116\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005117175751365721\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00044826779048889875\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0025]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0024924916215240955\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008643427863717079\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009552629198879004\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0021]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002119222190231085\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0021]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002125732833519578\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000800512672867626\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004636528028640896\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001319682807661593\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0021]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0020591942593455315\n",
      "tensor([[0.9838]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.016347626224160194\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012621496571227908\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015502661699429154\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008284900104627013\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0020]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002037334255874157\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001744718523696065\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010701181599870324\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010016210144385695\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0027]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00266991532407701\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001372026395983994\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009614677401259542\n",
      "tensor([[0.9996]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.00043467190698720515\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000647217791993171\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011055618524551392\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014251488028094172\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0025]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002461539814248681\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0020]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0020196554251015186\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00096749363001436\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006033810204826295\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001039091031998396\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0026]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0025714291259646416\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012962871696799994\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00132272660266608\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015601161867380142\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 5.960466182841628e-07\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007026875391602516\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 9.536788638797589e-06\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005636014975607395\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011133786756545305\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004430202243383974\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013969159917905927\n",
      "tensor([[0.9911]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.008903564885258675\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009050291264429688\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010978643549606204\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001816610456444323\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000521020672749728\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005756485043093562\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.5033982637978625e-06\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012375022051855922\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0020]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002033869968727231\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012539138551801443\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007563708932138979\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008735298761166632\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007842277409508824\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00042876851512119174\n",
      "tensor([[0.9941]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.005940043833106756\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005677762092091143\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005051577463746071\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0029]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002953117247670889\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011599828721955419\n",
      "tensor([[0.9994]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0006445934996008873\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013430789113044739\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0018139233579859138\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011688743252307177\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00047462517977692187\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005479167448356748\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.00010145224950974807\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006061841268092394\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008615985861979425\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006774573703296483\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0030]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00297427992336452\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0018646805547177792\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002053590869763866\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0020]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0020407983101904392\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006088679656386375\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0018781764665618539\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.1100266167195514e-05\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001575518399477005\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011711419792845845\n",
      "tensor([[0.9998]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.00015891861403360963\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001190416980534792\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003829735505860299\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004921574145555496\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001296406495384872\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005572798545472324\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005189930670894682\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.768372718899627e-07\n",
      "tensor([[0.0064]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0063974615186452866\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008947082096710801\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011815850157290697\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005760063068009913\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003988942189607769\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 5.960466182841628e-07\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00037635493208654225\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.5497220147153712e-06\n",
      "tensor([[0.0023]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0023294975981116295\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010078261839225888\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006508560618385673\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000664275954477489\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005411777528934181\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012837540125474334\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006076155113987625\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 7.867844033171423e-06\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004669922054745257\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004427220846991986\n",
      "tensor([[0.9996]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.00036162714241072536\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009509673109278083\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006076751160435379\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004842857888434082\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008995405514724553\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.2993897144042421e-05\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008416140335611999\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 5.960466182841628e-07\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004530979203991592\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001285186386667192\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014558298280462623\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004369378730189055\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004426028172019869\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001291452907025814\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001153001096099615\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005393886822275817\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007868523825891316\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008260442409664392\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005002080579288304\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009395123925060034\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006803203723393381\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014212093083187938\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.576279254957626e-07\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020553793001454324\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00036347555578686297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00027517290436662734\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009377822279930115\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008665500208735466\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005394482868723571\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000722251832485199\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00026169861666858196\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005164883914403617\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011246565263718367\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005751117714680731\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015347449807450175\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007918631308712065\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001182897831313312\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.4186006410454866e-05\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017672887770459056\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008540222770534456\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.349837061250582e-05\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00043025927152484655\n",
      "counter: 5000\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006366013549268246\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 9.536747711536009e-07\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009569930844008923\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0022]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002230089157819748\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001597010064870119\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 7.152560215217818e-07\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00035465092514641583\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006314124912023544\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000852411612868309\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004451669519767165\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001366714364849031\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004713453818112612\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003310393658466637\n",
      "tensor([[0.9962]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.00383748859167099\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005574587848968804\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.3351529560168274e-05\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002453032066114247\n",
      "tensor([[0.8345]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.18089844286441803\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013803825713694096\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 5.734131264034659e-05\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017154617235064507\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011794366873800755\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0019457780290395021\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013732201186940074\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006706579006277025\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007084136595949531\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0024]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002435787348076701\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0029]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0029275910928845406\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0028]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002851195866242051\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0030]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0030224653892219067\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015561762265861034\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001310969004407525\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0028]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002821787027642131\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.9431303371675313e-05\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0016740856226533651\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0052]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005214673932641745\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001179556013084948\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0038]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0037671863101422787\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0024]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002352797891944647\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008791376021690667\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0057]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005753586534410715\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0016964750830084085\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0021]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002145922277122736\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0033]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0032715012785047293\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0026]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0025931214913725853\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013454663567245007\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0031]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0030671856366097927\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0043]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00427670544013381\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00038577604573220015\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007216553203761578\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017995923990383744\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014099280815571547\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013628944288939238\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0023]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0023034499026834965\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014561282005161047\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009915378177538514\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000881106301676482\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011380231007933617\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006249711150303483\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004919188795611262\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017033412586897612\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004598959640134126\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014074211940169334\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0070]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00701215909793973\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0034]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0034490032121539116\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008255073335021734\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009176768362522125\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015751004684716463\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.707477662828751e-05\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008025408606044948\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009213757002726197\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0026]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0026209100615233183\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0020]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0020342881325632334\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013768014032393694\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003821387654170394\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009146938682533801\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0016410695388913155\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001127819181419909\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009231058065779507\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007433673599734902\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001486034132540226\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0018529166700318456\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010761446319520473\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0031]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0031509518157690763\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.695494797284482e-06\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006244343821890652\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008379750652238727\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0024]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002380639547482133\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001635636668652296\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007141994428820908\n",
      "tensor([[0.9995]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0005249566747806966\n",
      "tensor([[0.0024]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0023674953263252974\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009439869318157434\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009596182499080896\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012067086063325405\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007258903351612389\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000800154753960669\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006582518690265715\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007249359623529017\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014923616545274854\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0033]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0032609167974442244\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010933293960988522\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00036347555578686297\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006940984749235213\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005103459698148072\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006595640443265438\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00106981978751719\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004478503542486578\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006974386633373797\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.217317342001479e-05\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009226285619661212\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.768372718899627e-07\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009609307744540274\n",
      "tensor([[0.7671]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.2651253938674927\n",
      "tensor([[0.0076]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.007586214225739241\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0018769224407151341\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.00014198834833223373\n",
      "tensor([[0.0069]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.006902201101183891\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0034]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0033626405056566\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0090]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00902823731303215\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0036]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00358771369792521\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010535900946706533\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0049]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004876204300671816\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0032]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003206859575584531\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0045]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004507253412157297\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0019493612926453352\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0029]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0029523400589823723\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 6.795160152250901e-05\n",
      "tensor([[0.0070]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.006985388696193695\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.576279254957626e-07\n",
      "tensor([[0.0055]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005500695202499628\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0039]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003860584693029523\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001491824397817254\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017111628549173474\n",
      "tensor([[0.9984]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.001614562002941966\n",
      "tensor([[0.0041]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004110667388886213\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012669838033616543\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007814241107553244\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.145769485650817e-06\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014905111165717244\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008598088752478361\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0049]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00494161294773221\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0061]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.006144287530332804\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0031]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0030751973390579224\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013276804238557816\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0023]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0023449116852134466\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.561653880344238e-05\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008253880077973008\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0026]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002571787452325225\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0034]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003455761820077896\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0034]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0033858453389257193\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015889506321400404\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017005947884172201\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0104]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.010444034822285175\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0024]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00238099810667336\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003556049196049571\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0035]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0035061240196228027\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.4305124977909145e-06\n",
      "tensor([[0.0041]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004092772491276264\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00105233711656183\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001782514969818294\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008538433467037976\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.4662849935120903e-05\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010138523066416383\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009373645880259573\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0025]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002472235355526209\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0025]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0024987657088786364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0033]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0033191628754138947\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001499644247815013\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006527646328322589\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0019100657664239407\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000815485545899719\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0025]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002543342998251319\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005228694062680006\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015165378572419286\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0033]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0033476892858743668\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0022]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0022498625330626965\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0022]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0021971145179122686\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008776462054811418\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006087486981414258\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0039]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003953812178224325\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011301464401185513\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0030]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0029730245005339384\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 5.996406980557367e-05\n",
      "tensor([[0.0034]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003405521856620908\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0025]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002535873558372259\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000867206254042685\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004958547651767731\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0019359838915988803\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001379367895424366\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014985697343945503\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006451899535022676\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014619780704379082\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.576279254957626e-07\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015593401622027159\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0023]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002325554611161351\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013246365124359727\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0062]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.006238447967916727\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001370952115394175\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0026]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002562345936894417\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010128379799425602\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011761545902118087\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013477940810844302\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011117676040157676\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0046]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004590421449393034\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001011227024719119\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0032]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0031970529817044735\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008589140488766134\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007111573941074312\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013614627823699266\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009525781497359276\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0029]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002937873126938939\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0020]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002041335916146636\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0024]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0023533357307314873\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007633499335497618\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011673227418214083\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000369974848581478\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006301599787548184\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007931158179417253\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0024]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002420371863991022\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010111674200743437\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0016139650251716375\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008967365720309317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0063]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.006335016805678606\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014912871411070228\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001354359439574182\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 5.781817526440136e-05\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010657623643055558\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017486594151705503\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0018465270986780524\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007355533307418227\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015804136637598276\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007178378873504698\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012218665797263384\n",
      "tensor([[0.1901]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.6599880456924438\n",
      "tensor([[0.0264]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.026705486699938774\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0502]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.05145619809627533\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0263]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.026668082922697067\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0965]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.10147710144519806\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0105]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.01057293638586998\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0100]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.010099335573613644\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0295]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.02998349815607071\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0033]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003279095981270075\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0268]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.027142126113176346\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0041]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004140413366258144\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0073]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.007343546953052282\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0041]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004091575276106596\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0050]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005047820974141359\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0095]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.009567765519022942\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0078]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.007795840501785278\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0211]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.021296605467796326\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0060]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.006041860673576593\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0178]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.017974073067307472\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0042]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0041954186744987965\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0033]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003304630983620882\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0080]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.008049492724239826\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0111]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.01118827797472477\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0021]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002121252939105034\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0036]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0036556702107191086\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0022]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002234211191534996\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0075]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.007513966411352158\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0034]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003380462760105729\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0058]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0058519067242741585\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.145769485650817e-06\n",
      "tensor([[0.0045]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004474742338061333\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0023]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0023159359116107225\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0074]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.007435058243572712\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0122]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.012246065773069859\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0069]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.006887136958539486\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0030]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0030464394949376583\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0116]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.011660043150186539\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012039038119837642\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0135]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.013637667521834373\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0132]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.013329255394637585\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0027]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0027058341074734926\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001368504948914051\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0045]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004559704102575779\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0036]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003571263514459133\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0018724437104538083\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0056]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005639391019940376\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0099]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.009963752701878548\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0076]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.007619186770170927\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0026]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0026096152141690254\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0050]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0049889348447322845\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011127223260700703\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0028]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0028356544207781553\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0068]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.006862710230052471\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0036]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003593516070395708\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0040]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004008807707577944\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0027]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0026797165628522635\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007992599857971072\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015804733848199248\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0074]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00738593889400363\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0055]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005477621220052242\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0019202776020392776\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0101]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.01015810202807188\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005056348163634539\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 9.536747711536009e-07\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001216913340613246\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017370758578181267\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015801151748746634\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001879788818769157\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0036]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003646038705483079\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007827364024706185\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0022]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0021685613319277763\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0018563204212114215\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001825746614485979\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012104085180908442\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0020]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001968054100871086\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005263282801024616\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015700857620686293\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000823180831503123\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001116362283937633\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013627751031890512\n",
      "tensor([[0.9919]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.00817075464874506\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009457767591811717\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0021]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002132243709638715\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0024]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002357517834752798\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001726089627481997\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0087]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.008732186630368233\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013861722545698285\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0045]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004557009786367416\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006595043814741075\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011588491033762693\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0025]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0024886077735573053\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0021]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0020646294578909874\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0064]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.006398541387170553\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0044]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004407867323607206\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011405294062569737\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0038]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00381654710508883\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011754981242120266\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009169608820229769\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0020]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0019839999731630087\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0031]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0030799205414950848\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0020]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00198573200032115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001594323548488319\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013359764125198126\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00118874607142061\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000730841129552573\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001286678365431726\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010689247865229845\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011240598978474736\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013450485421344638\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008874299819581211\n",
      "tensor([[0.9138]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.09018581360578537\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00116935174446553\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00060469307936728\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0016991021111607552\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0030]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002961426740512252\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012425151653587818\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012305794516578317\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000769314996432513\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0047]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004720307420939207\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0022]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002211929066106677\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0021]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002109844470396638\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006098818266764283\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0030]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0029942472465336323\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0035]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0035353137645870447\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012868574121966958\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0031]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0030838067177683115\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0023]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0023173100780695677\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0036]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0036218708846718073\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0039]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003945255186408758\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011542541906237602\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012666257098317146\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0024]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0023782497737556696\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001262209378182888\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010278141126036644\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014051529578864574\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011274612043052912\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013243380235508084\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005426090792752802\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0040]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0039732009172439575\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012567187659442425\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013781144516542554\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011197634739801288\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000867206254042685\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0026]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0026431414298713207\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0027]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0026978852692991495\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000984736136160791\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00035405464586801827\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001344451680779457\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003594806184992194\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017028636066243052\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010592584731057286\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0028]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0027602824848145247\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010809778468683362\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001110633835196495\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008103553554974496\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0029]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002912705997005105\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006080329767428339\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0016450099647045135\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012977792648598552\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0027]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002664655912667513\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015692500164732337\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005875166971236467\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0022]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002160616684705019\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004728361964225769\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012084988411515951\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001659816363826394\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0016582640819251537\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0048]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004793731030076742\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007146765710785985\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008045690483413637\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011291319970041513\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0022]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0022075085435062647\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0026]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0025592981837689877\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0020]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002021148568019271\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0022]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002193888882175088\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006891479133628309\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008167382329702377\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008116677636280656\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001897524925880134\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011158252600580454\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00111105153337121\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012140488252043724\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014134497614577413\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012009795755147934\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0035]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0035545146092772484\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0021]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0020903723780065775\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015165378572419286\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009177961619570851\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0026]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0026123044081032276\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001396557898260653\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0016146814450621605\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005670008831657469\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0020]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001971099991351366\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007569674053229392\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003855375398416072\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011535977246239781\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00040825593168847263\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003113635175395757\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008133976953104138\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001331917941570282\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010802617762237787\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0020]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002002275548875332\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012739663943648338\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 7.033372639853042e-06\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006059455336071551\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005674183485098183\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000552329933270812\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006169791449792683\n",
      "tensor([[0.9997]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0003188164555467665\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0016810114029794931\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008936940575949848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.861027041944908e-06\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007342410972341895\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004901895299553871\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000747542770113796\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007649604813195765\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008644023910164833\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006276550120674074\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000396389834349975\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000569982803426683\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008263424970209599\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010262030409649014\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004833912826143205\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0016070397105067968\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007240412523970008\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001076681655831635\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005984308663755655\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014363108202815056\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0027]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0027136036660522223\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008173348032869399\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00040813666419126093\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006799028487876058\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008842084789648652\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015833985526114702\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001138500520028174\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000543086149264127\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010224441066384315\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00040628816350363195\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.0728841743912199e-06\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000351610011421144\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013699374394491315\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0027]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002737928880378604\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009211967117153108\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007613218622282147\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005032494082115591\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008997791446745396\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.933914285880746e-06\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007912666187621653\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001358955167233944\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00036073275259695947\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008104150183498859\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000629742513410747\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011793770827353\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006146531668491662\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000682825455442071\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0016697869868949056\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005257319426164031\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001019162591546774\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007530305301770568\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 5.006802894058637e-06\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000865297275595367\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017266867216676474\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015820324188098311\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002456609217915684\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010383749613538384\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0019428518135100603\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008587947813794017\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017162378644570708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009139779140241444\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006989298271946609\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009410039056092501\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00047074907342903316\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001304881414398551\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004446899110917002\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011145721655339003\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920935776288388e-06\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005692075355909765\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006789485341869295\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010791877284646034\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006134006544016302\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011907153530046344\n",
      "tensor([[0.9998]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0002226481301477179\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00024118948203977197\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016529734421055764\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011164816096425056\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017078790115192533\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008963189902715385\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007065049721859396\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004870885459240526\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006220486829988658\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008269390091300011\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004563180555123836\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013763835886493325\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007874488946981728\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001382471644319594\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00033694220473989844\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006332613993436098\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008037339430302382\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004850013938266784\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00025490194093436003\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006157863535918295\n",
      "tensor([[0.9909]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.009137587621808052\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007185536669567227\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019564157992135733\n",
      "counter: 6000\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008590930374339223\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004001464112661779\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006020092405378819\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013378863222897053\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001449144328944385\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007241009152494371\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0016503235092386603\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013441751070786268\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008964979788288474\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009053870453499258\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003533987619448453\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003882804303430021\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003433816891629249\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007459322223439813\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009989958489313722\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019254154176451266\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020553793001454324\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005458890809677541\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00027606720686890185\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010115851182490587\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004850609984714538\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010650462936609983\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00023826815595384687\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005961048882454634\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00041463624802418053\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000855811988003552\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002403548132861033\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003442164452280849\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.933984044124372e-05\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004981208476237953\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000405035971198231\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003585265949368477\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000415649963542819\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007552971947006881\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006401202990673482\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.8000764612224884e-05\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017573769437149167\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005675972788594663\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004874463484156877\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018234722665511072\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005201858002692461\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009266257402487099\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015434607630595565\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006615919410251081\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005407602875493467\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003059378359466791\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00040193527820520103\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003244807303417474\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00034594559110701084\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017930685135070235\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008733508875593543\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0019040341721847653\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017577948747202754\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00030945558683015406\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013463018694892526\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007276201504282653\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002906744775827974\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005576377152465284\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005420723464339972\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004154710622970015\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009262081584893167\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005067082238383591\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0016124724643304944\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00026825687382370234\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 7.534310861956328e-05\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006327842711471021\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005709370598196983\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007345989579334855\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00032650792854838073\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006709560984745622\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.8114e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.811405470827594e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009299070807173848\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005212592659518123\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012531380634754896\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003336628433316946\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005446367431432009\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007604270940646529\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005359297501854599\n",
      "tensor([[0.9857]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.014404972083866596\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000561752705834806\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015915706171654165\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005529263289645314\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.2649790025752736e-06\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004981805104762316\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004076000186614692\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005015796632505953\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010998931247740984\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007058488554321229\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00048601513844914734\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009346202132292092\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006820500711910427\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00038971149479039013\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006568800890818238\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009599762270227075\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00028274478972889483\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009449415374547243\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000530741352122277\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008006319985724986\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019063382933381945\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001364443451166153\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007050734129734337\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000829563825391233\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.8519359552301466e-05\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007199852261692286\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001115109189413488\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005256722797639668\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013949463609606028\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006023671012371778\n",
      "tensor([[0.9895]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.010551010258495808\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004335389530751854\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019844355119857937\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007005999214015901\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005179792642593384\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.576279254957626e-07\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003287140280008316\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005585919134318829\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019397232972551137\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005708177341148257\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012702661333605647\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003556049196049571\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000704596284776926\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009177364991046488\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00034994049929082394\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008942309650592506\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00045118972775526345\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021954800467938185\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007671078783459961\n",
      "tensor([[0.9986]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0013635510113090277\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009917168645188212\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002502516144886613\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014783044753130525\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015079417498782277\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004470155108720064\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007486164686270058\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 9.536747711536009e-07\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006398817058652639\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.2649790025752736e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008814642205834389\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 6.198902156029362e-06\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022926574456505477\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003380750713404268\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000970059132669121\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004683041188400239\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005138047854416072\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007771292002871633\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0026]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002554218750447035\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.0848104466276709e-05\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003202474326826632\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001889645791379735\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000693502021022141\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001328079670201987\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004623408894985914\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020941304683219641\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014401907101273537\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005395079497247934\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012046796036884189\n",
      "tensor([[0.9920]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.007991631515324116\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008101167622953653\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0020]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0019971393048763275\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006544943316839635\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00041016406612470746\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007714623934589326\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021710367582272738\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00029896196792833507\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000650438538286835\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00042280551861040294\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006994666764512658\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005477974773384631\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005750521086156368\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015015537792351097\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007263675215654075\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009060432785190642\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005704002687707543\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002893031924031675\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006091065006330609\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008036146173253655\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002510266494937241\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004065863322466612\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013453469146043062\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008043304551392794\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004278144333511591\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005526877357624471\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008404208929277956\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005672394181601703\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0021]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0020778293255716562\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013116851914674044\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00029019752400927246\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014771122368983924\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003523851337376982\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00038404687074944377\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002270002441946417\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00040163713856600225\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00032585207372903824\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00023248513753060251\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.0728841743912199e-06\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005273420829325914\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00030766689451411366\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003291314060334116\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013513285375665873\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011678598821163177\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00043848829227499664\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014526706945616752\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001812145346775651\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003442164452280849\n",
      "tensor([[0.9993]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0007140204543247819\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000941182894166559\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009185120579786599\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00024679367197677493\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00040020604501478374\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006789485341869295\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006153092253953218\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006280725356191397\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003427258343435824\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001288886647671461\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00044403396896086633\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013322525774128735\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002426203282084316\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.2226e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.224344153655693e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00029812727007083595\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00033402061671949923\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000469258229713887\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004303189052734524\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004272181249689311\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 8.344653679159819e-07\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014845418045297265\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00041511328890919685\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005564449238590896\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00025311336503364146\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004675289092119783\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000988316023722291\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00042131476220674813\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.3113030945532955e-06\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004377130826469511\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001360506983473897\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022097883629612625\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008772285655140877\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00037438725121319294\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000292165030259639\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004239981062710285\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021495745750144124\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00037057112785987556\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017686024075374007\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022652330517303199\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 8.225474630307872e-06\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00032424222445115447\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000715511676389724\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003362267161719501\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018037992413155735\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003259117074776441\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002942518040072173\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001265486644115299\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010055809252662584\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006438181735575199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021406318410299718\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004603134002536535\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005469625466503203\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012033666716888547\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003697959764394909\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00059055833844468\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002640238089952618\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00031285409932024777\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000283102534012869\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013236815575510263\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005315166199579835\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008287286618724465\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0016716377576813102\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011152651131851599\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019707236788235605\n",
      "tensor([[0.9996]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.00039483950240537524\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002364795800531283\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 9.536747711536009e-07\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007802907493896782\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002779154747258872\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 8.344653679159819e-07\n",
      "tensor([[8.1221e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.124443411361426e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021281122462823987\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.2755474926962052e-05\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00045560247963294387\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022807337518315762\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007626938167959452\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 9.417578439752106e-06\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00024882075376808643\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016088587290141732\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009578880271874368\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009711330640129745\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001418095052940771\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005964030860923231\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004466577374842018\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012801134726032615\n",
      "tensor([[0.2755]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.2893439531326294\n",
      "tensor([[0.0630]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.06504740566015244\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0320]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.03255312144756317\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0164]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.01657194457948208\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0039]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0039509995840489864\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0087]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.008711803704500198\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0085]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.008493930101394653\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0228]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.023079827427864075\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0239]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.02419283613562584\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0058]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005808380898088217\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0120]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.012060600332915783\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0159]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.015987137332558632\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0103]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.01038748025894165\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0107]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.010796384885907173\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0104]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.010431326925754547\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0052]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005199815146625042\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0064]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.006384324282407761\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0044]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004387153312563896\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0197]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0199385154992342\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0101]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.010180080309510231\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0050]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0050345822237432\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0035]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0035556512884795666\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0139]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.014036660082638264\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0019018246093764901\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0104]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.010481077246367931\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0046]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004589523188769817\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0027]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002739124232903123\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 7.915810419945046e-05\n",
      "tensor([[0.0057]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005718037486076355\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0086]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.008672061376273632\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0200]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.020166926085948944\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0048]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0048273904249072075\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0029]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002926215995103121\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0019235622603446245\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012312359176576138\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011935201473534107\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0048]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004855061415582895\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0053]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005351112689822912\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0042]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0042024217545986176\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0068]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.006779112853109837\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014668130315840244\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013809794327244163\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0032]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003200282109901309\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0019008690724149346\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0032]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003193465294316411\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0021]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002119640354067087\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013779354048892856\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0050]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005058424547314644\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0027]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0026608312036842108\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0031]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003140548011288047\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0037]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0036849838215857744\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0043]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004272754769772291\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017743943026289344\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0030]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002983187325298786\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012984954519197345\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0055]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005530183203518391\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001330127357505262\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0047]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004744082689285278\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0067]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.006741367280483246\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 5.960466182841628e-07\n",
      "tensor([[0.0034]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0033852471970021725\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0023]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002298192586749792\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0035]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0035358520690351725\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00044206614256836474\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0042]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004159925039857626\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011790787102654576\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0018037722911685705\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0052]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005252721719443798\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0016466219676658511\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015131948748603463\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0016845937352627516\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0047]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004684315994381905\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0031]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0031322967261075974\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013560305815190077\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005770201678387821\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0057]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005703530739992857\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0037]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003710469463840127\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00067668198607862\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008521729614585638\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0024]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0023989223409444094\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005604407051578164\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010827082442119718\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0018653972074389458\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0036]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0036430475302040577\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00034844985930249095\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0053]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005321810021996498\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011911330511793494\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0027]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002740379422903061\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010006664087995887\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0023]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0022977744229137897\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014854371547698975\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 5.960466182841628e-07\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014812586596235633\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001746808411553502\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0036]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0035982418339699507\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006892075180076063\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006140567129477859\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008893390186131\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0045]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004493662156164646\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0070]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.007071164436638355\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0027]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002702845726162195\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0030]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003036993322893977\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001473617972806096\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001194176496937871\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0025]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0025230858009308577\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0033]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00330277718603611\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013458840548992157\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0020]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0020442623645067215\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004716435505542904\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0029]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0029216131661087275\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017887249123305082\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001274443813599646\n",
      "tensor([[0.9942]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.005861739628016949\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005618123686872423\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0040]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0040048579685389996\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0018068175995722413\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011308027897030115\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001344451680779457\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012107666116207838\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0047]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004714258946478367\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001243171631358564\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006494246190413833\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001016417983919382\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011279385071247816\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00030325481202453375\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006120289326645434\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000989509280771017\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.78815535036847e-05\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007037015748210251\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014570832718163729\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006361242267303169\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00043640119838528335\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001421746565029025\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007964563556015491\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0022]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0021975922863930464\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014712302945554256\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0030]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003048292826861143\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000542907218914479\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0016805934719741344\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006240168586373329\n",
      "tensor([[0.9635]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0371425561606884\n",
      "tensor([[0.0023]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0023060785606503487\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0021]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0021308099385350943\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0025]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0025262529961764812\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011972200591117144\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0043]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004261441063135862\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017741554183885455\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0016002935590222478\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013023150386288762\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00028781263972632587\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004944235552102327\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013433772837743163\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012263423996046185\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000587218499276787\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0021]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00207149819470942\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017803653609007597\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001693609170615673\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001290557673200965\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006548521923832595\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001217797034769319\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014484877465292811\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009803808061406016\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007526726112700999\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0024]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002367674605920911\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0024]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0023767559323459864\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0039]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003948426805436611\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010376590071246028\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0027]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002677624812349677\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000552329933270812\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0018109974917024374\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0031]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0031147778499871492\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003237056080251932\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0016322933370247483\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004679463163483888\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002831621386576444\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0042]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004222712945193052\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009626609971746802\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000595150631852448\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003435009566601366\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013839638559147716\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009067592327482998\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007317955023609102\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006079733138903975\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011013252660632133\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014974356163293123\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841887468734058e-06\n",
      "tensor([[0.0030]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003022226272150874\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010564541444182396\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001082529197447002\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 8.344653679159819e-07\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001757436664775014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012413215590640903\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013015392469242215\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010434467112645507\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001998743537114933\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0027]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0027314142789691687\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012403667205944657\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006604586960747838\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 5.960466182841628e-07\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016547618724871427\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00041368219535797834\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011280578328296542\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006998245371505618\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013743542367592454\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003528025117702782\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009216740145348012\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001518746605142951\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005113001097925007\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008794955210760236\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004098659264855087\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009331883629783988\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0055]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00548942806199193\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000778202957008034\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007595919887535274\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014320728369057178\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0020]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0020449792500585318\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006316510844044387\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0079]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.007887152954936028\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0024]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0024442120920866728\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00048315274761989713\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005866221035830677\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006497824797406793\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007891787681728601\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005636014975607395\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00100293371360749\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001862222416093573\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011699484894052148\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002237212611362338\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001318541617365554\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022390010417439044\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018830879707820714\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013038668548688293\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005517335375770926\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014215674018487334\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015811300836503506\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002825659466907382\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009212563745677471\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015869805356487632\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00061709841247648\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011673227418214083\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00129837600979954\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001576070935698226\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006078540463931859\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006876567495055497\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00029866385739296675\n",
      "counter: 7000\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0110]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.011100823059678078\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006149513646960258\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012967645889148116\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000913321680855006\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003255539631936699\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007727746851742268\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007625148282386363\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005704002687707543\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005105248419567943\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001468305359594524\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005959856207482517\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014411457814276218\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007135432679206133\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00046251979074440897\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00039173883851617575\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013233234640210867\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008862368413247168\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006921898457221687\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00039990790537558496\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005297872121445835\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008469829917885363\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018592417472973466\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003230497532058507\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001836587762227282\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003200685605406761\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001733453245833516\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00041237034020014107\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00042268625111319125\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006613533478230238\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006862252485007048\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005071852938272059\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014991691568866372\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 5.1260126383567695e-06\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010575877968221903\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006375556695275009\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00033557083224877715\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004102236998733133\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00042179180309176445\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005708177341148257\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00023063697153702378\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008368416456505656\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0023]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0023256144486367702\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00040449932566843927\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00045625842176377773\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013417658628895879\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015766671276651323\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000532649748492986\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004705701721832156\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00027159563614986837\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004442128702066839\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00036502585862763226\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012577371671795845\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006815132801420987\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001040701987221837\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012500943848863244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015891861403360963\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007046558894217014\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00039567428757436574\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00040628816350363195\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001968339056475088\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007409813697449863\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008959014085121453\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002156728587578982\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00030391066684387624\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016917228640522808\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000561931636184454\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004828545788768679\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003120193723589182\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005152956582605839\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017428676364943385\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017870529554784298\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013161017559468746\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016100511129479855\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008528888574801385\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014629927463829517\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005473800119943917\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002503112191334367\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001698280539130792\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00029580199043266475\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007481988868676126\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003426065668463707\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022634444758296013\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006658266647718847\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000727381557226181\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003220361249987036\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006097029545344412\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004013986326754093\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002606850757729262\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003645488468464464\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 7.152560215217818e-07\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002530537312850356\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00043574528535827994\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005614545079879463\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001254160306416452\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020905534620396793\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012189423432573676\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015426872414536774\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005492288037203252\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000292165030259639\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00024142795882653445\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00031142315128818154\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006964246858842671\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000755834043957293\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.9298e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.927732076495886e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0026]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0026054917834699154\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013125804252922535\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.0625e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.064833673415706e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013143112882971764\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007687781471759081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007690167403779924\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001489034912083298\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00046937749721109867\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0027]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0026903548277914524\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00030230084666982293\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00036860344698652625\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020845916878897697\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013763659808319062\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003394464438315481\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006822290015406907\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005829841247759759\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0020]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002038170350715518\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002949076588265598\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00040211417945101857\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001881299540400505\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.3150e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.315193554153666e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0024]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0024008939508348703\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020148399926256388\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00034892684197984636\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017197418492287397\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022074035950936377\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006396431708708405\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005830437294207513\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00047534078476019204\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003601961361709982\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001759087754180655\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006856287945993245\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005682532791979611\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005501233390532434\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010001295013353229\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00027123792096972466\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017924723215401173\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004857169697061181\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00044856593012809753\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019713198707904667\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002573463716544211\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001632704515941441\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00023403522209264338\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004569143638946116\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00043950200779363513\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010952985612675548\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00027302655507810414\n",
      "tensor([[0.9940]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0060569122433662415\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006613533478230238\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00025311336503364146\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000409746658988297\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021960762387607247\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000341175589710474\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002541865105740726\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021364586427807808\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017191456572618335\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022670216276310384\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019021650950890034\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008031970355659723\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001587993756402284\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008566471515223384\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009726245771162212\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022276736854109913\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005244795465841889\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00046198308700695634\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00026008888380602\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020428598509170115\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005074834916740656\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017877030768431723\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00023755272559355944\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003351534833200276\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005991465295664966\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00042674108408391476\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00037218103534542024\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014645933697465807\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003913810651283711\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.9976e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.996406980557367e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018729532894212753\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021012844808865339\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017292801931034774\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00042435587965883315\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004085540713276714\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001375769788865\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004999098600819707\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 7.021673809504136e-05\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001918857597047463\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00033574970439076424\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004412909329403192\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003741487453226\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006510946550406516\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.2289e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.231740503106266e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005561467260122299\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009040149161592126\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004942446248605847\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00035363726783543825\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00035769183887168765\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005119561101309955\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00042369996663182974\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00039120219298638403\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020106667943764478\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008164996397681534\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002432761393720284\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005767220281995833\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00045959779527038336\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004888775874860585\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.3355e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.333076402777806e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007121117669157684\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003988942189607769\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00024804568965919316\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005552521906793118\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00024214337463490665\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003612693981267512\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022574827016796917\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009531747782602906\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011546086170710623\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006760259275324643\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016851653344929218\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004066459368914366\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005358104826882482\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00039227548404596746\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003180413623340428\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004498778434935957\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00033252997673116624\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014723431377205998\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000369974848581478\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00035524717532098293\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005408796132542193\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00024136833962984383\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.8716e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.872652011225e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001164742570836097\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014704542700201273\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003429047064855695\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002995581889990717\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022497323516290635\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015230146527756006\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003107076627202332\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009918361902236938\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002626525529194623\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003336628433316946\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022133653692435473\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004776664718519896\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001811549300327897\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005197683349251747\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.9212e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.923464803141542e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00040825593168847263\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000547618605196476\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006612937431782484\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017620685684960335\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015391103806905448\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003759971586987376\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00042805293924175203\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018157223530579358\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015009575872682035\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00025669054593890905\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001791279937606305\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010604228737065569\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019069343397859484\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.9531e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.953018550528213e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003606731479521841\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005636611604131758\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00048601513844914734\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003629389393609017\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005745153757743537\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000327044544974342\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013400021998677403\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00024595900322310627\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.1364e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.136365067912266e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000769314996432513\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00030242008506320417\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017435879271943122\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018383761926088482\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00034946348750963807\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017090111214201897\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00037313508801162243\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.2707e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.270605081226677e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011856066703330725\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020094744104426354\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003174451121594757\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001675030798651278\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00042525032768025994\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005330671556293964\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.8520e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.8533474657451734e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002839968365151435\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00029580199043266475\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011316979071125388\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012148164387326688\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005563853192143142\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005228694062680006\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00028870697133243084\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001382327318424359\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014222679601516575\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005008640582673252\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00024750910233706236\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005473800119943917\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00023051773314364254\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000123031553812325\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00038959222729317844\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001875934103736654\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004708683118224144\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000376235693693161\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001626743032829836\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.7453e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.744642137666233e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.8445e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.84284750605002e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00045613915426656604\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002122150472132489\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001714972750050947\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002631891402415931\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021346702123992145\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005701020709238946\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002944306761492044\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016732423682697117\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004495796747505665\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013179455709178\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019206460274290293\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.5848e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.586530071217567e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011742804781533778\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010485780658200383\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00030879973201081157\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021281122462823987\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007487357361242175\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004099851648788899\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003826158063020557\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006673774332739413\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019486657402012497\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000280658045085147\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.2981e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.29899334651418e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001998743537114933\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.0993e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.100883597857319e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006260446971282363\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00026634903042577207\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.0490472959645558e-05\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001847914682002738\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005276999436318874\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002435742353554815\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004251906939316541\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00028113502776250243\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015784555580466986\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.576279254957626e-07\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005460680113174021\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00031583529198542237\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004979419172741473\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003889959480147809\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014312099665403366\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002097707474604249\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002899590181186795\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.8892e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.891966379247606e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001483669620938599\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013316563854459673\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019927817629650235\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006800221162848175\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.5937e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.596808376954868e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008364240638911724\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004643683787435293\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019021650950890034\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002933574724011123\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004619234532583505\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018854725931305438\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00026515661738812923\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.7840e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.783238495700061e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013483480142895132\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017781645874492824\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.3856013033073395e-05\n",
      "tensor([[3.9377e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.9399445086019114e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002907937450800091\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018324147094972432\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0016565326368436217\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001881299540400505\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002447666192892939\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020470330491662025\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00025728673790581524\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015778593660797924\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001770414673956111\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018652032304089516\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00030379139934666455\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00034803248126991093\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003303835110273212\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.2159421203250531e-05\n",
      "tensor([[8.1299e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.130403875838965e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00023087543377187103\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00029699443257413805\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.768372718899627e-07\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016726461763028055\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.0094e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.012628288473934e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00045560247963294387\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003087400982622057\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021149964595679194\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00044033685117028654\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001746808411553502\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00042525032768025994\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018532801186665893\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017735583242028952\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019170691666658968\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002639642043504864\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002416068164166063\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012076630082447082\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011778571206377819\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011671270476654172\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.8991e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.897927571320906e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010151186143048108\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019242230337113142\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00026748180971480906\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.2029e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.2036208217032254e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002337967453058809\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003337821108289063\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016958959167823195\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.9641e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.9651902372715995e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010377707076258957\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004025315574835986\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00032585207372903824\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022926574456505477\n",
      "tensor([[0.7982]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.22541265189647675\n",
      "tensor([[0.0021]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0021303321700543165\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011711419792845845\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00042089735507033765\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0025]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0025344991590827703\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000516011321451515\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006800817791372538\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008403612882830203\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 7.152560215217818e-07\n",
      "tensor([[0.0021]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0021110987290740013\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004851802659686655\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008070148178376257\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0027]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002678461605682969\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014896157663315535\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014476521173492074\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008272373233921826\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00036931896465830505\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014265814097598195\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022986192198004574\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012522428296506405\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0027]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002659277291968465\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003111250407528132\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008543205913156271\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002851296740118414\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009510866366326809\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010729822097346187\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0053]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0053052715957164764\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00034487232915125787\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00044963930849917233\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008267600787803531\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0025]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0025050996337085962\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016493965813424438\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00033390134922228754\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020750529074575752\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013026731321588159\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0020]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0020286142826080322\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0030]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003020133823156357\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017894915072247386\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007882243953645229\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001303807133808732\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006145935039967299\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004603730340022594\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014276558067649603\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003890555817633867\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004878637846559286\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005015796632505953\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004691986250691116\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011456612264737487\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008586754556745291\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0016717571998015046\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003517888835631311\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0018531555542722344\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008376768091693521\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020726682851091027\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005315166199579835\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005870396271348\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004759371222462505\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005098688998259604\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021877296967431903\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005744557129219174\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011842703679576516\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011351588182151318\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021400356490630656\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014866502897348255\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017378521151840687\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007162870606407523\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004997309879399836\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00046055190614424646\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00038386796950362623\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005831630551256239\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009328900487162173\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006370785413309932\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012714001350104809\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009619450429454446\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007045366219244897\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004023526853416115\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007328095380216837\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005543575971387327\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000626700755674392\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001951511250808835\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010322293965145946\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0021]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0021438912954181433\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007613814668729901\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005121350404806435\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00042030104668810964\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006171580753289163\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005766027024947107\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00034403757308609784\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005480360123328865\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000649007095489651\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011338461190462112\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003987749805673957\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012443652376532555\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008354099118150771\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0018473631935194135\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00077134306775406\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000276424951152876\n",
      "counter: 8000\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006458460120484233\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005068871541880071\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013584821135737002\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008673255797475576\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00033217223244719207\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006982737104408443\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0026]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0025551151484251022\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0021]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002124777063727379\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022723872098140419\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010675761586753651\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00033694220473989844\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005853100446984172\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003728965821210295\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004474925808608532\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010202962439507246\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012503927573561668\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00035954025224782526\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003959128225687891\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011295496951788664\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017324783839285374\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005102267023175955\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001475886325351894\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005071256891824305\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00040193527820520103\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015650114510208368\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022205195273272693\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00039728425326757133\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00023594302183482796\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000498240115121007\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00025818104040808976\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00045590061927214265\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004044396919198334\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012601216440089047\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00045423093251883984\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011233438272029161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0055]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0055627282708883286\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015949206426739693\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003201281651854515\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004819600726477802\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011087078746641055\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00046997383469715714\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00030540121952071786\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010570507729426026\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008083868306130171\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012098713777959347\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017355234595015645\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012899277498945594\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022431743855122477\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003574533329810947\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013402140466496348\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009097421425394714\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017378521151840687\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002002916589844972\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002953846415039152\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005024741403758526\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001063999516190961\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006863445742055774\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003368229663465172\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005114790401421487\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008475795038975775\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015402371063828468\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007462304783985019\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006286689313128591\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003303835110273212\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005532841314561665\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004179158713668585\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001117137959226966\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018926266056951135\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00035566455335356295\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003610905259847641\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004932904848828912\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016207816952373832\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00045071265776641667\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00043037853902205825\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008241949835792184\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00044999708188697696\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015659366908948869\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013960382784716785\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003516696160659194\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004734921676572412\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00028131387080065906\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007215360528789461\n",
      "tensor([[0.0407]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.201432228088379\n",
      "tensor([[0.1856]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.2052917629480362\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0905]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.09487652778625488\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0174]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.017503749579191208\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0410]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.04190051183104515\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.1268]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.1355733871459961\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0140]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.014131687581539154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0115]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.011578274890780449\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0173]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.017429446801543236\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0096]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.009676933288574219\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0230]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.023273082450032234\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0070]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0070129395462572575\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0131]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.013136825524270535\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0520]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.05339227244257927\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0046]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00458515202626586\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0032]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0032445916440337896\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0126]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.012721167877316475\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0047]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0047334227710962296\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014808408450335264\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0063]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.006312882993370295\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0107]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0107367979362607\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0026]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002623718697577715\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0035]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003505466040223837\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010439240140840411\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0061]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.006084437482059002\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0033]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0032609167974442244\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0104]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.010504568926990032\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0039]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003863397054374218\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0020]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0020486225839704275\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0026]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002642424078658223\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0069]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.006952196825295687\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0157]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.015824653208255768\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0020]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0020503546111285686\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0201]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.020315881818532944\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0086]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00862209964543581\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0021]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0021015419624745846\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000811548437923193\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008966769091784954\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009074154659174383\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006229433347471058\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0026]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002627901965752244\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0028]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0028208307921886444\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0065]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.006549720652401447\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0022]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002182897413149476\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008885037968866527\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011276998557150364\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0021]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002105722902342677\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001805444248020649\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001827299129217863\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009167222888208926\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0022]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0022478315513581038\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0029]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002912287600338459\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0044]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0044449265114963055\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017424497054889798\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001863068318925798\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0025]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0024801227264106274\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0022]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002239468041807413\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005244199419394135\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001462813699617982\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 9.536747711536009e-07\n",
      "tensor([[0.0022]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002210077363997698\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0027]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002716711489483714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010319310240447521\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006633216398768127\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0040]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0040292744524776936\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015735483029857278\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0037]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003688214346766472\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0027]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002740020863711834\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0035]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0034761575516313314\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005635418347083032\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0030]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002962921280413866\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0043]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004321481566876173\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011303850915282965\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.5497327694902197e-05\n",
      "tensor([[0.0029]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002896625781431794\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001009616069495678\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001553251058794558\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0054]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005390244070440531\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013129982398822904\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009870630456134677\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0019111406290903687\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007611428736709058\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0020]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002001379616558552\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0022]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002205716446042061\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001066001015715301\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000521378533449024\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0016768918139860034\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007210588664747775\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008932167547754943\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0022]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0021885724272578955\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014570832718163729\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007202834240160882\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011372474255040288\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005913933273404837\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007642447017133236\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0030]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003014274872839451\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00045285941450856626\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0029]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0029072065372020006\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005873378249816597\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0024]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002376397605985403\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007120521040633321\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0021]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0020964648574590683\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017793502192944288\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0020]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001962977694347501\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017496147193014622\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005567431217059493\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0019393282709643245\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011279385071247816\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004101044323761016\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010131363524124026\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014156581601127982\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014981519198045135\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001235950505360961\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00039495876990258694\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011317575117573142\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0028]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0027788111474364996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001169769442640245\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008287883247248828\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017024456756189466\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0084]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.008409775793552399\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002432165201753378\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008267600787803531\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012267003767192364\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013692211359739304\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015232834266498685\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006559854373335838\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007778450381010771\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002539480337873101\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00027988298097625375\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0024]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0024273027665913105\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012076633283868432\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007622165721841156\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022950422135181725\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011761545902118087\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013969159917905927\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003997886669822037\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007437252206727862\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.576279254957626e-07\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005206032656133175\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009901059092953801\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010065732058137655\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001536714960820973\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001093926141038537\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006512735853902996\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0018679052591323853\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0016591596649959683\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006665423861704767\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001001919386908412\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004598959640134126\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003544720239005983\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013816957361996174\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009844974847510457\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003297872608527541\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0034]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0033927829936146736\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0034]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0034191582817584276\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00023159086413215846\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012740259990096092\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010363463079556823\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00032489807927049696\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00037552014691755176\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00024309728178195655\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006402395665645599\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.768372718899627e-07\n",
      "tensor([[0.0024]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0024526966735720634\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007918631308712065\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015112250111997128\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0040]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004034481011331081\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013511960860341787\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006333210621960461\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011939378455281258\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0016890715342015028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007367462967522442\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008797938353382051\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005630051018670201\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00033771732705645263\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0037]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003734280588105321\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008318903273902833\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002478072128724307\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004247136530466378\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00040628816350363195\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007632306660525501\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005294293514452875\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015841149725019932\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002460782416164875\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0022]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0022098382469266653\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011127223260700703\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0016699660336598754\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002496554225217551\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011569395428523421\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007544620893895626\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005569220520555973\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007031050627119839\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013941703364253044\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0022]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002195979468524456\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008425088599324226\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00034946348750963807\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007329288055188954\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003483902255538851\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005978344124741852\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00035667818156071007\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008026005234569311\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003028970677405596\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00024691291037015617\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009719086228869855\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007280376739799976\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0020]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0019595737103372812\n",
      "tensor([[0.9935]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0065199630334973335\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009982798947021365\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001874712877906859\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013278594706207514\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0036]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003647953039035201\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009217336773872375\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016034934378694743\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00023630072246305645\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006891479133628309\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011546086170710623\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004622812557499856\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009951177053153515\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000649364956188947\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001779409940354526\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00024017595569603145\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022533093579113483\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013085220707580447\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009085490019060671\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006199612398631871\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00029377485043369234\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002479860559105873\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005522106657736003\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0027]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002698064548894763\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0029]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0029460033401846886\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00036592024844139814\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000384166109142825\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001821683836169541\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00043228670256212354\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009768606396391988\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018407608149573207\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015901445876806974\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015845329035073519\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017817984335124493\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007718203123658895\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000927222368773073\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.768372718899627e-07\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000562468369025737\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005957470275461674\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010522177908569574\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009452994563616812\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000557220249902457\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008392278105020523\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005875166971236467\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005159516585990787\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000882060790900141\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00042101662256754935\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00024023557489272207\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0018620530609041452\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013692124048247933\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002539480337873101\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018389723845757544\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0021]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0020522659178823233\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 8.344653679159819e-07\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001503342209616676\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001540302619105205\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018246646504849195\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00026354685542173684\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001723104272969067\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002911514602601528\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002205018827226013\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00045506577589549124\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003533987619448453\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003772493510041386\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00027892901562154293\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002551404177211225\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001644031290197745\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002503112191334367\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00034695921931415796\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013144903350621462\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006769205792807043\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00038440461503341794\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002754113811533898\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013072090223431587\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010115419718204066\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00041553069604560733\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004047378315590322\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005831033922731876\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006231818697415292\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002590157091617584\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006693457253277302\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009223302477039397\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010598267544992268\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 5.69836629438214e-05\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009395123925060034\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019474733562674373\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004691986250691116\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008611810044385493\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.5848e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.583702320815064e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004973455797880888\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00107065518386662\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005668219528160989\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006899232976138592\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00039406432188116014\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00045244197826832533\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003340802213642746\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009193472797051072\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015634592855349183\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006543154013343155\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021144002676010132\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003658606146927923\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012962274486199021\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005958066903986037\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001833010755944997\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00024458777625113726\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00035584342549555004\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00045315755414776504\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008764530648477376\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00044588252785615623\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004660380946006626\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004471347783692181\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00043592415750026703\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018210876442026347\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005922879208810627\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00048279494512826204\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012946967035531998\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0018911948427557945\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007573252660222352\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016684731235727668\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000647575652692467\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00032960838871076703\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000122912329970859\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019486657402012497\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007217746460810304\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002089361078105867\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00024369347374886274\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005045613506808877\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005409392178989947\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020637256966438144\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003073091502301395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.5864e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.584885992808267e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021609017858281732\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003265675622969866\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016499926277901977\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00024733025929890573\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00026944928686134517\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001675030798651278\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00060558773111552\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003280581731814891\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.372293056396302e-05\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011245969217270613\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006834219093434513\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001822280028136447\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00040068308589980006\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.8064e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.805445006350055e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013149649021215737\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00036192528204992414\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001744183973642066\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004055130120832473\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006602201028726995\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00029908123542554677\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000751777901314199\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0050]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005039973650127649\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016285314632114023\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002160305593861267\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005667026853188872\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002528748591430485\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017358378681819886\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00032859478960745037\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00036574137629941106\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003501789760775864\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001698280539130792\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003654432366602123\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022825223277322948\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002599696454126388\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.3434e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.346444858238101e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022908688697498292\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010537691414356232\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005000291275791824\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.0005e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.0009548431262374e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00046890045632608235\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.8849064619862475e-05\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000823359820060432\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017316648154519498\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001857453171396628\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005433843471109867\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005946735036559403\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.4263e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.425587343983352e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003299661329947412\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010466090170666575\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003151794080622494\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005650328239426017\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00042208994273096323\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007708062184974551\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001918857597047463\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007481988868676126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009923134930431843\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010878439206862822\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001167501788586378\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015878163976594806\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00029365558293648064\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022217119112610817\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000971431378275156\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001663107832428068\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003015853581018746\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00070513307582587\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010371745884185657\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021310930605977774\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00027553061954677105\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000586860696785152\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.218656047465629e-06\n",
      "tensor([[4.6136e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.613506098394282e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.7756e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.775639773579314e-05\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 7.951575389597565e-05\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003790381597355008\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.9609e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.963497773744166e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.7378e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.735551869496703e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015838208491913974\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00035423351801000535\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00024560128804296255\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008948275353759527\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013489440607372671\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001253564259968698\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006291460595093668\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021376510267145932\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006877760170027614\n",
      "tensor([[0.9215]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.08178061246871948\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004436165327206254\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014484877465292811\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005567431217059493\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006702403770759702\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012224633246660233\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009104580385610461\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013256510719656944\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00038834006409160793\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000543086149264127\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001033900072798133\n",
      "counter: 9000\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007486761314794421\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002933574724011123\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.2923e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.289913628483191e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006085697677917778\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013663562713190913\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007336445851251483\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003255539631936699\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020315326401032507\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022169425210449845\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00036890158662572503\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.9270e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.924665937433019e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00025168247520923615\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006601008353754878\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008259249152615666\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020219940051902086\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00043860755977220833\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00030373179470188916\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007527322741225362\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000163806980708614\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001346559583907947\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.8361e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.838317833375186e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018789149180520326\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004841068875975907\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006609358824789524\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00039042701246216893\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00025430574896745384\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005107634351588786\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005587111809290946\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003090382379014045\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00028357948758639395\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010476233437657356\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004824967763852328\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00030474536470137537\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009454187820665538\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00024148757802322507\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00046466654748655856\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001532552851131186\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.7418175250204513e-06\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003412351943552494\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012653723824769258\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003782033745665103\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005428476142697036\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015766671276651323\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010860556358238682\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003795151715166867\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0023]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0023435375187546015\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00044164873543195426\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011057272786274552\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00023594302183482796\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005055751535110176\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012992712436243892\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014164939057081938\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015367257583420724\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004952584276907146\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002215153945144266\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00042536959517747164\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003851201618090272\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012732362665701658\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00038440461503341794\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012786014121957123\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003205455432180315\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012815819354727864\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014526706945616752\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004076000186614692\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003262098180130124\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003372403443790972\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011510915355756879\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00044635956874117255\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002599696454126388\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00024858227698132396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00025561737129464746\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00026968776364810765\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00024482625303789973\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021728253341279924\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00031345034949481487\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001518245553597808\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005121946451254189\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001913492160383612\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00024279918579850346\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00036281967186369\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004141592071391642\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00023767196398694068\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005489306058734655\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002610428200569004\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000341175589710474\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00031667001894675195\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020607448823284358\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001657742541283369\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00030587820219807327\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016696655075065792\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00033616708242334425\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004517264023888856\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003892344539053738\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010985739208990708\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006980947800911963\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003437394625507295\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00023719501041341573\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003129137330688536\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005856082425452769\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005136855179443955\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004890564596280456\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011885872663697228\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013727891200687736\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.4758e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.476139919366688e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000292165030259639\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000557041319552809\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022312506916932762\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008183488971553743\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013352332462091\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006334999925456941\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 8.344653679159819e-07\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011608779896050692\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000489652797114104\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00033646522206254303\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002296234597451985\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010502487421035767\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009176171733997762\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.2159421203250531e-05\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000212632367038168\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00040968702523969114\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009226285619661212\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008310551638714969\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021066500630695373\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017048382142093033\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002182960306527093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012380651605781168\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011858816724270582\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020875725022051483\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017286841466557235\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004414698341861367\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000388518936233595\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003611501306295395\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015761153772473335\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010002488270401955\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020458406652323902\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005730243865400553\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.5831e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.5823031794279814e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002788097772281617\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000290614872938022\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00028817038401030004\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007121117669157684\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006982141057960689\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00023749310639686882\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007333463872782886\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004200625407975167\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012223439989611506\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019206460274290293\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014908233424648643\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020005319674964994\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00034654184128157794\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.57628505298635e-06\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002716552698984742\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006227047415450215\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001467574038542807\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.3331e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.3333515350241214e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016034934378694743\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006209154962562025\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004620427207555622\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.5582e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.561041952110827e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007313183159567416\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005961644928902388\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005655099521391094\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013788307551294565\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004844050563406199\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003059378359466791\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003041491436306387\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.1984e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.195974805857986e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005507197347469628\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.0014e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.000706631923094e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005711755948141217\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015850130876060575\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000297232938464731\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001338213769486174\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00023099467216525227\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003161334025207907\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016958959167823195\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000791028025560081\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020678988948930055\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021132078836672008\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002684357459656894\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019021650950890034\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00028405647026374936\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00045142826274968684\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005260300822556019\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005613352404907346\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.8863e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.888899512588978e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00023975862131919712\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000290614872938022\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.7895e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.787561430130154e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019820508896373212\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007454550359398127\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017775685410015285\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019301846623420715\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010055809252662584\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007666306919418275\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.1717e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.173817044124007e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002729669213294983\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014628049393650144\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00034051970578730106\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 7.510213436034974e-06\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019075305317528546\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002910918556153774\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011051311594201252\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017525300791021436\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005169654614292085\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016368775686714798\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00024715138715691864\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005519124679267406\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002588368661236018\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013405982463154942\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 8.905330469133332e-05\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003061167080886662\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008562295697629452\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001262506120838225\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012911199883092195\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015784555580466986\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001806183863664046\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007560726371593773\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.3457e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.344998786924407e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000163806980708614\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001775183918653056\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00047599675599485636\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002503112191334367\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00030671292915940285\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013203300477471203\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002918669197242707\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006088083027862012\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022568865097127855\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004981805104762316\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.9502e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.951575389597565e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021424204169306904\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002391028101556003\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002532326034270227\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003640122013166547\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003259117074776441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00028972054133191705\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014383635425474495\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003576322051230818\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00026801839703693986\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001401999470544979\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020619372662622482\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014531436609104276\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013018501340411603\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014711508993059397\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00023701615282334387\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022998116037342697\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017012612079270184\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016696655075065792\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00035113299963995814\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012935044651385397\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010074084857478738\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017340494378004223\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002893627970479429\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015790517500136048\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001021675780066289\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019117037300020456\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015921668091323227\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005321130156517029\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018037992413155735\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.5319e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.531236719340086e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016833769041113555\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011724547948688269\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016947036783676594\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.6367e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.638758375425823e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016976843471638858\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000595150631852448\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016338967543561012\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00024303766258526593\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005814931355416775\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020619372662622482\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011319562327116728\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014538629329763353\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014091530465520918\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.7551e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.757974577136338e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022616558999288827\n",
      "tensor([[0.9998]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.00024351461615879089\n",
      "tensor([[7.5027e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.504506356781349e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012845626042690128\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003836294636130333\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006564625655300915\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014222679601516575\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005687900702469051\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.3074e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.306370050879195e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001954627368832007\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020488214795477688\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00039567428757436574\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00034815171966329217\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004992539179511368\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001286351034650579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010395590652478859\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001669069315539673\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00030420877737924457\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.8483e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.850534267257899e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00036902082501910627\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000688789994455874\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007388340309262276\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010628072777763009\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 9.89442014542874e-06\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003560222976375371\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00023993747890926898\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000627595407422632\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003844642487820238\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00029979669488966465\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017572993237990886\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005230483366176486\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005396272172220051\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005095110973343253\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007777257123962045\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.7268e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.725060277152807e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.6558e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.65641884249635e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003167892573401332\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009912991663441062\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009924924233928323\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.7918e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.793522622203454e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018198954057879746\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003213802701793611\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.7137e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.716029308037832e-05\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 7.796591671649367e-05\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005479167448356748\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009322338155470788\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002903167624026537\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00026712409453466535\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012231621076352894\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018342031398788095\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003023604513145983\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.3144e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.312331242952496e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014419402577914298\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010413473501102999\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.7149e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.71457887114957e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.9984e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.999263470992446e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00032161877606995404\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018389723845757544\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00035548568121157587\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012827741738874465\n",
      "tensor([[0.9953]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.004748394712805748\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010932089207926765\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.7452e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.745834540808573e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001490227150497958\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.0065e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.0086967197130434e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00023224666074384004\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00035083486000075936\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002718937466852367\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000345885957358405\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00025072856806218624\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00023653919924981892\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001821683836169541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021865374583285302\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00040259119123220444\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00037355246604420245\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00034862873144447803\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014634011313319206\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003071302780881524\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020088783639948815\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013459633919410408\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.1901e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.190013613784686e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.8532e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.851681195665151e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017352418217342347\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002028551825787872\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001384115603286773\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010949972784146667\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00025841951719485223\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004656802921090275\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00033121826709248126\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012911199883092195\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002820889640133828\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00023331979173235595\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00024142795882653445\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00023665843764320016\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002066706510959193\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022074035950936377\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00044194687507115304\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000232723614317365\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005191719974391162\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006657670601271093\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013101959484629333\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00023820853675715625\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003683649410959333\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00033557083224877715\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.1756e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.173817044124007e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004209569888189435\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.6097e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.608954234048724e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003699152439367026\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010282330185873434\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017722031043376774\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.4754e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.47327397018671e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021883258887100965\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.9046e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.9027884011156857e-05\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 8.821526535029989e-06\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019564157992135733\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.9828e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.982823055703193e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.6569e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.6566408602520823e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010556539928074926\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.8635e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.863603579811752e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011045350402127951\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.4307e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.428723332239315e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.3031e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.303548121242784e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003234671021346003\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018455300596542656\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00046478581498377025\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004431991255842149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.3113030945532955e-06\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00028852809919044375\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00027690190472640097\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.4150e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.416530181420967e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00030426838202401996\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013906729873269796\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010383668268332258\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007105012773536146\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001318541617365554\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00037969404365867376\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010312135418644175\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001709607313387096\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.4066e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.406055323779583e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.9721e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.9723540188279e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.1832e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.181193020893261e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000650259607937187\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017119919357355684\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000188487654668279\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005844751140102744\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011718960013240576\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007389532984234393\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.3248e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.324252899503335e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003658606146927923\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004539327637758106\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00024804568965919316\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001847914682002738\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00036389296292327344\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013966343249194324\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.4671e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.468740659533069e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005170251242816448\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.6577e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.6566408602520823e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.4790e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.482100383844227e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011432824248913676\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00026444115792401135\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.3327e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.33163979020901e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015301682287827134\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016476081509608775\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005088550969958305\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001731068769004196\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008577806293033063\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.7236e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.7207991883624345e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010180991375818849\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015575907309539616\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 9.584885992808267e-05\n",
      "tensor([[9.2556e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.25702988752164e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004773683031089604\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014365751121658832\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.3052e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.303548121242784e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016607233555987477\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002065514272544533\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002743978111539036\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004099851648788899\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.6882e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.689294579904526e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004232229257468134\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012237582996021956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004105218395125121\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001918857597047463\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00029896196792833507\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003122578782495111\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022950422135181725\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018753379117697477\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001732261007418856\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.8464e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.8473866374697536e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012124319619033486\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002002916589844972\n",
      "tensor([[0.9954]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.004566709976643324\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00031875682179816067\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021269198623485863\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015027460176497698\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005683129420503974\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.9868e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.990237595047802e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010616150393616408\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.2074e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.20503633399494e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001748953218339011\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010043886868515983\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.3129e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.313757669180632e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000210188067285344\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007032840512692928\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002816119813360274\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012911199883092195\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010944011592073366\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005018181982450187\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.768382950715022e-06\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012225660611875355\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017394148744642735\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020714759011752903\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003203070373274386\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.3917e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.39124880055897e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017567031318321824\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005138047854416072\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00030045254970900714\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017739915347192436\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.8917e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.893408812582493e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.4206e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.421053305733949e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006023074965924025\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000325315457303077\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.3021e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.3049541747896e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017459724040236324\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013668279279954731\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002121554280165583\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.2386e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.237700967583805e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018389723845757544\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022324430756270885\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006622479995712638\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841887468734058e-06\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010365784692112356\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.8184e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.819004192948341e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002571078948676586\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.4837e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.486623508157209e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00023224666074384004\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021000920969527215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002121554280165583\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012780052202288061\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.8656e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.865269122296013e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00032865439425222576\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018103569163940847\n",
      "counter: 10000\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.2103e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.209341806126758e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010473084694240242\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011075156362494454\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004417679738253355\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.7866e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.787778354715556e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017507416487205774\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.5660e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.564115367131308e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000147591985296458\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.3474e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.349837061250582e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.0328e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.032171950209886e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016166086425073445\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002205615019192919\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001408557000104338\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018151261610910296\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.7442e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.745834540808573e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018872611690312624\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015677249757573009\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00027248996775597334\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016422428598161787\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012017018161714077\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002556770050432533\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.9436e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.945905336877331e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022908688697498292\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.0857e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.088677714113146e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015522254398092628\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001753722463035956\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00024983426555991173\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002215750137111172\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.5987e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.599881064379588e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.7807e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.783238495700061e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00032412298605777323\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000285547022940591\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000111645735159982\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016392620455008\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018437416292726994\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00030092953238636255\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009064609184861183\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.2490e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.249623351730406e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001408557000104338\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00031076729646883905\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.3487e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.34952263883315e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013727891200687736\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.8164e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.819339351844974e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000251742108957842\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00036061351420357823\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014842658129055053\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002670048561412841\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00025835991255007684\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021066500630695373\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00024744949769228697\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013894807489123195\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.1219479094725102e-05\n",
      "tensor([[6.1152e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.115623546065763e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013626550207845867\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.7248e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.723630212945864e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004785013443324715\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022145577531773597\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001600512769073248\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.9907e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.9921979148639366e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019564157992135733\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010043886868515983\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.6075e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.605842256452888e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.9857e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.9862372684874572e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000569982803426683\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003485690976958722\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016774154209997505\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00031971081625670195\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00025841951719485223\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002073860669042915\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.00011397057824069634\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020833993039559573\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007610832108184695\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001727491762721911\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001366231736028567\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015957436698954552\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011331484711263329\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00027916752151213586\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00027362274704501033\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.2659e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.264644616749138e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001382923364872113\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012118358426960185\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012786014121957123\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017602801381144673\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001435978920198977\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003688419528771192\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00026766068185679615\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016225701256189495\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002759479684755206\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002387450949754566\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.0017e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.002367808832787e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.6319e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.63257480179891e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019397232972551137\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.8716033082455397e-05\n",
      "tensor([[3.7189e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.71939895558171e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.6064e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.604412919841707e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.8861e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.887447620509192e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021149964595679194\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.3700e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.370522558223456e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000224436676944606\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010753256356110796\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001256544783245772\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013990188017487526\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015456679102499038\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00034892684197984636\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.0056e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.0041192076168954e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011373213055776432\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011873950279550627\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011122845171485096\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.7935e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.79207145771943e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00030045254970900714\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012672750744968653\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015498408174607903\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013912691792938858\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016726461763028055\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013659981777891517\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.0080e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.008328637108207e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011623580940067768\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006630233838222921\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018079722940456122\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016947036783676594\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014157105761114508\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.1715e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.171017306158319e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.8535e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.8533474657451734e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010926128015853465\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013698085967916995\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 6.198902156029362e-06\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014818811905570328\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001253564259968698\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018502993043512106\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.6835e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.683334115426987e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.5984e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.5970329412957653e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.8893e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.890534132253379e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013113270979374647\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.8455e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.845720731187612e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.8972e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.897927571320906e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.3265e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.324252899503335e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000296338606858626\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00035357766319066286\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.5535e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.552499765646644e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.8053e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.804249692824669e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018192992138210684\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001204682412208058\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 5.483642325998517e-06\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003700941160786897\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.1009e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.100883597857319e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019331654766574502\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021394396026153117\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.2558e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.255583816207945e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001495592441642657\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00024899959680624306\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.8105e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.808513328200206e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.3263e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.324252899503335e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001533149043098092\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.6430e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.64449645834975e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.4256e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.425938509986736e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020130514167249203\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00037140591302886605\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011569930939003825\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019283962319605052\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015015537792351097\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.1147e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.115623546065763e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016821845201775432\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010997661593137309\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011832221935037524\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.5968e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.596808376954868e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.0651e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.063399971229956e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013865000801160932\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.3576e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.358601265470497e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015295721823349595\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011444746633060277\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00023791044077370316\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.4023e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.40317118470557e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010407512309029698\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00029896196792833507\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001972512254724279\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005106441676616669\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.4305124977909145e-06\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010151186143048108\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.3492e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.352406050311401e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012577371671795845\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.3683e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.370288898935542e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003779648686759174\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011444746633060277\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018586455553304404\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019766853074543178\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002334986493224278\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.7286e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.727950964588672e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.4521e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.453973790281452e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.9000e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.902455788804218e-05\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.7313202483346686e-05\n",
      "tensor([[9.4085e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.406055323779583e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003234671021346003\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018240684585180134\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 5.960466182841628e-07\n",
      "tensor([[6.3176e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.318291707430035e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.8734e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.871229950571433e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002519209519959986\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00025985040701925755\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010979778016917408\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00027833282365463674\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.5617e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.562686758115888e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021680559439118952\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010657878738129511\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017960491823032498\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005466047441586852\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001408557000104338\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003883996687363833\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006719104130752385\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.1384e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.139829666703008e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016034934378694743\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000150393825606443\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.6349e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.634216697420925e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.2916e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.288844552822411e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021948838548269123\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.1271e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.124726547161117e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020434560428839177\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013489440607372671\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.1668e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.1696325347875245e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.7764e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.7762342395144515e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.9898e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.9921979148639366e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.1595e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.15909601340536e-05\n",
      "tensor([[0.9977]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0023111565969884396\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001254756498383358\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012279310612939298\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.0696e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.072238754015416e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010920166823780164\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002888858434744179\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.9459e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.944181950530037e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010180991375818849\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019081267237197608\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.6834e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.684773638378829e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.0231e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.024797792430036e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005466644070111215\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.6232e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.6240282497601584e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.8334e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.835250239120796e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.1293e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.130687375436537e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00033372247708030045\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011462630209280178\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.6800e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.680483809555881e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010932089207926765\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003412351943552494\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010186952567892149\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.5572e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.559592970414087e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014139221457298845\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017018573998939246\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.2657e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.2632287406595424e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015009575872682035\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004325848422013223\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.6951e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.69669602252543e-05\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841887468734058e-06\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001656550302868709\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000294371071504429\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019498579786159098\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00028477192972786725\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022419820015784353\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.9642e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.963497773744166e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010049848060589284\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.3903e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.39124880055897e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005953891668468714\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004917400074191391\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.9565e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.958979742601514e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001063403396983631\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014407480193767697\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002201441820943728\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00037033262196928263\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006493053515441716\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014574397937394679\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.8733e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.875778904533945e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003790381597355008\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.8843e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.881739732809365e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.6353e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.634216697420925e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00025490194093436003\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.4967e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.49569922441151e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.9618e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.960432362277061e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.0219e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.024550672620535e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.1846e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.184333738521673e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013113881868775934\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.6083e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.60614612873178e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00025263638235628605\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00039895385270938277\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.9185e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.91891621297691e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001347751822322607\n",
      "tensor([[0.9966]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0033899121917784214\n",
      "tensor([[6.8206e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.819004192948341e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00028059844044037163\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.8137e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.815915498416871e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.5739e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.576037023682147e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.6556e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.658060738118365e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.4169e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.416530181420967e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.7629e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.7639354054117575e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.3326e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.334523201687261e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011969328625127673\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019331654766574502\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012851586507167667\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.3369e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.334523201687261e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016601271636318415\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.9073504518019035e-06\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010473084694240242\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003099922032561153\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.3551e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.358367242384702e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016786076594144106\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.4731e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.474701851606369e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013602705439552665\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.2367e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.237700967583805e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.1793e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.179777872399427e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011116883979411796\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00036359482328407466\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.576279254957626e-07\n",
      "tensor([[4.1909e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.190294202999212e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.0399e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.039556658128276e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.4650e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.464217535220087e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007301850127987564\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011170533980475739\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001785914646461606\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002226481301477179\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.218656047465629e-06\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015927630010992289\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017924723215401173\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.0029e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.002367808832787e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00025841951719485223\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00038201952702365816\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010926128015853465\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00024977466091513634\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010270407801726833\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019492617866490036\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002518017136026174\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010926128015853465\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.7308e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.729590677423403e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00025335184182040393\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00037158478517085314\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.5384e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.540578472893685e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003570359549485147\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.7100e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.707477662828751e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014228641521185637\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.6851e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.6836347135249525e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.6520e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.6492703404510394e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.2504e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.252722960198298e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012195853923913091\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.6587e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.659792128019035e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004680655838456005\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006535400170832872\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.2457e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.2457448065106291e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012076630082447082\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012130280811106786\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004339563602115959\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.9544e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.956104334676638e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014681702305097133\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.3416e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.339312363299541e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.1909e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.1931146774441e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.9873e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.988784247776493e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022002494370099157\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.1159e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.113965643336996e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.9339e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.933693268569186e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012118358426960185\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001296485133934766\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.4211e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.4199778636102565e-05\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 8.583106136939023e-06\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010616150393616408\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.9727e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.972563667455688e-05\n",
      "tensor([[0.9992]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0007971721352078021\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000210188067285344\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015319566591642797\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.1316e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.130403875838965e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015170533151831478\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004929922870360315\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010955933976219967\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001227335014846176\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015009575872682035\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00034815171966329217\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013388099614530802\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.8465e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.84880797052756e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.3623e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.364562093745917e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002354064490646124\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002645007916726172\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012660828360822052\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.5716e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.571515354560688e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.2790e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.276566273299977e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014592282241210341\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013310603389982134\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001021675780066289\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017775685410015285\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010997661593137309\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 7.152560215217818e-07\n",
      "tensor([[3.2438e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.242545426473953e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000229384982958436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.9633e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.9651902372715995e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.0824e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.08300147682894e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011903756239917129\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.9660e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.966393554350361e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00023582378344144672\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001483669620938599\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.3971e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.398647332796827e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.7641e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.765356374671683e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013584821135737002\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.0240e-06]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.046659786486998e-06\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022592712775804102\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017298763850703835\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.7055e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.705747364321724e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00023254475672729313\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.6687e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.666890789754689e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.0397e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.042433521244675e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015647443069610745\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.7252e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.723969373619184e-05\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[9.7890e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.787561430130154e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001620185503270477\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.7835e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.7835756554850377e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014949962496757507\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.1766e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.178372910246253e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.4888e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.48973839613609e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017406071128789335\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.743241904885508e-05\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017632608069106936\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.3842e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.386998261907138e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.9085e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.908416980877519e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014210757217369974\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00025090741110034287\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.6493e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.651062302698847e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.7589e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.759395182598382e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.9169e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.917503974866122e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.1497e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.1485694964649156e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.4132e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.416530181420967e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.6108e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.611802720930427e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00029514613561332226\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003127944946754724\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.3928e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.39124880055897e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.1519e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.149731340585276e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.7587e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.757756197359413e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00026986663579009473\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003886382037308067\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.8397e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.837221290974412e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017608761845622212\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001703049783827737\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010866516822716221\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.2310e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.229238634754438e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.3695e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.371939525706694e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.8419e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.840014298679307e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003108269302174449\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017930685135070235\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012583332136273384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001383519556839019\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014574397937394679\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.2469e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.248187466757372e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.3428e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.342490006412845e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.5501e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.552499765646644e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.7700e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.771654362732079e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017990299966186285\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.1997e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.19907514192164e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011969328625127673\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001581436226842925\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[5.0622e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.060562398284674e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021537477732636034\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015713018365204334\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00026151977363042533\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.7433e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.744642137666233e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.6600e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.662380034569651e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.6259e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.625427391147241e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.7175e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.7148387238848954e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.7699e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.768485450767912e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.7698e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.768485450767912e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003690804587677121\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013161571405362338\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012726400746032596\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.4261e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.425587343983352e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.1631e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.1604907892178744e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.7632e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.7656937163555995e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004192873602733016\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022312506916932762\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.4357e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.438936154358089e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001490227150497958\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013453673454932868\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.7505e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.748904317850247e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000234094841289334\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.2654e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.26299107959494e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00023612187942489982\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013489440607372671\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011397057824069634\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.3561e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.355483830906451e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016660886467434466\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010932089207926765\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.0155e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.0142894653836265e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.2927e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.2948051991988905e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.8804e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.881739732809365e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005017585353925824\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.3893e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.386998261907138e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.7905e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.7923284000717103e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006856287945993245\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.2108e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.209581649978645e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010312135418644175\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.5992e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.601319859735668e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016553579189348966\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.4199778636102565e-05\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011742804781533778\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013608665904030204\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.7473e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.749202369363047e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.0887e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.087243284331635e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.8950e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.893408812582493e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.1966e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.1994355847709812e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017888953152578324\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016154164040926844\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011617619747994468\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.7683e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.768485450767912e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001100958397728391\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004094485193490982\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010717489203670993\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.4921e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.49115681881085e-05\n",
      "tensor([[0.9998]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.00016094549209810793\n",
      "tensor([[3.0602e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.0577652069041505e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012923122267238796\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012577371671795845\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018747418653219938\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002848315634764731\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014127299073152244\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.3982e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.400094131706282e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014705547073390335\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.8389e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.841211431194097e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012011056969640777\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.2442e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.245346255833283e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.4658e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.464487574296072e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015158609312493354\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.7120e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.711708556395024e-05\n",
      "counter: 11000\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.9276e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.92487704125233e-05\n",
      "tensor([[0.9995]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0005479764076881111\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002344525564694777\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017829338321462274\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.4142e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.4168016756884754e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001512880262453109\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00023194856476038694\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 8.344653679159819e-07\n",
      "tensor([[9.6864e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.686224075267091e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020678988948930055\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010818828013725579\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.3520e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.352406050311401e-05\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.337865791763761e-06\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010568462312221527\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.6480e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.6464813345228322e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013704046432394534\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.7146e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.713138620601967e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011957406968576834\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.4772e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.47958396357717e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.2215e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.221503306529485e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.5463e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.544803909491748e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00023957976372912526\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010568462312221527\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001821683836169541\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.576279254957626e-07\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010955933976219967\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.0590e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.060316369868815e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.2275e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.230304618133232e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010687683970900252\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015140726463869214\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019289922784082592\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.4061e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.407704495359212e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.9423e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.939653733046725e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.8937e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.895073263673112e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.8981e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.896495324326679e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001409749238518998\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005930035840719938\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.0663e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.0665232265600935e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.1222e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.124726547161117e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.9219e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.923213317757472e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.7192e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.71939895558171e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.7980e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.8000764612224884e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018020108109340072\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.4908e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.48973839613609e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.7167e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.7180087272427045e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.1229e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.124443411361426e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011838183127110824\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010604228737065569\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.2442e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.242545426473953e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.5585e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.555693415866699e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018753379117697477\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013543092063628137\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.7819e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.784669287502766e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.8219e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.82496465742588e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.2732e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.275150033412501e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.1170e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.1187657188856974e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.8040e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.80708180880174e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.9012e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.9010340919485316e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.9915e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.993302278919145e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.7085e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.707477662828751e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004799921589437872\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.6071e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.605842256452888e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006742365658283234\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.4178e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.4182088206289336e-05\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.5497220147153712e-06\n",
      "tensor([[7.2295e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.230304618133232e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014139221457298845\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.7825e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.781817526440136e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000325315457303077\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022855032875668257\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.5648e-06]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.569818535557715e-06\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.5260e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.528657180140726e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.2846e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.285389048978686e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011957406968576834\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.5666e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.5672287999186665e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019558196072466671\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012082591274520382\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.3961e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.398647332796827e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.7151e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.717669020872563e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.7743e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.777278031222522e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011432824248913676\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.4485e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.4511685953475535e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001227335014846176\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.1925e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.191458957502618e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.5817e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.584885992808267e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010163108527194709\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00025555776664987206\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005492884083651006\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.6938e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.695556370075792e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.2448e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.246761768124998e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.3331e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.336174556054175e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015641482605133206\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.5312e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.533229417167604e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.5245e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.525275527266786e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012332962069194764\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021853450743947178\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019802623137366027\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.6730e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.675942859146744e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.6416e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.6433095121756196e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017280879546888173\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.1454e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.146852294681594e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012160086771473289\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.1246e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.127545202616602e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.8904e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.891966379247606e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001490227150497958\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003102306800428778\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011021506361430511\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010526733967708424\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.1709e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.170695607783273e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011516280210344121\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015695134061388671\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.0060e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.008328637108207e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.4167e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.417977707926184e-05\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 9.89442014542874e-06\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00035810921690426767\n",
      "tensor([[0.6174]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.4822671413421631\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015045988839119673\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002678991586435586\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0016801158199086785\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017174320528283715\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000919705256819725\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004558409855235368\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0019215317443013191\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010581845417618752\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0030]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0029617855325341225\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010026353411376476\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0036]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0035609151236712933\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004636528028640896\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009741161484271288\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0038]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0038020676001906395\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017883068649098277\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006696439231745899\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0041]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004058538936078548\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011186297051608562\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0027]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0027177275624126196\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007467673276551068\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020351096463855356\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008276549051515758\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005914529319852591\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009759060340002179\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003251365851610899\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0026]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002588161500170827\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00039680724148638546\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008031970355659723\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0045]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004469653125852346\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001257912372238934\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00048315274761989713\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0041]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004101091530174017\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0024]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0023790861014276743\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004940060898661613\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008008705917745829\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001182062434963882\n",
      "tensor([[0.9996]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0004210762563161552\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006457267445512116\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000769493926782161\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0049]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004961679689586163\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006589079857803881\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006363628199324012\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009518025326542556\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011051441542804241\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007213571225292981\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013389009982347488\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0021]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0020617623813450336\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011118869297206402\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0035]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0035256235860288143\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007266061147674918\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005268650129437447\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017486594151705503\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000470808707177639\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010417163139209151\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00026462003006599844\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011261483887210488\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005691478727385402\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002540076384320855\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015060316072776914\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001954627368832007\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001878236187621951\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006966632790863514\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006725665298290551\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011874331394210458\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00039012887282297015\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010581845417618752\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0016419054009020329\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011039506644010544\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010016807354986668\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00037718971725553274\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007594130584038794\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004885793896391988\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005442192777991295\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0016121739754453301\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000583341927267611\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0026]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0025684412103146315\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014367286348715425\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006814536172896624\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00035638007102534175\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011617619747994468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016619155940134078\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004504145181272179\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009493564139120281\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021322854445315897\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019570119911804795\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006922494503669441\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001901764888316393\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0022]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002213123720139265\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00032066478161141276\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005683725466951728\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00034540894557721913\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011671270476654172\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017304725770372897\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0030]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0030500267166644335\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00045768957352265716\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0023]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0023017770145088434\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010198785457760096\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011680388124659657\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004238788387738168\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001435382873751223\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003043280157726258\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013058363692834973\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00024864188162609935\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006184701924212277\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00035155037767253816\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003427258343435824\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003625811659730971\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008178120478987694\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0027]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002726632868871093\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011873950279550627\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011447661090642214\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0029]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0029240043368190527\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021352662588469684\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005638400325551629\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004212551284581423\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00045816664351150393\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000444153236458078\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005285348161123693\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005646153585985303\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001402586349286139\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003982383059337735\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00035715519334189594\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011135577224195004\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007701501017436385\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001329271908616647\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002640238089952618\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004981805104762316\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022306546452455223\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0029]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0029335690196603537\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003519677557051182\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000339267571689561\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015653404989279807\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013239067629911005\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005859064403921366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021346702123992145\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0044]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004375299904495478\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005964626907370985\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002303984947502613\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011506141163408756\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003013468813151121\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001253564259968698\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012892447412014008\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018824919243343174\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007371042156592011\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00039966939948499203\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00024148757802322507\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.7132e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.713138620601967e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002935363445430994\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.8473e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.850239489926025e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00036085202009417117\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.2381e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.236265810206532e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004378919838927686\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002695685252547264\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009608114487491548\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0033]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003339077578857541\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021281122462823987\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001489630958531052\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00031380809377878904\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0036]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003640654729679227\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003234671021346003\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.5144e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.511905616614968e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011224184709135443\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007665710290893912\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001822044374421239\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001638665999053046\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008253880077973008\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00047718940186314285\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005867414292879403\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005415356135927141\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004583455447573215\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018532801186665893\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016219739336520433\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003488076035864651\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00027463631704449654\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002695685252547264\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010705567547120154\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011474552593426779\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016458197205793113\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020953227067366242\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008646410424262285\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000295921228826046\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002024378627538681\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015988608356565237\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005324111552909017\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010411792900413275\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0021]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002067436696961522\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013590783055406064\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002793463645502925\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017109836917370558\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001582144876010716\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00024333575856871903\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018359915702603757\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002399970981059596\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000431511492934078\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005769605631940067\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010860556358238682\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005732033168897033\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001241641875822097\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00047474444727413356\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006415517418645322\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020869763102382421\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000510823039803654\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.7776150091085583e-05\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00031714700162410736\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008508605533279479\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002683165075723082\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003702729882206768\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00042847037548199296\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022503285435959697\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020881686941720545\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007690763450227678\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003139273321721703\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022819261357653886\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005649731610901654\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001697088300716132\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001063403396983631\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004085540713276714\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010887348325923085\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00035393540747463703\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00025013237609528005\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008502043201588094\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002888858434744179\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020994959049858153\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004038433835376054\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.1806e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.179536573356017e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00023820853675715625\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005266264779493213\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006399413687177002\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002709994441829622\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005506600718945265\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012118358426960185\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000618350924924016\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00035810921690426767\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016309160855598748\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00026128129684366286\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003090382379014045\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005944349686615169\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002777366025839001\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00029431143775582314\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003375980886630714\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018192992138210684\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00035828808904625475\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003869089705403894\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003774878568947315\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00023934128694236279\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022455590078607202\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001394845894537866\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020798222976736724\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002893031924031675\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002931786293629557\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00033658446045592427\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022986192198004574\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015200339839793742\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003003929159604013\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00047062980593182147\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001955223415279761\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00025311336503364146\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021322854445315897\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000346303335390985\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015361777041107416\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010890361591009423\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.0008e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.999263470992446e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004802306939382106\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.0652e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.063399971229956e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004443321086000651\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.0631e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.060562398284674e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010681722778826952\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00041815440636128187\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.9977e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.997830496402457e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005655099521391094\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003851201618090272\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020756490994244814\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004142784746363759\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00026551433256827295\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00029961782274767756\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016404544294346124\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017394148744642735\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00033765772241167724\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00041171442717313766\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021144002676010132\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.8106e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.80995430634357e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010466687381267548\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014246524369809777\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.0684e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.0679369198624045e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019480695482343435\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003470784577075392\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015450717182829976\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00033145674387924373\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.6664e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.666890789754689e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00042686035158112645\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004230440245009959\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00039042701246216893\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00027022435097023845\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005281173507682979\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000135490539832972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00024923807359300554\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004417679738253355\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.8261e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.827837882563472e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001954627368832007\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018264530808664858\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015248030831571668\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.6911e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.6882056772592478e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000137696202727966\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00029693482792936265\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002980676363222301\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00046508395462296903\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.4305124977909145e-06\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007024490041658282\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000629384652711451\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005129102501086891\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011558008554857224\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022753681696485728\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.6595e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.662380034569651e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005005061975680292\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004967492423020303\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004007427196484059\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020804184896405786\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00023963938292581588\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019289922784082592\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003254346956964582\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002582406741566956\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002036898076767102\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00033252997673116624\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010926128015853465\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006072576506994665\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.1322e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.1320916099939495e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002994389506056905\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.6066e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.604412919841707e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010097536869579926\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018187030218541622\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000861121341586113\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014866502897348255\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018228762201033533\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002670048561412841\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015277837519533932\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001600512769073248\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002619371225591749\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013304641470313072\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021817679225932807\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.576279254957626e-07\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006592658464796841\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010726838372647762\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022771567455492914\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000323586369631812\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.8342e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.832357368897647e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00043014000402763486\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.4181e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.4199778636102565e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.1521e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.152813486754894e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003502982435747981\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00024858227698132396\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003382539434824139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.3934e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.392958726384677e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002650969836395234\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.9273e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.926299829501659e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003268060681875795\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007021507481113076\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00032960838871076703\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00030540121952071786\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003087400982622057\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001568321167724207\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.4647e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.46731277811341e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.3143e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.3168758313404396e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00048064813017845154\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003023604513145983\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020708797092083842\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017841262160800397\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002588368661236018\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016237623640336096\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010347949573770165\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015575907309539616\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020303402561694384\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.9113e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.909849227871746e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003063551848754287\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.6393e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.641910006990656e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.2400e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.242545426473953e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000154745634063147\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022163463290780783\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001266678882529959\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020261670579202473\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004074211174156517\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002956827520392835\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010514812311157584\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014866502897348255\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.1714e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.173575381282717e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002566309121903032\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014550551713909954\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003238844801671803\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006614130106754601\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014800929056946188\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001497976918471977\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003170873678755015\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020404750830493867\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021006882889196277\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001151031901827082\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013996149937156588\n",
      "counter: 12000\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008646410424262285\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.0375e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.039883085875772e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000402770092478022\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003098729357589036\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002460782416164875\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00046621699584648013\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003616867761593312\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013203300477471203\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001601108961040154\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011462630209280178\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001775183918653056\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00024405118892900646\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.9912e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.991869304329157e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015492447710130364\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001487842673668638\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014216717681847513\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000318160600727424\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.6037e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.604412919841707e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.8496e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.851681195665151e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.9339e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.936587593983859e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015253991296049207\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011516280210344121\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00042167253559455276\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002460782416164875\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.6220e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.619466562871821e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000135490539832972\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015456679102499038\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003774282231461257\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005949120968580246\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014401519729290158\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00039794016629457474\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002776769979391247\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.7352e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.732720844913274e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00026855498435907066\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005219152080826461\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004159481031820178\n",
      "tensor([[0.9997]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.00033646522206254303\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011718960013240576\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003894729888997972\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018449338676873595\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004761756572406739\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00041797550511546433\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012118358426960185\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.9883e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.987341086845845e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011313601862639189\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014860542432870716\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018491070659365505\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010479045886313543\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004224477452225983\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020941304683219641\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011179137509316206\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.9645e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.9651902372715995e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020565716840792447\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002788097772281617\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017394148744642735\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016887421952560544\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.1228e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.124726547161117e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006232415325939655\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001651781058171764\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015146686928346753\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000375222007278353\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011033428745577112\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002810753940138966\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005473204073496163\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001909915154101327\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00023880471417214721\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000722609693184495\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001205278531415388\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.8513e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.8519359552301466e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006957685691304505\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003371807106304914\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00048792341840453446\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.5922e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.590847184881568e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015653404989279807\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.7062e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.708617679076269e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011456669017206877\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020398790366016328\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00037802450242452323\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001546860148664564\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001998147345148027\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00023695653362665325\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010055809252662584\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007434269646182656\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.7027e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.70432712265756e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.2453e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.243662159657106e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013626550207845867\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003649662248790264\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.875778904533945e-05\n",
      "tensor([[7.8744e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.874083530623466e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000452561245765537\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005059329560026526\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00027839242829941213\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001890241983346641\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.9064e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.905582682113163e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013888847024645656\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001620185503270477\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00026223520399071276\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008510395418852568\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011271873518126085\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020398790366016328\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001663107832428068\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000122912329970859\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002935363445430994\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005524492007680237\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.8896e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.888899512588978e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.1183e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.117047789506614e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001698280539130792\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00039805943379178643\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011110922787338495\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003797536774072796\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011617619747994468\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004402772174216807\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00044009831617586315\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.0692e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.0683026377810165e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.9051e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.9069949202239513e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002351679722778499\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.5484e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.546539301169105e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018550685490481555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.3553e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.355797889526002e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002680780307855457\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005167865892872214\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.4085e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.407704495359212e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003587651008274406\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006444742321036756\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.7585e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.759395182598382e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002614601398818195\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002028551825787872\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.5076e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.506212644628249e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.9039e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.905582682113163e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00028918395400978625\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.6899e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.689294579904526e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012875432730652392\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.589663149090484e-05\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000344812695402652\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021191696578171104\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.9675e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.9666028391802683e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.5300e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.5314641940640286e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010091575677506626\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010532695159781724\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.5049e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.505944424541667e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001918261405080557\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022747719776816666\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.6409e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.638535266276449e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.4410e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.443469465011731e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.0537e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.056015263311565e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012452185910660774\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013274834782350808\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.0738e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.072484054835513e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001618993264855817\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00046800595009699464\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.5036e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.501431486569345e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003048049984499812\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012022979353787377\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012591656995937228\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022652330517303199\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001816914591472596\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004925152170471847\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001614224020158872\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00028202933026477695\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.0680e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.0665232265600935e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016607233555987477\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.7937e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.793522622203454e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000142584482091479\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.1690e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.167614916805178e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.8972e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.898219802067615e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.6528e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.65045765042305e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011772610014304519\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013310603389982134\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013583583058789372\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001366231736028567\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.7152e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.713438491104171e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003074284177273512\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.7593e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.7565637942170724e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001435382873751223\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.2534e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.254148658830673e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003966283402405679\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019713198707904667\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013429828686639667\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.0886e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.085819404688664e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.3182e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.321154746226966e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.9216e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.919287933560554e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.6994e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.701216964051127e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004343141335994005\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.4383e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.440374222118407e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.3785e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.377900717779994e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020702836627606302\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.0934e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.096082794712856e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001179645478259772\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001496784680057317\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010079653293360025\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00033288775011897087\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006081522442400455\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.0966e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.098105505865533e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018556647410150617\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.1141e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.11420912516769e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006874778191559017\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.6540e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.65641884249635e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017847222625277936\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012249505380168557\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001726299524307251\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012899277498945594\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005247181397862732\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010365784692112356\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.1747e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.17523182881996e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001393653656123206\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013042346108704805\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003358093381393701\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002772000152617693\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.7114e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.708877895609476e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00034964235965162516\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.3008e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.29899334651418e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.5213e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.520960596390069e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00029699443257413805\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011558008554857224\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.7101e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.712048080866225e-05\n",
      "tensor([[0.9791]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.021121568977832794\n",
      "tensor([[5.1168e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.11420912516769e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015921668091323227\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007393708801828325\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.5781e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.579150456469506e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004251906939316541\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00039955013198778033\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.9397e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.941347287967801e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.3554e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.3526404371950775e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001134340709540993\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002803003299050033\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010049848060589284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00034308357862755656\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.768382950715022e-06\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011468591401353478\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.4357e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.4364822163770441e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000412608846090734\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018854725931305438\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013972305168863386\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.6713e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.671713420771994e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.2706e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.270605081226677e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001288139319512993\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.4192e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.422490645898506e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010854595166165382\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011277834710199386\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00034457421861588955\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013030423724558204\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.2036e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.2008207199396566e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003866108600050211\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002141228032996878\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011377247283235192\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011975289817200974\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.3039e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.3049541747896e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001650588819757104\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.1090e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.108248660690151e-05\n",
      "tensor([[0.9980]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0020074713975191116\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001587993756402284\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.98023678624304e-06\n",
      "tensor([[3.9808e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.9816695789340883e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.4816e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.4837775678606704e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.7809e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.781600238056853e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018628186080604792\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.9291e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.93062712950632e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.9757e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.978315210901201e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003466014750301838\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016154164040926844\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.9230e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.920338637428358e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008184085600078106\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00024172605480998755\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010985739208990708\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018103569163940847\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.0540e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.052912016864866e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.3931e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.394365871325135e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017608761845622212\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010771139204734936\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011587814515223727\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001382923364872113\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012446223990991712\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.9622e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.963497773744166e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010622111585689709\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.5335e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.531236719340086e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.0137e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.0142894653836265e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.7663e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.769678581506014e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014222679601516575\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001650588819757104\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016696655075065792\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.8126e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.814474520273507e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00029699443257413805\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001048697391524911\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.7732e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.774445915245451e-05\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.768382950715022e-06\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022467513917945325\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.1344e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.134930638130754e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.6384e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.638535266276449e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010878439206862822\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002738016191869974\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.0402e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.0381327784853056e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.6529e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.652099546045065e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.1948e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.194539648480713e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.0397e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.040989632718265e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016362813767045736\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.5357e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.534618008416146e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010705567547120154\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00027254957240074873\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.2420e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.245107503375039e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00038064809632487595\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.9499e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.951866165152751e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014174988609738648\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.5944e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.594224835978821e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.5053e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.5076208809623495e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003787996538449079\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.7610e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.762266952544451e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.7979e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.7997000112663954e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017453763575758785\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.8771e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.875525236362591e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005006851279176772\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013340408622752875\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016952997248154134\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006352892378345132\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017555108934175223\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.8566e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.857642387738451e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.8278e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.826396176824346e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.1540e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.154247916536406e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.2142e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.215542478254065e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017447801656089723\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.3249e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.3246082491823472e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020780338672921062\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00026462003006599844\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004767719947267324\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001434190635336563\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.1317e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.1320916099939495e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.4399e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.437509000534192e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010848633974092081\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005425494164228439\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011212262324988842\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.1888e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.190294202999212e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011176495172549039\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013119842333253473\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.1171e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.117047789506614e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005056944210082293\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.1630e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.166169573087245e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012833703658543527\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.5215e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.524094765656628e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001687549811322242\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.5774e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.580569606740028e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.7904e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.787778354715556e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011617619747994468\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021155926515348256\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00024077214766293764\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002032725024037063\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018967996584251523\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.2095e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.2113566956249997e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001946280972333625\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.6116e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.614691225579008e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.1394e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.139829666703008e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.6435e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.644719203701243e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010884400398936123\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005327690159901977\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.1698e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.172130765160546e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003574533329810947\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.5521e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.553898543235846e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013137726637069136\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016040896298363805\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.6798e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.677372923353687e-05\n",
      "tensor([[0.9994]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0005773780285380781\n",
      "tensor([[7.1074e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.105126132955775e-05\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.576279254957626e-07\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001532552851131186\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013233107165433466\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004355663841124624\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015707057900726795\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022032303968444467\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.1201e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.119926835410297e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016714539378881454\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.0369e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.036472329171374e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014365751121658832\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021537477732636034\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013763659808319062\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001663704024394974\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.2373e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.2391470388975e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011128806363558397\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001319733855780214\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016899344336707145\n",
      "tensor([[0.9981]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0018892241641879082\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001908722915686667\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013262912398204207\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012905237963423133\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016714539378881454\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.7546e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.75630576047115e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00032877366174943745\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010747295164037496\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017614723765291274\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001644031290197745\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.6837e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.6864446378313005e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010258485417580232\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005207821377553046\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.2330e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.232019273331389e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00025430574896745384\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.1797e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.178091957233846e-05\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 5.614915062324144e-05\n",
      "tensor([[2.4389e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.4378596208407544e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016106471593957394\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.9358e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.933693268569186e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.0087e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.006915671401657e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.2100e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.2113566956249997e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003419507120270282\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010049848060589284\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016535694885533303\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022884841018822044\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019588004215620458\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00029413256561383605\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.6901e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.6909954107832164e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.4183e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.417977707926184e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.1706e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.1678562158485875e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.2023e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.201935270335525e-05\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.576279254957626e-07\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021191696578171104\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012589294055942446\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000675250543281436\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.8675e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.865055471891537e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.7748e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.7762342395144515e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.8588e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.859308294020593e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.0556e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.056015263311565e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.0310e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.029067976167426e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019379347213543952\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017233187099918723\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.1556e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.157349707791582e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.2907e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.2930325182387605e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012386612070258707\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.7739e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.771316839149222e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00029562311829067767\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.7107e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.713138620601967e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011218223517062142\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000163806980708614\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.7700e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.769896233687177e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003217976482119411\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002024378627538681\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014628049393650144\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.4830e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.482100383844227e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.4706e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.468740659533069e-05\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019909933325834572\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001101554516935721\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010693645162973553\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.3556e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.358601265470497e-05\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[9.1504e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.149731340585276e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.3619e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.36144429538399e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.2665e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.266070315381512e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005121350404806435\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.6505e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.6506800319766626e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002219327143393457\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014222679601516575\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004761756572406739\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.7750e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.7758566981647164e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011277834710199386\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.9416e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.944181950530037e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6.4532e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.45539112156257e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010556539928074926\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014407480193767697\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.8156e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.813043000875041e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019033574790228158\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.8862e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.889112435397692e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012809858890250325\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.2103e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.2127420126926154e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012350844917818904\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002565713075455278\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.3513e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.3526404371950775e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012481991143431515\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007507042028009892\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.5163e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.5167358873877674e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00027076093829236925\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010961895168293267\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000147353537613526\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.3404e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.343562174355611e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013453673454932868\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.6724e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.673113289754838e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001427633105777204\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.4270e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.428451837971807e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.0129e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.012628288473934e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017167611804325134\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.2412e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.2439409298822284e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00024679367197677493\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008882055408321321\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022795413678977638\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.7685e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.768485450767912e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.6420e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.6391411918448284e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.9854e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.9862372684874572e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00024715138715691864\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011867989087477326\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006593254511244595\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00030540121952071786\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014055763313081115\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010765178012661636\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001410345284966752\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010341940651414916\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002255694125778973\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000133761772303842\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000202020542928949\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.2474e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.249623351730406e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.3758e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.374803292099386e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.9412e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.941347287967801e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.4096e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.407704495359212e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.6156e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.613506098394282e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003199492930434644\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000129827341879718\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920935776288388e-06\n",
      "tensor([[8.4123e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.410568989347667e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.0826e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.081282819854096e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.6874e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.689294579904526e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011683192860800773\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013537131599150598\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011420901864767075\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020959188987035304\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.5091e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.510467548854649e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.8559e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.85476916260086e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014246524369809777\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.7622e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.7639354054117575e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00036276006721891463\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001504534448031336\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00043872679816558957\n",
      "counter: 13000\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013429828686639667\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004736114351544529\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016714539378881454\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004314518882893026\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.9038e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.906782361213118e-05\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006055280682630837\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015432832879014313\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011891833855770528\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004660380946006626\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.1931e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.194539648480713e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.8467e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.84880797052756e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011850105511257425\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.8930e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.895073263673112e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.8062e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.80708180880174e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.6881e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.6864446378313005e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.2071e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.209581649978645e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002808965218719095\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.9877e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.9904461522819474e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.9179e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.919287933560554e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.7820e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.780149801168591e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.7959e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.795160152250901e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.5284e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.5314641940640286e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010437318269396201\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011540124978637323\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005277595482766628\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.9521e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.9504733902285807e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012827741738874465\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.5212e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.523827273165807e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.9493e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.94850997813046e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001229719491675496\n",
      "tensor([[0.9969]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.003104254836216569\n",
      "tensor([[3.7813e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.7790057831443846e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.5501e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.549120295559987e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012440263526514173\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017888953152578324\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013388099614530802\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.7305e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.728170435759239e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010711528011597693\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.6164e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.614691225579008e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012112397234886885\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001748953218339011\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014234601985663176\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.1499e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.152813486754894e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.1245e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.12613078171853e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.4827e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.483548637945205e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.7552e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.755163197638467e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010866516822716221\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000129708118038252\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013686163583770394\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017048382142093033\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.4298e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.431547808460891e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001217797034769319\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005714737926609814\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018669918063096702\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002515036321710795\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013429828686639667\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.2236e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.225779311032966e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011826260742964223\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.6637e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.6657529562944546e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.4027e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.406055323779583e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021310930605977774\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014604204625356942\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.6508e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.653529610252008e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.0348e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.036719448980875e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.2043e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.20503633399494e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010705567547120154\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.6694e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.66834049904719e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.3120e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.315193554153666e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.8495e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.847171895671636e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.4115e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.414017217233777e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005254934076219797\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.5898e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.588264007703401e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002415471972199157\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016112433513626456\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.0252e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.023106784094125e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.8737e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.871229950571433e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016362813767045736\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.1361e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.1352534278994426e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015355335199274123\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006100011523813009\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010407512309029698\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018753379117697477\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014729391841683537\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001435382873751223\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000591989723034203\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.2746e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.2769234419683926e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017125881277024746\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017722031043376774\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012481991143431515\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.9969e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.996406980557367e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001497976918471977\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.6050e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.607281051808968e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.5590e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.559592970414087e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013000618491787463\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010490967542864382\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.3163e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.313757669180632e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.9251e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.9280232158489525e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.4515e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.453973790281452e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016303198935929686\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016732423682697117\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.0013e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.0009548431262374e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.5819e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.583437011111528e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014789005217608064\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011158612323924899\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.0775e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.078199218492955e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.1760e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.178372910246253e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.4375e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.4392473025945947e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.0022e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.002736073336564e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006301599787548184\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015999165771063417\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.8424e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.844573438982479e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011128806363558397\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.3986e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.39720999263227e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00032883326639421284\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00025555776664987206\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001241045683855191\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.2671e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.269189568934962e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019921857165172696\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010520773503230885\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.4748e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.4778167395852506e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.9602e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.960432362277061e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000293059361865744\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.7635e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.7639354054117575e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017686262435745448\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017686262435745448\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.1849e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.1875144739169627e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.8184e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.819004192948341e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.9683e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.969458238221705e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.3390e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.3411135114438366e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021799794922117144\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.1496e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.1471747206524014e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00028357948758639395\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.3537e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.3526404371950775e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010812866821652278\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012189892731839791\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017614723765291274\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010783061588881537\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001663704024394974\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.0077e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.009752152953297e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.9620e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.964940207079053e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00034505120129324496\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.0928e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.0917802329640836e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.9101e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.908416980877519e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.5226e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.523827273165807e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00028429494705051184\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00033431872725486755\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.6808651707833633e-05\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010914206359302625\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.6186e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.6180674214847386e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.8489e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.8477610865375027e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010842672782018781\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.6423e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.6433095121756196e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.7513e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.75034456839785e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000229384982958436\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.6478e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.649007941130549e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001710203505354002\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001780549209797755\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.7503e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.75034456839785e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010312135418644175\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.8386e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.839759539114311e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018139337771572173\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016183970728889108\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.7508e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.7506029659416527e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.3290e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.3287971240933985e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011617619747994468\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004758178547490388\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012744285049848258\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.6484e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.646139081567526e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012279310612939298\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.1937e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.1934749383945018e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002251520927529782\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.5896e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.588264007703401e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001419287291355431\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00041797550511546433\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.5158e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.5181343011790887e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003756990481633693\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.6402e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.643046749057248e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007077575428411365\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1682578588079195e-05\n",
      "tensor([[2.4680e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.467662670824211e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010258485417580232\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.2271e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.230304618133232e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015707057900726795\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.0354e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.036472329171374e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011927601008210331\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019057421013712883\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.4756e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.4778167395852506e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.1240e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.124443411361426e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.5062e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.5076208809623495e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006370785413309932\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.8356e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.83688631397672e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.0592e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.060562398284674e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.8926e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.892258973792195e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00030558009166270494\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014210757217369974\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.9085e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.906782361213118e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.4431e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.4438202672172338e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001385307841701433\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.9106e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.908416980877519e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.4529e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.453743405174464e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014490938337985426\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001418095052940771\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.0537e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.051478314679116e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.2383e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.236584598198533e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.1076e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.108248660690151e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.6346e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.63257480179891e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.2360e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.2379801016068086e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.1032e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.105450377915986e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.9743e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.976862591225654e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.2747e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.273466664832085e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013352332462091\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.5128e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.510467548854649e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.5042e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.50307847536169e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.8062e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.808513328200206e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003522062615957111\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.6186e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.617763913003728e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.8252e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.82496465742588e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013781544112134725\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001305426994804293\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.7189e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.719099085079506e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002308158145751804\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007107995334081352\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00025555776664987206\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001021079660858959\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018091646779794246\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.0345e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.033595466054976e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.0663e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.069360435707495e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.7280e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.728170435759239e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.1498e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.1517507775570266e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.8354e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.837221290974412e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010550578736001626\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00041344366036355495\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011826260742964223\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.1484e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.1471747206524014e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011379174247849733\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.9216e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.919287933560554e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.5205e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.522696715663187e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.2740e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.272030779859051e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.7369e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.734131264034659e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010479045886313543\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.8042e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.80708180880174e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.0586e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.0577652069041505e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.0472e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.048394713317975e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.6474e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.646139081567526e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.8488e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.849142583727371e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.2583e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.2572679123841226e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019033574790228158\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.6874e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.689595541800372e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.4461e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.443469465011731e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.0843e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.081607792410068e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021966724307276309\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.2569e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.2572679123841226e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.4463e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.4484033272310626e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.7935e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.79207145771943e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.6915e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.693825707770884e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.2754e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.275150033412501e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.4363e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.435860556550324e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.7553e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.753433990525082e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006128639215603471\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[7.9697e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.969458238221705e-05\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.5139694369281642e-05\n",
      "tensor([[9.5676e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.567003144184127e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016607233555987477\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013191378093324602\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001243430160684511\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.3553e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.356920443475246e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.1878e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.188899427186698e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005838190554641187\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.8082e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.8074182409909554e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.1466e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.149731340585276e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014538629329763353\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010794983973028138\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.9609e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.9606420109048486e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.6084e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.608730033505708e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.0730e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.072484054835513e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.7692e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.76822741702199e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.8863e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.887700197286904e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.3236e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.322836295817979e-05\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.6689314179529902e-06\n",
      "tensor([[3.5945e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.594224835978821e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015140726463869214\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.3009e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.303271897602826e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019254154176451266\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.5668e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.564115367131308e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.7292e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.727950964588672e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017680300516076386\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016291276551783085\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.7372e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.739873348735273e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015420910494867712\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.1476e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.1471747206524014e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000244468537857756\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.1085e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.1100266167195514e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.9211e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.923213317757472e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011951445776503533\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00035870546707883477\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.4848e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.4855446099536493e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.4663e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.4690503525780514e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.7291e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.73246171977371e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.6573e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.6566408602520823e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.8024e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.802848732448183e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.7571e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.7583524822839536e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.5524e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.552193710580468e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011873950279550627\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.4432e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.445207767072134e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.7857e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.7849666114198044e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.0815e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.08300147682894e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.6973e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.695556370075792e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.2840e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.282526737777516e-05\n",
      "tensor([[0.9614]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.03932451456785202\n",
      "tensor([[7.9751e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.975419430295005e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010920166823780164\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018502993043512106\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00031476205913349986\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.0477e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.046950824791566e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012851586507167667\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001418095052940771\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.6205e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.619203435955569e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.3975e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.3975225960602984e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004281722067389637\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.7642e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.762524622492492e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004760563897434622\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.3989e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.401744030881673e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012076630082447082\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.0299e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.032171950209886e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.1966e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.195974805857986e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00025406727218069136\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.9045e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.905330469133332e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.9229e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.9206701583461836e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001382327318424359\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001862222416093573\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00029001865186728537\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003525640058796853\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.6094e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.608954234048724e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.6754e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.673113289754838e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015778593660797924\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[4.0868e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.08896230510436e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021346702123992145\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00041922772652469575\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.0476e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.045517122605816e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00040533411083742976\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.6069e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.6047568098874763e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012589294055942446\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.3285e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.3273907067487016e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010842672782018781\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.0150e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.012628288473934e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.4909e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.4901272152201273e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004010408592876047\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.5115e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.511905616614968e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.6903e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.687864515697584e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014771122368983924\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015277837519533932\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022205195273272693\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018037992413155735\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021817679225932807\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.4944e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.495471022091806e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001697088300716132\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012935044651385397\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012374689686112106\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019486657402012497\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.9558e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.95643403660506e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.9287e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.9294256314169616e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019033574790228158\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.1547e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.157349707791582e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014622088929172605\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011808377166744322\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003886382037308067\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003816021198872477\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002432761393720284\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.0072e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.005223935469985e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.7563e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.759395182598382e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.9954e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.9949940148508176e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000396389834349975\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.9228e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.922062387573533e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.0197e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.020250293659046e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.2922e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.29279558476992e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.2524e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.252722960198298e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020792261057067662\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011385135439923033\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.8590e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.860729627078399e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.2872e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.2856656364165246e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.4826e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.4837775678606704e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012100474850740284\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.0071e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.006667096400633e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.7508e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.75034456839785e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.4058e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.407704495359212e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000257167499512434\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008601071895100176\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002915688091889024\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020464368571992964\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.3581e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.355797889526002e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011826260742964223\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013423866766970605\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000325732835335657\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001875934103736654\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.1263e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.1279083739500493e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.9892e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.989033186575398e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012017018161714077\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011760687630157918\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002745766832958907\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.6693e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.669982394669205e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.5751e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.576037023682147e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.1763e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.176656799856573e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.2117e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.2127420126926154e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.3010e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.300409586401656e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021436128008645028\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.0351e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.033595466054976e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017340494378004223\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021662673680111766\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003409370838198811\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.0271e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.0265784769435413e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012678711209446192\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010067731636809185\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014163066225592047\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.6705e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.668562516802922e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.9606e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.9606420109048486e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.7499e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.749202369363047e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.4515e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.4525659177452326e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003426065668463707\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.3397e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.340718780644238e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021758062939625233\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016034934378694743\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.7424e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.742943125776947e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001520630030427128\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.8630e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.8656428437680006e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.6098e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.612106593209319e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020255710114724934\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.2061e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.206461305031553e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.6286e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.631125092506409e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021167848899494857\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003169681294821203\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020500138634815812\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010145224950974807\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018443378212396055\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.1842e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.184333738521673e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001554609916638583\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.2561e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.2590415028389543e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013262912398204207\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.3955e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.39720999263227e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010848633974092081\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00039024814032018185\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.9111e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.908416980877519e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010097536869579926\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.6557e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.653529610252008e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002957423566840589\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000137696202727966\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.0283e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.0307586207054555e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021203620417509228\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.6164e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.613506098394282e-05\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001791279937606305\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015569945389870554\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012708517897408456\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.6747e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.677372923353687e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002519209519959986\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014789005217608064\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001465189561713487\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.5614e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.559592970414087e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00025186134735122323\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.3436e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.343562174355611e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013417906302493066\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022920612536836416\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.6996e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.699786172248423e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00031690849573351443\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00025090741110034287\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001179049359052442\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.5526e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.555080760037526e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013203300477471203\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014216717681847513\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001218393153976649\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001954627368832007\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013990188017487526\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002069687470793724\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002307561953784898\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[8.0018e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.999263470992446e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000250609329668805\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010318096610717475\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015242068911902606\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012601216440089047\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001218393153976649\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.2833e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.281110861687921e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.2897e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.2916264646919444e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012094513658666983\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004094485193490982\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017710108659230173\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.7453e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.747473526047543e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010747295164037496\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.4997e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.50166005268693e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.4249e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.425587343983352e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002500727423466742\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002575252146925777\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.9912e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.9904461522819474e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014789005217608064\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.5188e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.5199299923551735e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.1172e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.12016995344311e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.6192e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.620875890599564e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.1191e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.12016995344311e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00040497633744962513\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.5167e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.517866808688268e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.9905e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.9908149624825455e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013388099614530802\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.5113e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.510775059112348e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.4849e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.48693247360643e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.3483e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.348096212605014e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001540302619105205\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.4809e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.47923443466425e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.5620e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.563032649050001e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.4041e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.406055323779583e-05\n",
      "counter: 14000\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.3450e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.346679608919658e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018628186080604792\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014383635425474495\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.5801e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.579150456469506e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.3438e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.342490006412845e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017674338596407324\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.8419e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.839759539114311e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.2712e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.273744343663566e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003423084563110024\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.2248e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.224663305445574e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.2643e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.2632287406595424e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.3847e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.383861182257533e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022604636615142226\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002399970981059596\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.0503e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.048394713317975e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.4476e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.447782213101164e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.7589e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.7565637942170724e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.6143e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.614915062324144e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.3049e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.3053502698312514e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.7896e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.787778354715556e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.8967e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.896495324326679e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.3048e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.306370050879195e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011665309284580871\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003786803863476962\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.8363e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.83861297450494e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001675030798651278\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016726461763028055\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.0392e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.041276406496763e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.2640e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.2650021492154337e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001466381800128147\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.5652e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.567003144184127e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.2558e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.258683424675837e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013107919949106872\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011438785440986976\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.5286e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.526921788463369e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.2464e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.248505890951492e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013715968816541135\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.9474e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.945905336877331e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.3362e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.3333515350241214e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.2768e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.2797051719389856e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003263886901549995\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.8683e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.871016663964838e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.9558e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.956104334676638e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.7325e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.731021469226107e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00025764445308595896\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017722031043376774\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.0844e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.084160410566255e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002523979055695236\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.0951e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.093529085163027e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013855156721547246\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.2731e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.270605081226677e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00024840340483933687\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001426440867362544\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.3912e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.3901748136267997e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.5598e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.558154902653769e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019033574790228158\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00024238185142166913\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.1644e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.16505684168078e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.4953e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.49429135187529e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.4105e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.407704495359212e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022819261357653886\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.3380e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.337600982282311e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010973817552439868\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.9765e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.976862591225654e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011045350402127951\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.5961e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.595623613568023e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.2710e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.273744343663566e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018055876716971397\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011867989087477326\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.8550e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.8537215510150418e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.2541e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.252722960198298e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.2786e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.279427856905386e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.5979e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.595359395258129e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.3798e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.37964047503192e-05\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 8.344653679159819e-07\n",
      "tensor([[3.8037e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.802848732448183e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014496900257654488\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002938940888270736\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.9457e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.947057358454913e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013584821135737002\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011993173393420875\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.8062e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.8074182409909554e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014043840928934515\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00043300227844156325\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.2833e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.2856656364165246e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020237824355717748\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.6113e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.614691225579008e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012619099288713187\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.9853e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.982823055703193e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.9712e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.9729332052520476e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.8232e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.821876690490171e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.4157e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.41509284125641e-05\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.145769485650817e-06\n",
      "tensor([[2.4964e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.4974657208076678e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014067685697227716\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.2868e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.2856656364165246e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014783044753130525\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.0854e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.087568256887607e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.2401e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.240801303647459e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.4990e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.4974657208076678e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.7116e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.713438491104171e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.2874e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.285389048978686e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00026944928686134517\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.9996e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.999551699962467e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000836543389596045\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.8066e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.805660839541815e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.9185e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.920338637428358e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003513715055305511\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003061763127334416\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.0531e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.051478314679116e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.9241e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.924665937433019e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.1299e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.130687375436537e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.0903e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.088677714113146e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.6894e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.6909954107832164e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.6189e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.619203435955569e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.4592e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.4585267460206524e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.3050e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.303271897602826e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.5477e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.546539301169105e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.0136e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.011472992715426e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.5676e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.5658201997866854e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.0790e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.0770406485535204e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.9927e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.9921979148639366e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.6431e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.646139081567526e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.8318e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.829504152643494e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018646071839611977\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.8055e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.804249692824669e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.8198e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.8179582184529863e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.4239e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.42701449780725e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00024733025929890573\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.4846e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.48693247360643e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.6047e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.604412919841707e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00027254957240074873\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.9751e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.977111530024558e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.0567e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.059158527525142e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.7871e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.7863675717962906e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.5872e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.589663149090484e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.6588e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.6584024453768507e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.7311e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.732720844913274e-05\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.8000764612224884e-05\n",
      "tensor([[9.3531e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.352406050311401e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.7392e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.7400920923100784e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.5286e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.5314641940640286e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019850317039527\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011838183127110824\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.3839e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.383861182257533e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.4264e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.425587343983352e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.6619e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.662601688527502e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001364443451166153\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022038265888113528\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003266868006903678\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.5732e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.5703818866750225e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.7012e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.702656487002969e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014860542432870716\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.3551e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.3526404371950775e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012493913527578115\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001481285144109279\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.5054e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.5048145946348086e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013596743519883603\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.7238e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.726760016637854e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.3896e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.3915617677848786e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00023463139950763434\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.5343e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.534618008416146e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018264530808664858\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.8119e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.814770025201142e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010747295164037496\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013763659808319062\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021686521358788013\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011897795047843829\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.9472e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.9445127438521013e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.6194e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.622295040870085e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005650328239426017\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012952927500009537\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.4131e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.4108408474130556e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.2050e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.207896462408826e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002250924735562876\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.1391e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.136647839914076e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.5727e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.573189628194086e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.8629e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.865055471891537e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.6512e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.649007941130549e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.1792e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.179777872399427e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011546086170710623\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 8.344684829353355e-06\n",
      "tensor([[8.5495e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.547671313863248e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5.9824e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.984485324006528e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001452074502594769\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012660828360822052\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012124319619033486\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011707037629093975\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.1370e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.139829666703008e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.1600e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.161653724731877e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.2922e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.290230597485788e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.1411e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.144013266544789e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.9761e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.977111530024558e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010651917546056211\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.9256e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.923213317757472e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.7269e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.7285496141994372e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.6189e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.620652417652309e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.2576e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.2572679123841226e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.9471e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.948720354354009e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.8890e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.887447620509192e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.5345e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.537197911413386e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.2101e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.210996798472479e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.1253e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.124443411361426e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00024399156973231584\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015134764544200152\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001020483614411205\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002219923335360363\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.3040e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.30183528503403e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001731068769004196\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.4341e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.43297568988055e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00042417700751684606\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001227335014846176\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.5879e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.585111284744926e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.5324e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.532882252940908e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010932089207926765\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.4990e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.50166005268693e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.0726e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.0756469641346484e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010872478014789522\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.4614e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.459934618556872e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.3975e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.398919554660097e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004183929122518748\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.4201e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.4199778636102565e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.3889e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.3887978639104404e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.0902e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.090366175863892e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.8230e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.82496465742588e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012154125579399988\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.4198e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.421365181566216e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.3238e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.32260081754066e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.8032e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.801457594614476e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.9011e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.8996218538377434e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.9949e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.996406980557367e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003437990671955049\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.5819e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.581998215755448e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.8930e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.8936610255623236e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022431743855122477\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.8389e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.841211431194097e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.9822e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.983072358299978e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.2673e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.2695870282186661e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020458406652323902\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.480972009128891e-05\n",
      "tensor([[1.0962e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.0967314665322192e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.2029e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.20503633399494e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00044296059058979154\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.0387e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.0381327784853056e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.5637e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.562686758115888e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.6152e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.616334576392546e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012100474850740284\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.0140e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.015712617430836e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.1292e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.1292929634219036e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.8067e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.80995430634357e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011444746633060277\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.5908e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.5928356990334578e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.0761e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.073897748137824e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018735494813881814\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001020483614411205\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.7171e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.7162487792083994e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.9075e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.9069949202239513e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00033586897188797593\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.0091e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.009752152953297e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.6514e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.6524417990003712e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011760687630157918\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019826470816042274\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.7778e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.7776150091085583e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.7275e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.7253597838571295e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.6594e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.661191997001879e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.8557e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.8533474657451734e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.2661e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.264644616749138e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.5764e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.576342714950442e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.8464e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.844279025448486e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.5520e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.5497327694902197e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.4754e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.4778167395852506e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.5682e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.570076559204608e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.6359e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.638758375425823e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.0149e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.014657366089523e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.8253e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.826691317954101e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.9717e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.9697482861811295e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001650588819757104\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[9.8596e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.859094279818237e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014622088929172605\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.4712e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.470448402571492e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.9739e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.976862591225654e-05\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003299661329947412\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0033]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0032848366536200047\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.768372718899627e-07\n",
      "tensor([[2.9968e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.998158561240416e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.1527e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.151388515718281e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001727491762721911\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010997661593137309\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001309599756496027\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011337445903336629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.3454e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.342490006412845e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011110922787338495\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.2098e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.2127420126926154e-05\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 8.344653679159819e-07\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010097536869579926\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.4711e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.4736233172006905e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.3904e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.389822374330834e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016303198935929686\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001657742541283369\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.0661e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.064833673415706e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.8927e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.891966379247606e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00029508653096854687\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012064707698300481\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.3699e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.368842100026086e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.4919e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.49429135187529e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011128806363558397\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.9891e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.989033186575398e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.9141e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.914378172950819e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.8893e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.890867108362727e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.0724e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.0710801840759814e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017179534188471735\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.1112e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.11420912516769e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.5489e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.547937714960426e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.3575e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.356920443475246e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.9404e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.938552097475622e-05\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.0728841743912199e-06\n",
      "tensor([[9.3918e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.394132939632982e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002708205720409751\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.1034e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.106560562737286e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010675761586753651\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.4726e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.4722455489390995e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.2851e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.2856656364165246e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.4434e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.446334686595947e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001134936828748323\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.7546e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.752391835907474e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.7880e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.787561430130154e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.4128e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.410568989347667e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011850105511257425\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011528202594490722\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.9115e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.912955748499371e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014777082833461463\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.2142e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.215302998200059e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.8094e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.8074182409909554e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001063403396983631\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013107919949106872\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.1785e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.179536573356017e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014300177281256765\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.5350e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.537197911413386e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018246646504849195\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.6989e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.69669602252543e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019796661217696965\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.1261e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.12613078171853e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010508851119084284\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.9410e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.942759526078589e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010061770444735885\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6.4190e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.419626151910052e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.0136e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.014657366089523e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011444746633060277\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.0975e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.097741061239503e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.9995e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.999263470992446e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.5404e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.541977250482887e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.3480e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.349837061250582e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.5382e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.540578472893685e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.9816e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.981380622368306e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002637853322084993\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010264446609653533\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.3360e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.3333515350241214e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.0778e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.078444519313052e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00031285409932024777\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.5323e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.5314641940640286e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.3465e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.3470740668708459e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.6801e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.681904051220044e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.8156e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.817582496092655e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.4227e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.421365181566216e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.3515e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.3512336560525e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.6705e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.6689440599293448e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014753238065168262\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.9217e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.921770884422585e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.7956e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.798032649792731e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019570119911804795\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000177577996510081\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012994656572118402\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012124319619033486\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.0541e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.051804378628731e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.4433e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.44489734643139e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001205874650622718\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000347376597346738\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.9565e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.95782662963029e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.0100e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.008328637108207e-05\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 5.960466182841628e-07\n",
      "tensor([[1.9417e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.9431303371675313e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.2286e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.2274641348049045e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.7985e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.796591671649367e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.1101e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.108248660690151e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001118245636462234\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.7752e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.77418860909529e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.9695e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.969458238221705e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.4665e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.467662670824211e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.0897e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.087568256887607e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.0453e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.0444602341740392e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.3448e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.342135020531714e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.4075e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.410568989347667e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.8146e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.811997572076507e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016702616994734854\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.5743e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.576342714950442e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.2486e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.251068695448339e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013090037100482732\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.0710e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.073897748137824e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011540124978637323\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.5237e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.522696715663187e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.9015e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.8996218538377434e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.9163e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.916101923095994e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.9229e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.92487704125233e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.5935e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.593919872306287e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.4005e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.401744030881673e-05\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00030212197452783585\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.2572e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.255862222635187e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010681722778826952\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.7404e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.738681673188694e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.6642e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.664021202595904e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019844355119857937\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.8196e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.819339351844974e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.7972e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.798032649792731e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.2355e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.236584598198533e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.4195e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.422490645898506e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00023391598369926214\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.3334e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.3351529560168274e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001625550794415176\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001675030798651278\n",
      "tensor([[0.9998]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.00015784555580466986\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012327000149525702\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.5868e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.586530071217567e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.5899e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.589663149090484e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013203300477471203\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.3190e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.3186476028058678e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.5432e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.544803909491748e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016148202121257782\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.7789e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.781817526440136e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.2077e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.2081763240275905e-05\n",
      "tensor([[0.4997]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.6937528252601624\n",
      "tensor([[0.0045]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004479113034904003\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0060]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005978418979793787\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0210]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.021264823153614998\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0296]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.030008619651198387\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0054]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005395937245339155\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0110]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.011062974110245705\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0026]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0025933007709681988\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0040]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004048724193125963\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0025]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002527208998799324\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001307268743403256\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0053]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005298859905451536\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015234624734148383\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0031]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003151190932840109\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0210]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.02118445746600628\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0047]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004733662120997906\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0027]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002694598166272044\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009265660773962736\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0046]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004595152102410793\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0051]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0051366062834858894\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0183]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.018488820642232895\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0027]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0026645364705473185\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0040]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003960275091230869\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001775289885699749\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0101]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.010111738927662373\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0028]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0028145546093583107\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0034]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0033557030837982893\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008036146173253655\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005803003441542387\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0018769224407151341\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0039]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00388410035520792\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0045]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004512282554060221\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0034]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0033676044549793005\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0020]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002038170350715518\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013203300477471203\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0021]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002088520908728242\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0025]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002532706595957279\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015190450940281153\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001075846259482205\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001054723747074604\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0021]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0020679144654423\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0059]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005917499307543039\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011539558181539178\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004273970262147486\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009554419084452093\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002515632368158549\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000610537885222584\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005124331801198423\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006806185701861978\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005914529319852591\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003959724272135645\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0062]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00625578174367547\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006528242956846952\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0028]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0028445010539144278\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0045]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00452120415866375\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013518526684492826\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0041]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004142987076193094\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0018952556420117617\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010489956475794315\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006625462556257844\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00027743849204853177\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011981251009274274\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0016992812743410468\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002944306761492044\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001016417983919382\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001062957919202745\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005696250009350479\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009259695070795715\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000504621013533324\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015585044166073203\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0018756684148684144\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0036]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0035658201668411493\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00091660296311602\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014440705999732018\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00036085202009417117\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0024]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0024141580797731876\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010707745095714927\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.2928e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.291349513456225e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002370757720200345\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00039698611362837255\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008150083012878895\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0053]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005273573566228151\n",
      "counter: 15000\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007120521040633321\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0104]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.010449877940118313\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009557998855598271\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006398221012204885\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007460515480488539\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001628353027626872\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0020]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002027001464739442\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0071]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.007168414071202278\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002876337675843388\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010423130588606\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007920420612208545\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0016641747206449509\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017092522466555238\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0031]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003124763024970889\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012216278119012713\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015901445876806974\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006226451369002461\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006241957889869809\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016857613809406757\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0034]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0033879983238875866\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003913810651283711\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00028232744080014527\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008436422795057297\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00030361252720467746\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001048518344759941\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006082119070924819\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002826255513355136\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0048]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004803074058145285\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009685078985057771\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0028]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002801524242386222\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005961048882454634\n",
      "tensor([[0.9969]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0031166314147412777\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000742770847864449\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.8853e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.887447620509192e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0022]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002207269659265876\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00024267994740512222\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004085540713276714\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020619372662622482\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000675966264680028\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015851896023377776\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018049914797302336\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002262848283862695\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005244795465841889\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007586376159451902\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0020]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001989972312003374\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009217933402396739\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002226481301477179\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00047009310219436884\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000969462504144758\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.576279254957626e-07\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001256544783245772\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00047104721306823194\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00041964513366110623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007376410649158061\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003767723392229527\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003767127054743469\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005346773541532457\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009050291264429688\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006885514594614506\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002568693889770657\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016613194020465016\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000276424951152876\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007825574721209705\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006400010315701365\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008046287111938\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008896373328752816\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006425059982575476\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006445934996008873\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005085568991489708\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004179158713668585\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00032925064442679286\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001560571399750188\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002875741629395634\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003285351558588445\n",
      "tensor([[0.9998]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0001721530279610306\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017513378406874835\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00085772096645087\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009976235451176763\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001320100505836308\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012470068759284914\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002258675085613504\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010328260250389576\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00035828808904625475\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016333007079083472\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005009833257645369\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005863835685886443\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009771590121090412\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007547006825916469\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000176504923729226\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004864922084379941\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003001544391736388\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021907106565777212\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013333503156900406\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001235114992596209\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007524340180680156\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004612078773789108\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.6702e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.672851981827989e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010210121981799603\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000463116099126637\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003390290657989681\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012809858890250325\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005008043954148889\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000266170158283785\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00037635493208654225\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00047844171058386564\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00034731696359813213\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003709885058924556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003587651008274406\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007391918916255236\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001450286217732355\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0033]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00332580110989511\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005887095467187464\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00046359316911548376\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006671985029242933\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000681930803693831\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00031637187930755317\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009091455722227693\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00040575151797384024\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021316892525646836\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003186972171533853\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015170533151831478\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010693645162973553\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005081394920125604\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003202474326826632\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005220345337875187\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001715568796498701\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007830943213775754\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018198954057879746\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00034171220613643527\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018520878802519292\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020309364481363446\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00047194171929731965\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010988786816596985\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016619155940134078\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0044]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004371707793325186\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008342168293893337\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002955635136459023\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004686022875830531\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00029496729257516563\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010991700401064008\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003936469438485801\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002222308103227988\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003259117074776441\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00027040322311222553\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000435327849118039\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011085453443229198\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00042805293924175203\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002120958233717829\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004720013530459255\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0016719362465664744\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002920457918662578\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00035828808904625475\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005644960911013186\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005578762502409518\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00027481516008265316\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009418987901881337\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015504370094276965\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019254154176451266\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005290715489536524\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006233011954464018\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019653582421597093\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00043652046588249505\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00038273504469543695\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006839587003923953\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000533186481334269\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.576279254957626e-07\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018246646504849195\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00033032387727871537\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0016726527828723192\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00043652046588249505\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.6907e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.689294579904526e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003772493510041386\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00032209575874730945\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005736207822337747\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000532649748492986\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001421030261553824\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017626646149437875\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010872478014789522\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004366993671283126\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 7.152560215217818e-07\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00040175640606321394\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015921668091323227\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.8395e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.838317833375186e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012540929019451141\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002128708438249305\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017280879546888173\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003341994888614863\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005892462795600295\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020023203978780657\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00034791321377269924\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004005041846539825\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014252486289478838\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016368775686714798\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00047575822100043297\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00024303766258526593\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006431621150113642\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006617708713747561\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014595306711271405\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.0054e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.005223935469985e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004247136530466378\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003754009085241705\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.6262e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.623724377481267e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021304968686308712\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007315569091588259\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017722031043376774\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004869692784268409\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008088044123724103\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020589564519468695\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005435036146081984\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011307640670565888\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00028876657597720623\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.5995e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.599881064379588e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.8515e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.850239489926025e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000848235737066716\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003635351895354688\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000104253958852496\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00046019413275644183\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003606731479521841\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.7184e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.7162487792083994e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016821845201775432\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000188606878509745\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003421891888137907\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00034457421861588955\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004233421932440251\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003700941160786897\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001367423974443227\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003622234216891229\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021424204169306904\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004737307026516646\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00026348725077696145\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00027201298507861793\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002422626130282879\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003871475055348128\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00026742220507003367\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005750521086156368\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015766671276651323\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012439474230632186\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012367262970656157\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00032191688660532236\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003111250407528132\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002829236618708819\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006404184969142079\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017012612079270184\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.4846e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.48519562673755e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001816914591472596\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011170533980475739\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010353863035561517\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001000547083094716\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002701051125768572\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015268651768565178\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005614545079879463\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003097536973655224\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001614800770767033\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006729840533807874\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017048382142093033\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012088552466593683\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002562135923653841\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011552047362783924\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002248539967695251\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005000291275791824\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005234061391092837\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.4173e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.417977707926184e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012726400746032596\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005590689834207296\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010860556358238682\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00047534078476019204\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010520773503230885\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.2382e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.2391470388975e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001504534448031336\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003896518610417843\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012493913527578115\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010824789205798879\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000409746658988297\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005725472583435476\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.7116e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.710068115964532e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00023558530665468425\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000551614270079881\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.5220e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.523827273165807e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004406349908094853\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021907106565777212\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004163655103184283\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011248028749832883\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00031714700162410736\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010091575677506626\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0026]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002634416101500392\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022419820015784353\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0016167709836736321\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.7299e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.731021469226107e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001918857597047463\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018097607244271785\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00031953194411471486\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00034743623109534383\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.3633e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.3631553126033396e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.3896e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.3884050430497155e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003299661329947412\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007949053542688489\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018210876442026347\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00036818606895394623\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011045350402127951\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015456679102499038\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00043908460065722466\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009424954187124968\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004297225968912244\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005483342101797462\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018926266056951135\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.6871e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.6882056772592478e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00024309728178195655\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.576279254957626e-07\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012657901970669627\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017173572268802673\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00038118474185466766\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019832431280519813\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012774091737810522\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013328486238606274\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00048553806846030056\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.3113e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.313757669180632e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017179534188471735\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00034266620059497654\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003429643402341753\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.3331e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.333076402777806e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022324430756270885\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00031249638414010406\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001704242022242397\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002538287953939289\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.8122e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.814474520273507e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005992657970637083\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.4760e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.477587445871904e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022497323516290635\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019969549612142146\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.8513e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.8519359552301466e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00025561737129464746\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011808377166744322\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014574397937394679\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015856092795729637\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00026330837863497436\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 9.536747711536009e-07\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016589349252171814\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010926128015853465\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021072462550364435\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.4993e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.499983232468367e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.1652e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.163310172269121e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002790482831187546\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.1172e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.118482219288126e-05\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[7.6634e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.665451266802847e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011128806363558397\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018157223530579358\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005441596149466932\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.1934e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.191458957502618e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005174425896257162\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010073692101286724\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021644789376296103\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000511001970153302\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.1807e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.182617991929874e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.0427e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.0426799134584144e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00031011144164949656\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009759656968526542\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016428389062639326\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017328571993857622\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004750426160171628\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.8002e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.801121344324201e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020661104645114392\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00042405774001963437\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00047611602349206805\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003382539434824139\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.9667e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.964940207079053e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013555014447774738\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.3749e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.373366679530591e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00034326245076954365\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000281552376691252\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002772000152617693\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022646368597634137\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005844751140102744\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011253989941906184\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002720726188272238\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003833313239738345\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020917457004543394\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019260114640928805\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6.1183e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.115623546065763e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.9993e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.999263470992446e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004711068468168378\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00027129752561450005\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004164251440670341\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006569993565790355\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00025579624343663454\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011163025628775358\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.0332e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.033595466054976e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.6854e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.6868256352609023e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006011146469973028\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.4285e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.428723332239315e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014860542432870716\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.8053e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.8074182409909554e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.4141e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.416530181420967e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00048380871885456145\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003052223473787308\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017471647879574448\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011146689939778298\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013346370542421937\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012839664123021066\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002435742353554815\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019826470816042274\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000849369156640023\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001426440867362544\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018234722665511072\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013018501340411603\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.7788e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.781817526440136e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001319733855780214\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00039680724148638546\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017991147469729185\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013209260941948742\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.1315e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.130403875838965e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00026295066345483065\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00039120219298638403\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003002140438184142\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00032227463088929653\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016482041974086314\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001600512769073248\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012774091737810522\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.5546e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.555307143367827e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.6446e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.641910006990656e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.6474e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.6492703404510394e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022753681696485728\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010055809252662584\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002551404177211225\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00027869053883478045\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011498397361719981\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.2763e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.27491273614578e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010145224950974807\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00038893631426617503\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017388186824973673\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001555206108605489\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5.3104e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.31091500306502e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013680201664101332\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020047051657456905\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.2608e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.260427183704451e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002037494268734008\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019969549612142146\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015224184608086944\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.4827e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.48066304367967e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.9569e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.957536581670865e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002611620584502816\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000205299467779696\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.8028e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.804249692824669e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016941074864007533\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017125881277024746\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001497380726505071\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.7562e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.7537724236026406e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.6838e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.681904051220044e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00027779623633250594\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.2895e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.289913628483191e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.9899e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.987341086845845e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.7749e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.777278031222522e-05\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005748731782659888\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001955223415279761\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00032227463088929653\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.6182e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.616334576392546e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.2285e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.228879647096619e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012994656572118402\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003662183880805969\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.4307e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.4318991563632153e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.7873e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.7863675717962906e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.1526e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.149974094820209e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.8846e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.886298509314656e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001019887495203875\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001759087754180655\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019701276323758066\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001243430160684511\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00034803248126991093\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004890564596280456\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00033914833329617977\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005148782511241734\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00027040322311222553\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010496928734937683\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005762449000030756\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003017046255990863\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00023057735234033316\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011546086170710623\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001452074502594769\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.4950e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.495471022091806e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.8980e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.897927571320906e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001822280028136447\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.7274e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.728170435759239e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.8367e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.838317833375186e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.0808e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.081607792410068e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.5382e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 9.537197911413386e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016458197205793113\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022676178195979446\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017400109209120274\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003319337556604296\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003151794080622494\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.7247e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.722209607483819e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.7680e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.765356374671683e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006400606362149119\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.5537e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.555693415866699e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00023153124493546784\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018204915977548808\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00023206780315376818\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.9963e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.999263470992446e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013906729873269796\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001384115603286773\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004674096417147666\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010186952567892149\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.6410e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.638535266276449e-05\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.3113030945532955e-06\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001317349378950894\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001227335014846176\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00038804192445240915\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001806780055630952\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.5523e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.552193710580468e-05\n",
      "counter: 16000\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014592282241210341\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.4221e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.421053305733949e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002156132395612076\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00041952586616389453\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00037707044975832105\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.6748e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.67430169112049e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013352332462091\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00025126515538431704\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.8390e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.83688631397672e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.2935e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.296191061963327e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020625334582291543\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.8385e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.841425809194334e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001164146451628767\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.1459e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.146852294681594e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019158767827320844\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.4227e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.422762503963895e-05\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 7.152560215217818e-07\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001179645478259772\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021185734658502042\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.9693e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.9666028391802683e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011671270476654172\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.3452e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.348096212605014e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.4931e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.49429135187529e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015236108447425067\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.0906e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.090121602639556e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000538553751539439\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011927601008210331\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001732261007418856\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016833769041113555\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011981251009274274\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.7907e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.790927439695224e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004231036582496017\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.7918e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.793739182990976e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.5272e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.5258905477821827e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00043025927152484655\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000158858994836919\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.0574e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.057438779156655e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.7681e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.76822741702199e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012082591274520382\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009019268909469247\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010026004019891843\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.7864e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.789199687773362e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012321039685048163\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022378088033292443\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010061770444735885\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013793466496281326\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.1275e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.1292929634219036e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011176495172549039\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010508851119084284\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.4613e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.461702206346672e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010508851119084284\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.7604e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.759395182598382e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011975289817200974\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.7701e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.76822741702199e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.4166e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.416530181420967e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00037874002009630203\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.5200e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.522696715663187e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001754914701450616\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007143187103793025\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013578860671259463\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.4619e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.46278019505553e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003866704646497965\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011391096631996334\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.3237e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.321154746226966e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005884709535166621\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.0724e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.072484054835513e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00033324549440294504\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.0152e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.012876135879196e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00047718940186314285\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013787504576612264\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003497615980450064\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.0076e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.006915671401657e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.4715e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.4690503525780514e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010902283975156024\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001899780472740531\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.8077e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.808809196925722e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.5039e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.505944424541667e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.9962e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.998158561240416e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00023653919924981892\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.6164e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.614691225579008e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005308010149747133\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014234601985663176\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.0728841743912199e-06\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002367776760365814\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.3540e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.3544111172668636e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.1461e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.1444157280493528e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.5371e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.540272054029629e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010115419718204066\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012583332136273384\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021698445198126137\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010914206359302625\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017507416487205774\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.6152e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.613242243882269e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002720726188272238\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.7096e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.7102879509329796e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.0005e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.997830496402457e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.5478e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.5451507099205628e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.0063e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.003790960879996e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007108591962605715\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010794983973028138\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.576279254957626e-07\n",
      "tensor([[6.8358e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.83688631397672e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.6572e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.6584024453768507e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012815819354727864\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015635520685464144\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.1327e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.130403875838965e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.2467e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.248505890951492e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010669800394680351\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.4633e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.463089888100512e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001601705007487908\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.9653e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.9623946829815395e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.7789e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.778708823025227e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00026742220507003367\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.8689e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.869564771885052e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.8360e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.835250239120796e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.6457e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.6464813345228322e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010401551116956398\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.7738e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.771654362732079e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.3523e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.352406050311401e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010496928734937683\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018872611690312624\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002506689343135804\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013590783055406064\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.0010e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.0009548431262374e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.1930e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.194539648480713e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003058185975532979\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014228641521185637\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.9559e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.95782662963029e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015146686928346753\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.887592695013154e-06\n",
      "tensor([[7.4058e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.40317118470557e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.4410e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.443469465011731e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002945499145425856\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.8205e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.8221321776509285e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016178008809220046\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.6798e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.6808651707833633e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018979920423589647\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000150393825606443\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00046621699584648013\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.7429e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.746052920585498e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00026634903042577207\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020345134544186294\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004579877422656864\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.6908e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.689294579904526e-05\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.7881409348774469e-06\n",
      "tensor([[4.7581e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.7565637942170724e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.5008e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.501431486569345e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.7566e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.7537724236026406e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001044923992594704\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013972305168863386\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.2507e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.2530810383614153e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.2194e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.2187024771701545e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005217959405854344\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.1926e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.190294202999212e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.4432e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.44489734643139e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.7945e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.795160152250901e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014413442113436759\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.5250e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.522696715663187e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.8844e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.882938320515677e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005455908831208944\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.5145e-06]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.536788638797589e-06\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.0530e-06]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.07969241173123e-06\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.1096e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.109663081588224e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003863127203658223\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.7875e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.789199687773362e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.3860e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.3842141672503203e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018985880888067186\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.8408e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.839759539114311e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.0975e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.096082794712856e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.1919e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.190013613784686e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.2425e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.2411597456084564e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.5402e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.5374246585415676e-05\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[9.5954e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.596808376954868e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013787504576612264\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.8100e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.811405470827594e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012476030678953975\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012732362665701658\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.5899e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.5914567484287545e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.6308e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.632797547150403e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015695134061388671\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.7186e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.7207991883624345e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010276368993800133\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015903783787507564\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003305623831693083\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022103845549281687\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010544617543928325\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003606731479521841\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.1251e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.123008981579915e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.8849e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.882938320515677e-05\n",
      "tensor([[0.9988]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0011985328746959567\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00023731424880679697\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.8002e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.802552136126906e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.9244e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.926299829501659e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.9265e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.927732076495886e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0016875789733603597\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003018238639924675\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.9987e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.999551699962467e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.2691e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.269189568934962e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.3341e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.33163979020901e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003787996538449079\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019605889974627644\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.0621e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.060316369868815e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.7523e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.755163197638467e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003956146538257599\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.0182e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.020617830567062e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.9148e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.9147096938686445e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00023266399512067437\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004962721723131835\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003652643645182252\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.6069e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.605842256452888e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.7928e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.79207145771943e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012249505380168557\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013250990014057606\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001555206108605489\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.1594e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.1604907892178744e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.0888e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.087243284331635e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00023612187942489982\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015641482605133206\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001452074502594769\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00023123314895201474\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.7242e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.725060277152807e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015306856948882341\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.7794e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.7790057831443846e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001306619233218953\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022258851095102727\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.9163e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.915810419945046e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.9532e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.95447117020376e-05\n",
      "tensor([[0.9568]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.044158730655908585\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00030868049361743033\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.0364e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.035315942019224e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002925227745436132\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00024655519519001245\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001512284216005355\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00023021963716018945\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020684950868599117\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010622111585689709\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001256544783245772\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010312135418644175\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.0288e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.0307586207054555e-05\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.4305124977909145e-06\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015057266864459962\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.9754e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.976862591225654e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004914418095722795\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.9999e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.999263470992446e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005958662950433791\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.4775e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.47923443466425e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014681702305097133\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.3998e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.398647332796827e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00031750474590808153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.2973e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.29875677684322e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012583332136273384\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001913492160383612\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006143549107946455\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00031780285644344985\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004652628558687866\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003650854923762381\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.0511e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.051478314679116e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001657742541283369\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.0193e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.018836964154616e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018729532894212753\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019999357755295932\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001151031901827082\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.9119e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.9147096938686445e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.3990e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.398647332796827e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001119437874876894\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016392620455008\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003800518170464784\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.3406e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.343562174355611e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.4423e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.4438202672172338e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002927016466856003\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012696595513261855\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.8626e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.8638572479831055e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.1228e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.124726547161117e-05\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.7881409348774469e-06\n",
      "tensor([[9.5158e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.513353870715946e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002065514272544533\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011885872663697228\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018133377307094634\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013966343249194324\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010735372779890895\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00024709178251214325\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.0430e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.0444602341740392e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013668279279954731\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009056853596121073\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.9990e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.998158561240416e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021132078836672008\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011814338358817622\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011289757094345987\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013405982463154942\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005418337532319129\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005415952182374895\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013978265633340925\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015367257583420724\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009603938087821007\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.1297e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.127545202616602e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.5292e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.528349669883028e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001787106884876266\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005795250181108713\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.9575e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.960432362277061e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.2481e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.2499013943597674e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.9808e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.982823055703193e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.9605e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.962064799154177e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012732362665701658\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.2990e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.30076584557537e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013847117952536792\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010818828013725579\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012708517897408456\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.3860e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.386724948650226e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.2486e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.251307084108703e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000104194346931763\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005760063068009913\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.9341e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.932260293979198e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010783061588881537\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001632108323974535\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001205278531415388\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002941921993624419\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019957625772804022\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013143688556738198\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.7458e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.748904317850247e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.5849e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.583702320815064e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.6197e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.619203435955569e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002351679722778499\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002571078948676586\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.9033e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.905582682113163e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016219739336520433\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012905237963423133\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019391271052882075\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011540124978637323\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001276812981814146\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004168425512034446\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.8665e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.8684163882862777e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.8589e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.859094279818237e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00038887670962139964\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.5290e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.526921788463369e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003111250407528132\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.9110e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.911543510388583e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000154745634063147\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00024786681751720607\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018699724751058966\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00029717330471612513\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001626743032829836\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001022271899273619\n",
      "tensor([[0.9996]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.00041189329931512475\n",
      "tensor([[9.1965e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.197419421980157e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017161649884656072\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.9212e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.922062387573533e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.6606e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.662601688527502e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019003766647074372\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002145401231246069\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.6513e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.652099546045065e-05\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[8.7917e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.79207145771943e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013638472591992468\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.6937e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.69669602252543e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.2702e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.269189568934962e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.2905e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.288487929850817e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.1894e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.191458957502618e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010026004019891843\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.9973e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.999551699962467e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.6270e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.625427391147241e-05\n",
      "tensor([[0.9943]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.005695438012480736\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014067685697227716\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.0132e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.011472992715426e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.0385e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.036472329171374e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003730754542630166\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.2869e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.285389048978686e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.5288e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.5314641940640286e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005008640582673252\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013453673454932868\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.9934e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.996198787121102e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000111645735159982\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.2463e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.246761768124998e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.3618e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.3631553126033396e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000188606878509745\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002492977073416114\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017692222900222987\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.5045e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.506212644628249e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.2056e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.203380614053458e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00045560247963294387\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016285314632114023\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.3758e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.374803292099386e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.9471e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.945614925120026e-05\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.576279254957626e-07\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011927601008210331\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005112405051477253\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002973521768581122\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00034171220613643527\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015319566591642797\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002301003987668082\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.0437e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.042433521244675e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002513843937776983\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012327000149525702\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005054558860138059\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.7991e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.7982888645492494e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016034934378694743\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013322525774128735\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000271536031505093\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.3091e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.310679160989821e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002968751941807568\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017340494378004223\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.1191e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.12016995344311e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015444756718352437\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.0940e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.094922769581899e-05\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 5.960466182841628e-07\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001697088300716132\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.9541e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.951575389597565e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.9807e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.983072358299978e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00023469101870432496\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.0476e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.048640741733834e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.0321e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.033595466054976e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010604228737065569\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002650969836395234\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010139263758901507\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013763659808319062\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.3397e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 6.342135020531714e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.7991e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.801457594614476e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014469954185187817\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.1567e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.1545299609424546e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010586345160845667\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.5944e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.594224835978821e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.0533e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.051804378628731e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.5034e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.504506356781349e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020673027029260993\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.6795e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.67881317390129e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002940133272204548\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.6859e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.684773638378829e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021072462550364435\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002456609217915684\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014729391841683537\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00043795161764137447\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.7364e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.735551869496703e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.2536e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.251307084108703e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022062112111598253\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.7193e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.72054006322287e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.9308e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.9280232158489525e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.6297e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.631388219422661e-05\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.57628505298635e-06\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00033890982740558684\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.2727e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.270605081226677e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.3126e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.315469413995743e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00042131476220674813\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.2728e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.275150033412501e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011528202594490722\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000123031553812325\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014312099665403366\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002575252146925777\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001164742570836097\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001020483614411205\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011391096631996334\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019301846623420715\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.4214e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.422762503963895e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.7029e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.705747364321724e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003842853766400367\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013042346108704805\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.6952e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.69669602252543e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.1626e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.163310172269121e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011635503324214369\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.4272e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.430130477179773e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.6368e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.6345600417698734e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00045804737601429224\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.2229e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.221503306529485e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003148812975268811\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.9041e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.903888035798445e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.1442e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.143770876107737e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.3003e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.2993897144042421e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.5915e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.594224835978821e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014473055489361286\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000574038247577846\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002215153945144266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.9272e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.926630804722663e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014473055489361286\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020249748195055872\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003449915675446391\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011575892131077126\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.94730775244534e-05\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010276368993800133\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012511797831393778\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.0055e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.006667096400633e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.6557e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.6584024453768507e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.7433e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.741851312748622e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011391096631996334\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00029341710614971817\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001753722463035956\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00028495080186985433\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011581853323150426\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011408979480620474\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.9489e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.947057358454913e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010461162310093641\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.4649e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.464487574296072e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013459633919410408\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.0791e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.078444519313052e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00023570454504806548\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.3697e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.370522558223456e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006247325800359249\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002757690963335335\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018288377032149583\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.9017e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.903888035798445e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.7553e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.757756197359413e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019605889974627644\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.9944e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.993302278919145e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000316014135023579\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.5790e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.577741492539644e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.5629e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.563032649050001e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011176495172549039\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002007685980061069\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.7295e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.731021469226107e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.9262e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.929174509830773e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.7716e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.771654362732079e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020297440642025322\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.3009e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.300409586401656e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002496554225217551\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.1515e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.149731340585276e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.5688e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.5672287999186665e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.4051e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.4034830605378374e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010121380910277367\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.7941e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.793739182990976e-05\n",
      "counter: 17000\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010139263758901507\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.4584e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.4571290598250926e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.4483e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.446334686595947e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.9801e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.981380622368306e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019540311768651009\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010902283975156024\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011075156362494454\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001990397140616551\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00026444115792401135\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007621569093316793\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005940771079622209\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.1819e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.179777872399427e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.3306e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.3287971240933985e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021913068485446274\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00048368945135734975\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.4374e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.435860556550324e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00036860344698652625\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00023522759147454053\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005300257471390069\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014944000577088445\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002714167640078813\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.5302e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.529788465239108e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.9701e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.971151065547019e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016964921087492257\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.3957e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.395782838808373e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002300407795701176\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014443248801399022\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.7024e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.70432712265756e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000300094805425033\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.4651e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.46731277811341e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.8342e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.832357368897647e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002512055216357112\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.8042e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.805660839541815e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014932078192941844\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.4839e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.48519562673755e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.3691e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.368842100026086e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018878573609981686\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.7404e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.7400920923100784e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00024458777625113726\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.9640e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.963497773744166e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001254160306416452\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017435879271943122\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017602801381144673\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.5494e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.552193710580468e-05\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 8.344653679159819e-07\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000131556109408848\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.3352e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.3333515350241214e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013334448158275336\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.7710e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.7730453186668456e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003597787581384182\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.8524e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.8551032301038504e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.5893e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.589398203184828e-05\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[7.5673e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.570076559204608e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.6997e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.696956239058636e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.7438e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.744642137666233e-05\n",
      "tensor([[0.9984]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0015789808239787817\n",
      "tensor([[9.2879e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.28683512029238e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001418095052940771\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001149243616964668\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.7979e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.8000764612224884e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.5721e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.574608414666727e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003126752271782607\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.3111e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.309508585720323e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00030164499185048044\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.4215e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.4241696489043534e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.9522e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.950143142603338e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013984227553009987\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008150679641403258\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010528144193813205\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000202020542928949\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.4851e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.483548637945205e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019540311768651009\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.6136e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.614691225579008e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.0122e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.014657366089523e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007569077424705029\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.5947e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.594224835978821e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.9730e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.975419430295005e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.5610e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.5598593715112656e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.6436e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.641910006990656e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015862053260207176\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.7792e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.781817526440136e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015319566591642797\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002132881636498496\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011695115244947374\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.1170e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.117371670668945e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.5916e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.5910721130203456e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.0699e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.0679369198624045e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013286758621688932\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.0282e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.026211121934466e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.4708e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.4718562751077116e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.4601e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.463089888100512e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.7158e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.7162487792083994e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.4594e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.459934618556872e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021090346854180098\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022837147116661072\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015915706171654165\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.7996e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.798032649792731e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.8916e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.8936610255623236e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001601705007487908\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018985880888067186\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.8020e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.802848732448183e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014157105761114508\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.7725e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.7730453186668456e-05\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[2.2227e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.2232779883779585e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.8579e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.857642387738451e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022181347594596446\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.3689e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.3663324100198224e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002585983893368393\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002308158145751804\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.6486e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.6492703404510394e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.0274e-06]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.033372639853042e-06\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.3575e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.355483830906451e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.7395e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.7372810766100883e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00044856593012809753\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00027785584097728133\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001264890597667545\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.9073e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.908749047492165e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.9838e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.9862372684874572e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.6680e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.668562516802922e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.9609e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.958979742601514e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.1367e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.138052438269369e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020357058383524418\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001240449637407437\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.7895e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.79207145771943e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.4870e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.48973839613609e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.6906e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.689294579904526e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.9254e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.9280232158489525e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.4805e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.480972009128891e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.6903e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.6909954107832164e-05\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.576279254957626e-07\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019379347213543952\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.3891e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.386724948650226e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002597907732706517\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003712270117830485\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00023421407968271524\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.1018e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.103701889514923e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.6180e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.6152989701367915e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.0815e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.081607792410068e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014866502897348255\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.1999e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.2008207199396566e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016082626825664192\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.4375e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.437509000534192e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.0958e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.093529085163027e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.8029e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.801121344324201e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.4785e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.47958396357717e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.6546e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.654969133203849e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014079608081374317\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.6842e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.6836347135249525e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.9612e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.96378745790571e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00029878312489017844\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015343412815127522\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.5723e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.5717806642642245e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008221070747822523\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.5428e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.540272054029629e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011766648822231218\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.9848e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.984854316106066e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012505835911724716\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.0188e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.0174338209908456e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.9686e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.966393554350361e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.6599e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.661191997001879e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.9933e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.996198787121102e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.8906e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.891966379247606e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.1039e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.103701889514923e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012946967035531998\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.5762e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.574608414666727e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016941074864007533\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.7181e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.719099085079506e-05\n",
      "tensor([[0.4869]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.7196749448776245\n",
      "tensor([[0.0242]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.024517688900232315\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0153]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.015413870103657246\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0018425262533128262\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0040]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0039998311549425125\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0041]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00408666767179966\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 7.951575389597565e-05\n",
      "tensor([[0.0049]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00487111322581768\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015238206833600998\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0057]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005738239735364914\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0023]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002300641965121031\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0021]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0020925228018313646\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0033]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0033063653390854597\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0052]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005210180301219225\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0091]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.009185409173369408\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0049]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004905973095446825\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0035]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003516112919896841\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0097]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00976781640201807\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00097829254809767\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0058]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0058199516497552395\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007724764291197062\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0047]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004703659098595381\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006100011523813009\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011586700566112995\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0036]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0036121199373155832\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0045]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004522162023931742\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0081]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.008112405426800251\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004342544998507947\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.576279254957626e-07\n",
      "tensor([[0.0028]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002823998685926199\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006559257744811475\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014868698781356215\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0029]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002891185926273465\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0024]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002443494973704219\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0126]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.012691949494183064\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012654917081817985\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007128275465220213\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017171335639432073\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0018690995639190078\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0044]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004432952497154474\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0034]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003386144293472171\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007643043645657599\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00167653348762542\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0028]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002758848015218973\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0045]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0045519801788032055\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009602744830772281\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0024]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002426944440230727\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004522630770225078\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0096]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.009597313590347767\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0027]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0027124083135277033\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0028]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002771638799458742\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000927997927647084\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0040]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0039726621471345425\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014192992821335793\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0085]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.008586508221924305\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014738567406311631\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0016306814504787326\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001631512277526781\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017982787685468793\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0038]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003839941695332527\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0019279217813163996\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0020]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001978326356038451\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00036085202009417117\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0026]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002572504570707679\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012753986520692706\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009016285766847432\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0034]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0033708936534821987\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007464094087481499\n",
      "tensor([[0.9985]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0014805423561483622\n",
      "tensor([[0.0020]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001996780978515744\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005377188208512962\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00045178603613749146\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001929354970343411\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006951125105842948\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0016407710500061512\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0029]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002888496033847332\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012110649840906262\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009037166018970311\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001517791417427361\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005375995533540845\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001599875744432211\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0026]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002614635042846203\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0025]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0025098202750086784\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00041081997915171087\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0036]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0035831076093018055\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006758469971828163\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003990134864579886\n",
      "tensor([[0.9993]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0006812746869400144\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020082821720279753\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009747724398039281\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.3996e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.401744030881673e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0025]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0025348577182739973\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011947136372327805\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0030]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002970334142446518\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006168598774820566\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010744142346084118\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003095152205787599\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0021]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002088520908728242\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0018928071949630976\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0022]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002243530238047242\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0032]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00321104540489614\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0025]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0024659016635268927\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001285754842683673\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0024]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002452816115692258\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005142819136381149\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008895776700228453\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012702555977739394\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003268060681875795\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00023588340263813734\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010298427660018206\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014173295348882675\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004319289291743189\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00134242232888937\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009866454638540745\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007884033257141709\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00044355692807585\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008935150690376759\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014055707724764943\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013822328764945269\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.5497220147153712e-06\n",
      "tensor([[0.0023]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002318743849173188\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015295721823349595\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002586579939816147\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001146974042057991\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014789903070777655\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006187683902680874\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017465685959905386\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011417825007811189\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009168415563181043\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0124]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.012480269186198711\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004893546574749053\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0021]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002084996784105897\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008890407043509185\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000340758211677894\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002512055216357112\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004022334178443998\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005542979924939573\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005172636592760682\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0016853101551532745\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008185874903574586\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008045690483413637\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00043813048978336155\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00023200818395707756\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008963786531239748\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00024267994740512222\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004427220846991986\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00029246314079500735\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001296485133934766\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006548521923832595\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008191840606741607\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016070702986326069\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015466843033209443\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001321891089901328\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004293051897548139\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00045130899525247514\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001801414618967101\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.609338323760312e-05\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001812145346775651\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007817820296622813\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008185874903574586\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00041123738628812134\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009171995334327221\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013135353801771998\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018973958503920585\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009163046488538384\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017789322882890701\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00047379033640027046\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003174451121594757\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005170847289264202\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00029311899561434984\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001184986555017531\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003470188530627638\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001305426994804293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000267302937572822\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00023749310639686882\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0021]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0021249563433229923\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001954627368832007\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010422533378005028\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000971431378275156\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006577747408300638\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000539090484380722\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022461551998276263\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001540898811072111\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008299217442981899\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007938316557556391\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010890361591009423\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00046657476923428476\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011721564223989844\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000212632367038168\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00039370657759718597\n",
      "tensor([[0.7604]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.273951917886734\n",
      "tensor([[0.0206]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.020844677463173866\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0053]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005322529003024101\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0054]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005463417619466782\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0105]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.01058691181242466\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0032]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0031861106399446726\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0026]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002616726793348789\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0041]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004118807148188353\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000502891605719924\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0026]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0026360296178609133\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0025]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002537188120186329\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0024]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0024290953297168016\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0047]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004713539965450764\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0021]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0020799199119210243\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0018778181402012706\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0028]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002762195188552141\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006862252485007048\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0043]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004298854153603315\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008304586517624557\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0083]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.008351952768862247\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0020]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0019865084905177355\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0020]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002021148568019271\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0027]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002737630158662796\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0026]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0025813488755375147\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008451933390460908\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0022]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0021625282242894173\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0026]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002556190825998783\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0037]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0036869579926133156\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0041]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004138079006224871\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0016648911405354738\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.768372718899627e-07\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006021881708875299\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0018553650006651878\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006414324161596596\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0105]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.010514265857636929\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006986316293478012\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0029]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002928128931671381\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00048166190390475094\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.0\n",
      "tensor([[0.0022]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0022044023498892784\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0059]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005904848221689463\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008799131610430777\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005205436027608812\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.2649790025752736e-06\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010039479238912463\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005359297501854599\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003774282231461257\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022103845549281687\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015581462066620588\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017683635232970119\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0030]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0030274277087301016\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000875200261361897\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015952788526192307\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.5497220147153712e-06\n",
      "tensor([[0.0034]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0034468499943614006\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017847241833806038\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021346702123992145\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015422070864588022\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000323586369631812\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006013532401993871\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006825868622399867\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013688033213838935\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010695811361074448\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0020]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001961902715265751\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015504370094276965\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00034982123179361224\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007978879730217159\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0083]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00832833256572485\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007232658099383116\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010884961811825633\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004672903742175549\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00024864188162609935\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005869799642823637\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000551614270079881\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00033676333259791136\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0026]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0026016072370111942\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008235984132625163\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0045]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00454216031357646\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015668621053919196\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017976219533011317\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005159516585990787\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001288139319512993\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00045285941450856626\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004034260055050254\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001905145909404382\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015295721823349595\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011033540358766913\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0029]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0028709815815091133\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006649320130236447\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002490592305548489\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013648640597239137\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004122510727029294\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0018952556420117617\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007072803564369678\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9.8974e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.900821896735579e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00023218704154714942\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00044534585322253406\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011800334323197603\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011179137509316206\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0028]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002769546816125512\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0026]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002635491779074073\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0036]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0036088298074901104\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015592804411426187\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010869447141885757\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013456454034894705\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0030]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002986116800457239\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000586860696785152\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003327684826217592\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.4132e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.413665687432513e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010883171344175935\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004859555047005415\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0023]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0022950859274715185\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0039]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00387973221950233\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00036335631739348173\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011001914972439408\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000959498924203217\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000850144715514034\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004552446771413088\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001204682412208058\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00030140651506371796\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002016628423007205\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012499153381213546\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002334986493224278\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005658677546307445\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003346168377902359\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00027123792096972466\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006234800675883889\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003259117074776441\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003557837917469442\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001407779287546873\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00033574970439076424\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002657528093550354\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004272777587175369\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009712523897178471\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00025376916164532304\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011747820535674691\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00047969401930458844\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008249108213931322\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.1603e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.161895387573168e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006292057223618031\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009701187955215573\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006295039202086627\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001518245553597808\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003430239448789507\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021459974232129753\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003548297972884029\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003551875415723771\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00028805111651308835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.98023678624304e-06\n",
      "tensor([[0.0023]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0023256740532815456\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002800618240144104\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012378005776554346\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012696595513261855\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013441751070786268\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002490592305548489\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002684357459656894\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.8358e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.835464980918914e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013317986158654094\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005388519493862987\n",
      "counter: 18000\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012362767301965505\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0019322813022881746\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004657995596062392\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007065049721859396\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000917378521990031\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00026038699434138834\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021406318410299718\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001691126817604527\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012453797971829772\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019450887339189649\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005687900702469051\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001998743537114933\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.7005e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.699786172248423e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002145401231246069\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.4305124977909145e-06\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007765923510305583\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009009126806631684\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000415649963542819\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.1410e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.140891102608293e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00023147162573877722\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00045423093251883984\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005582937155850232\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00039155996637418866\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0036]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0036123592872172594\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.1190e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.1219479094725102e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005909758037887514\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00023361788771580905\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008435826166532934\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008737088064663112\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001638665999053046\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019778776913881302\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00038780338945798576\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014723431377205998\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00029019752400927246\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009874210227280855\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022741757857147604\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004139803349971771\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0034]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003435187041759491\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003829139459412545\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001676223037065938\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0059]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005895854439586401\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005201858002692461\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002910322218667716\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0034]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0034420054871588945\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.0\n",
      "tensor([[5.4511e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.453973790281452e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003962705668527633\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015999165771063417\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001657742541283369\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004997309879399836\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011528202594490722\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.2318e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.230304618133232e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004214340297039598\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003061167080886662\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021316892525646836\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002502516144886613\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004061689251102507\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013610442401841283\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009395123925060034\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008034356287680566\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022640406677965075\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000417498464230448\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00024178567400667816\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000551972130779177\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017000689695123583\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004537538916338235\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004476118483580649\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012265810510143638\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005374206812120974\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021245352400001138\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00039144069887697697\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001087839831598103\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013322525774128735\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005183370667509735\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00039257362368516624\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001097148284316063\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006569993565790355\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00045566208427771926\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013596743519883603\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021108232613187283\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007858979515731335\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00028978014597669244\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004797536530531943\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005412970203906298\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003022412129212171\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014145181921776384\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00047474444727413356\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003144043148495257\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018258568888995796\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.7908e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.793739182990976e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012733099283650517\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001576070935698226\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022980230278335512\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015850130876060575\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008293251739814878\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00025967153487727046\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014049801393412054\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005358104826882482\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010439240140840411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017173572268802673\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00026712409453466535\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00031583529198542237\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016768192290328443\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00037909779348410666\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.1470e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.148286724463105e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005690882098861039\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005777358892373741\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012917160347569734\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00037033262196928263\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004517264023888856\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012368729221634567\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002159113355446607\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.8215e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.819004192948341e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018276454648002982\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008203174802474678\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00032501734676770866\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004139803349971771\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0032]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003217682708054781\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001519437792012468\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00035620119888335466\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006067208596505225\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00036973634269088507\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002069687470793724\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00026742220507003367\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.6214e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.622295040870085e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.1962e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.1962550312746316e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004796343855559826\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010562750976532698\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014842658129055053\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017996261885855347\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.0421e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.041276406496763e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005608581705018878\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014079608081374317\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013423866766970605\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001401403424097225\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002820889640133828\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013362749014049768\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00033139713923446834\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00034892684197984636\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000683004385791719\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00032424222445115447\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002301003987668082\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.2641e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.26299107959494e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00027159563614986837\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002872164186555892\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.3194e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.318291707430035e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018801071564666927\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004185121797490865\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.2130e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.212421769509092e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001474131568102166\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001822280028136447\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011903756239917129\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013657594099640846\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016947036783676594\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002406529092695564\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004339563602115959\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.3489e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.348096212605014e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010800945165101439\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005553714581765234\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020863802637904882\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003975823929067701\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001626146986382082\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002755902532953769\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00023361788771580905\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002788097772281617\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002564520691521466\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010014081635745242\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00023826815595384687\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001721530279610306\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003255539631936699\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00036073275259695947\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001994570338865742\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011230145901208743\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010878994362428784\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0018487365450710058\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00040843483293429017\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005244795465841889\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011534163786564022\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004954372998327017\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003500597376842052\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.5352e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.537197911413386e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00045768957352265716\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016428389062639326\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000735612993594259\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002630699018482119\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012214487651363015\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002479264512658119\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004123107064515352\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016643002163618803\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004578684747684747\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.4937e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.495471022091806e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017972415662370622\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.6533e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.654969133203849e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00023492949549108744\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00036681463825516403\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016768192290328443\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011027467553503811\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00027940599829889834\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016350891382899135\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.4251e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.425587343983352e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009034780086949468\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003666953998617828\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00026658750721253455\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00042560813017189503\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003157160244882107\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010759216820588335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003500597376842052\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002566309121903032\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005730243865400553\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000527282478287816\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.0325e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.033595466054976e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.7955e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.798032649792731e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000325315457303077\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020345134544186294\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007139011868275702\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000665170606225729\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00029508653096854687\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019433001580182463\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.0167e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.017146319616586e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.6530e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.6524417990003712e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001622263458557427\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002599696454126388\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000647217791993171\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015081111632753164\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002702839847188443\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0020]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0020291516557335854\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003188164555467665\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005049192113801837\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00023284285271074623\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.8260e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.8235433243680745e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001452074502594769\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013513285375665873\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016744346066843718\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00034040043829008937\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00026581244310364127\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.8089e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.811405470827594e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001522268634289503\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019951665308326483\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.9804e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.983072358299978e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001285754842683673\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002591349766589701\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.2138e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.213857654482126e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.0994e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.100599370663986e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011558008554857224\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00046913899132050574\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002515036321710795\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001286351034650579\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015242068911902606\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013179455709178\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021811718761455268\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012631021672859788\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.0659e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.066277561942115e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013118046335875988\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.3463e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.346679608919658e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011802415974671021\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006998245371505618\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010806906357174739\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006339174578897655\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004887583199888468\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000205299467779696\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003889959480147809\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005266860825940967\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010162986582145095\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000351610011421144\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014568436017725617\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018157223530579358\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.2542e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.252722960198298e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012017018161714077\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.0443e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.045517122605816e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00031780285644344985\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.7737e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.7730453186668456e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011307640670565888\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.4930e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.49289330188185e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003648469864856452\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002069687470793724\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.4714e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.470448402571492e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014365751121658832\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010896322783082724\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005495270015671849\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013006578956265002\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001667877077125013\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010759216820588335\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014216717681847513\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013239067629911005\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015230146527756006\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.8752e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.874083530623466e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003557837917469442\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007341814343817532\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.2416e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.240801303647459e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.4080e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.40913164918311e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00036061351420357823\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016595309716649354\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.5850e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.5823031794279814e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.2682e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.268951544072479e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011975289817200974\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013966343249194324\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014139221457298845\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.1540e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.155692532658577e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005463662091642618\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001981454697670415\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.6552e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.65641884249635e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00030343368416652083\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004926344845443964\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004457036266103387\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.9880e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.987630407209508e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.6723e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.672851981827989e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005529859336093068\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002595522964838892\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005117771797813475\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00027535174740478396\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.3459e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.345273191574961e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.9903e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.993302278919145e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006519296439364552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011403019016142935\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007879260811023414\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00033610747777856886\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022205195273272693\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013972305168863386\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.4861e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.486623508157209e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.1773e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.176656799856573e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.6215e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.619203435955569e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006113728741183877\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013727891200687736\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002317101025255397\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001256544783245772\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.0000e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.997830496402457e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015796477964613587\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017722031043376774\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.1878e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.1871534853708e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.7438e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.744642137666233e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016887421952560544\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.6778e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.680483809555881e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.1277e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.128969446057454e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000122912329970859\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.8499e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.849142583727371e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.5138e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.516428013332188e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015784555580466986\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015212262223940343\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005590093787759542\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013179455709178\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.2748e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.27491273614578e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.2702e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.272030779859051e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022968306438997388\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005312184803187847\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022330392675939947\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010496928734937683\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.8162e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.819004192948341e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021030730567872524\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000175431850948371\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.4397e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.442052133730613e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001204682412208058\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012481991143431515\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.9324e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.93062712950632e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00023838737979531288\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.3338e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.3365293600363657e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018103569163940847\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016052818682510406\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018783187260851264\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002611620584502816\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00025013237609528005\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010312135418644175\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011957406968576834\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002308158145751804\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002564520691521466\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002132881636498496\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.1471747206524014e-05\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017674338596407324\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014157105761114508\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00024411080812569708\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013286758621688932\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020559754921123385\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.7196e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.721989772515371e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.3924e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.394365871325135e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00034326245076954365\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001541494857519865\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.5627e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.563032649050001e-05\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 7.951575389597565e-05\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021680559439118952\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.7569e-06]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.735347596986685e-06\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.5609e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.561653880344238e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013000618491787463\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.4828e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.4837775678606704e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002497150271665305\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.1406e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.139466859167442e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001255352544831112\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013876924640499055\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022312506916932762\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015343412815127522\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.6845e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.681904051220044e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.1231e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.121584738139063e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.8978e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.897927571320906e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012398534454405308\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.3622e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.3631553126033396e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.4965e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.494022767990828e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.6890e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.6909954107832164e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002896609075833112\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.7449e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.744383376324549e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022324430756270885\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011289757094345987\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004538731591310352\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022807337518315762\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.6074e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.607281051808968e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.3029e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.30183528503403e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00036305817775428295\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.5525e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.550765101565048e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00031911456608213484\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001846722443588078\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.6613e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.662601688527502e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001443728688172996\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.9855e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.988784247776493e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001317349378950894\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000288647337583825\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015969359083101153\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017179534188471735\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008660131134092808\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.3328e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.330213363980874e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.8518e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.8537215510150418e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00027624607901088893\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00053926941473037\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006822290015406907\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001776376157067716\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005199472652748227\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004848821263294667\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013453673454932868\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015987243386916816\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.6511e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.6506800319766626e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.1024e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.102043259190395e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.0426e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.039883085875772e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.6195e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.620652417652309e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.5903e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.5910721130203456e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.8810e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.880044722696766e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.5776e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.577741492539644e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.1159e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.113965643336996e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002700455079320818\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011671270476654172\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003581688506528735\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001867587852757424\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015140726463869214\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020345134544186294\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.1740e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.17523182881996e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.0167e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.018836964154616e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004951391601935029\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.7727e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.7730453186668456e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022419820015784353\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012011056969640777\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019731084466911852\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.5234e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.524094765656628e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.3727e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.370288898935542e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.3246e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.327115210704505e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001265486644115299\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00026634903042577207\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021358624508138746\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000666065257973969\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005636611604131758\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016499926277901977\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.0815e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.084160410566255e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002568693889770657\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011045350402127951\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00026855498435907066\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002145401231246069\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010723450395744294\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018127415387425572\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00023087543377187103\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.8231e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.8235433243680745e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010180991375818849\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.1803e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.178091957233846e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.6480e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.649007941130549e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002470321487635374\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.8498e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.8519359552301466e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.2593e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.258683424675837e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.0551e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.057438779156655e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.4805e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.480972009128891e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00024494549143128097\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019731084466911852\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010926128015853465\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.2990e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.300409586401656e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013668279279954731\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.0933e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.094638178590685e-05\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 8.344653679159819e-07\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006908776122145355\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013417906302493066\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.7975e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.796591671649367e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011635503324214369\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.0200e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.023106784094125e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.1303e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.1292929634219036e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001119437874876894\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017978377582039684\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.2021e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.1994355847709812e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.2502e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.248505890951492e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011617619747994468\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.1527e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.150376283476362e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.2579e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.2576659173646476e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.4510e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.4525659177452326e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005256126751191914\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.4026e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.4048803829355165e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021507668134290725\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.1744e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.17523182881996e-05\n",
      "tensor([[0.9997]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.00032525582355447114\n",
      "tensor([[4.8475e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.845975126954727e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010949972784146667\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016976843471638858\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017608761845622212\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.0409e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.0426799134584144e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00027535174740478396\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.0376e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.0384995877975598e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.4651e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.467662670824211e-05\n",
      "counter: 19000\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.3666e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.3691157770808786e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.4684e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.468740659533069e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.6524e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.653529610252008e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022378088033292443\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008244335767813027\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.7353e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.7345102605759166e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.3448e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.342490006412845e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011760687630157918\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015313606127165258\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.8409e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.841211431194097e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000400921591790393\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002719533513300121\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021424204169306904\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005263282801024616\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.4512e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.452295878669247e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00027731925365515053\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.6279e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.628599395393394e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.7491e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.751795005286112e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.8345e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.835464980918914e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.0274e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.027634273981676e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.8202e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.8221321776509285e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.4279e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.429899364477023e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.4303e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.428451837971807e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00028250631294213235\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.7937e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.796591671649367e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014532668865285814\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.4171e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.417977707926184e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.7543e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.752013748860918e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002003512781811878\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020678988948930055\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020768414833582938\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.0345e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.033595466054976e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.0228e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.024797792430036e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.6226e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.625163900433108e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010526733967708424\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.8108e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.808809196925722e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.7418e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.7400920923100784e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017412033048458397\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00027910788776353\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012821781274396926\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015343412815127522\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011158612323924899\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010955933976219967\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.3022e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.30183528503403e-05\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00030063142185099423\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005275806761346757\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.2991e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.29899334651418e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.0579e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.060316369868815e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003497615980450064\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001662511785980314\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002487015153747052\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021227466640993953\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015718980284873396\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.5296e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.528349669883028e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022246927255764604\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001518841745564714\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.6862e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.6882056772592478e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002758883638307452\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.6802e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.681904051220044e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.6207e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.6212594346143305e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015987243386916816\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002012455224758014\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.8664e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.8670245228568092e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.6504e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.651062302698847e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014789005217608064\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00023016001796349883\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.6593e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.6584024453768507e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014842658129055053\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00038184065488167107\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.1311e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.130403875838965e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.0241e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.024797792430036e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.4752e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.474701851606369e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.1689e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.1696325347875245e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000956754491198808\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014312099665403366\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016684731235727668\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.4045e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.4020961063797586e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011683192860800773\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.4660e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.463089888100512e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.5267e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.524094765656628e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.1113e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.1128048906102777e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.9080e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.906782361213118e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012458146375138313\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.7872e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.789536301861517e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017877030768431723\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.3935e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.392686140723526e-05\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[5.0319e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.0307586207054555e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019927817629650235\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017245110939256847\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013143688556738198\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011170533980475739\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012082591274520382\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.7847e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.7835756554850377e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.7102e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.708877895609476e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.7683e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.76822741702199e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014067685697227716\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.6666e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.668562516802922e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00030587820219807327\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.6851e-06]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.702316335984506e-06\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012994656572118402\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002270002441946417\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010723450395744294\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013298681005835533\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001400807232130319\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.1898e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.191458957502618e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.6687e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.668562516802922e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.0467e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.046950824791566e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.1900e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.1875144739169627e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.2837e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.282526737777516e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020518022938631475\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.6375e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.6373490476980805e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.8265e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.826396176824346e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.0571e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.056015263311565e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.8473e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.845975126954727e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.8080e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.811405470827594e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.1567e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.158773951232433e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.0100e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.0086967197130434e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.5849e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.586530071217567e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00039305066457018256\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013245029549580067\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00023165048332884908\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.1497e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.1471747206524014e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020482252875808626\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013519247295334935\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.4928e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.49115681881085e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018139337771572173\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.8251e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.826396176824346e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014371713041327894\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014800929056946188\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011152651131851599\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017924723215401173\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020792261057067662\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00023624110326636583\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.7733e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.7762342395144515e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.6110e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.612106593209319e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014997653488535434\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.2862e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.288844552822411e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007542234961874783\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.7043e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.7060874344897456e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001923626841744408\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.8592e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.860729627078399e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.2973e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.2948051991988905e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.1161e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.113965643336996e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.6950e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.695255771977827e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010276368993800133\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020678988948930055\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000538732681889087\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.8904e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.893408812582493e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012350844917818904\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001776376157067716\n",
      "tensor([[0.9997]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.00034695921931415796\n",
      "tensor([[3.0721e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.0696864996571094e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012136242003180087\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.7240e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.723630212945864e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004697949334513396\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.0387e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.0381327784853056e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.5472e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.546539301169105e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001846722443588078\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014240563905332237\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013400021998677403\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00041863141814246774\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.1149e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.115623546065763e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003779648686759174\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012905237963423133\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.6919e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.690734830452129e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.5180e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.519542537513189e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.8391e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.83688631397672e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.1761e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.176656799856573e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.1520e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.152813486754894e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.0252e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.027634273981676e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019307808543089777\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011075156362494454\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.5758e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.5735749911982566e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001148051378550008\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.2189e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.2187024771701545e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001631512277526781\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.7740e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.774445915245451e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.4185e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.421365181566216e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.8362e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.83861297450494e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015152648848015815\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.6728e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.671411731280386e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016553579189348966\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021024768648203462\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.3109e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.308112354716286e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003095748252235353\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022252889175433666\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.8584e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.860729627078399e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.5556e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.553898543235846e-05\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[8.4571e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.458257070742548e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.0128e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.012876135879196e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.2036e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.20503633399494e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00036633762647397816\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001821683836169541\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.1139e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.111410842393525e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013960382784716785\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.2133e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.215542478254065e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.7349e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.736981933703646e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011593775707297027\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.6352e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.638535266276449e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.9635e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.9623946829815395e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.7009e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.704106923891231e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.2584e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.2572679123841226e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.0627e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.064833673415706e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00038857856998220086\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.9035e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.901406176330056e-05\n",
      "tensor([[0.9974]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0026107507292181253\n",
      "tensor([[4.7960e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.7982888645492494e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.0477e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.047237234772183e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.7034e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.701516834553331e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018043954332824796\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.7136e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.716029308037832e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.0791e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.081607792410068e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001308407518081367\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.0554e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.054601570009254e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.9292e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.93062712950632e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.4952e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.49429135187529e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016654924547765404\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.0797e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.078199218492955e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021358624508138746\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.0242e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.0265784769435413e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00031750474590808153\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014312099665403366\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010932089207926765\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.2060e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.2036208217032254e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.7066e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.704106923891231e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.8336e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.8326521462295204e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.0615e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.064833673415706e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.1193e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.1187657188856974e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.8463e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.8473866374697536e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.0575e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.0563813450280577e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.5289e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.528657180140726e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.9224e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.920338637428358e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.8892e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.8894850654760376e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001989204902201891\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.6960e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.6987467461149208e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004973455797880888\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003613886365201324\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.7047e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.707177428528666e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001742991735227406\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.8781e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.8771907788468525e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.2604e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.26299107959494e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.8593e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.859308294020593e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002512055216357112\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016648962628096342\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.3099e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.3126869564293884e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012815819354727864\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.2005e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.200500112958252e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.3681e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.370522558223456e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016541656805202365\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016654924547765404\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.6700e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.671713420771994e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010371745884185657\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010890361591009423\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013847117952536792\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00028196972562000155\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002731457934714854\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.0499e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.047237234772183e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.7558e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.753433990525082e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010687683970900252\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00035858622868545353\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.6938e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.69240546610672e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.5955e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.595359395258129e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.1676e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.166169573087245e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.0236e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.021673809504136e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001241045683855191\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.4893e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.4883305235998705e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.0851e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.082716522039846e-05\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.695494797284482e-06\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001889645791379735\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.4748e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.4778167395852506e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.9815e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.9802766221109778e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017519340326543897\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.3685e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.36740548745729e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001410941476933658\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.6255e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.6226387490169145e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.9552e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.95782662963029e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015820324188098311\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001339406007900834\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011707037629093975\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.5408e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.540578472893685e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.6284e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.631388219422661e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022312506916932762\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.8050e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.802552136126906e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.6981e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.695556370075792e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.7802e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.783238495700061e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.0725e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.072484054835513e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018646071839611977\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.6152e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.613506098394282e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.2965e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.294448394328356e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.5962e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.595623613568023e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.9398e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.938552097475622e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001698280539130792\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011295718286419287\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001319733855780214\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 5.960466182841628e-07\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001026561134494841\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.8652e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.8656428437680006e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011867989087477326\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.8520e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.85476916260086e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.7690e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.767084490391426e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.2177e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.217317342001479e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.7332e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.733912156661972e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.6000e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.601319859735668e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.0985e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.096082794712856e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.4125e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.4108408474130556e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.3853e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.3856013033073395e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.9580e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.9550514480215497e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.3617e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.360371763643343e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00026825687382370234\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.5399e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.5374246585415676e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006045738118700683\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021668635599780828\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00025520005146972835\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010753256356110796\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.6588e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.65641884249635e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.8098e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.810210521100089e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019951665308326483\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011498397361719981\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.2451e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.245346255833283e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.3138e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.314073182991706e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000318160600727424\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.4047e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.4020961063797586e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.2983e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.30076584557537e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.0745e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.0756469641346484e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.4152e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.415404353290796e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022807337518315762\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.3910e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.3887978639104404e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.5825e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.579535637574736e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.0463e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.0444602341740392e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.2611e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.2590415028389543e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.9031e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.905582682113163e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.0335e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.0339224394992925e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.3434e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.343876596773043e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.8125e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.813043000875041e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.4738e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.471626253798604e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.2431e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.240801303647459e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.0890e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.090366175863892e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.0145e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.014657366089523e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00035787071101367474\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015087073552422225\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.2800e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.2797051719389856e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.1197e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.118482219288126e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.4751e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.475011180853471e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.8175e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.817366662900895e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.3936e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.394365871325135e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.4393e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.440374222118407e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.8530e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.850534267257899e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.7867e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.784669287502766e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.1076e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.106560562737286e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00033873095526359975\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.7375e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.7400920923100784e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.4750e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.4722455489390995e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.0731e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.075321627780795e-05\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[5.7747e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.7758566981647164e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018735494813881814\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.9012e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.899369277060032e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015802439884282649\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016392620455008\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003651450970210135\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00028113502776250243\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.0033e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.002736073336564e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011707037629093975\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.7088e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.7060874344897456e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003983575734309852\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.8732e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.871229950571433e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.1783e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.175593181164004e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011766648822231218\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.9537e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.956104334676638e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011558008554857224\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002450647298246622\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001419287291355431\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.3554e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.35719448432792e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.4084e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.410568989347667e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001631512277526781\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.4778e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.4778167395852506e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.9583e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.95782662963029e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011206301132915542\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.8545e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.856200681999326e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.1436e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.142608668189496e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.2872e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.287071689963341e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.8138e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.813043000875041e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010455201118020341\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.4540e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.452295878669247e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004596574290189892\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.4904e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.48973839613609e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.6705e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.6689440599293448e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.5266e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.528657180140726e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.8114e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.810210521100089e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.5396e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.537197911413386e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.1818e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.179777872399427e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.3977e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.39720999263227e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.3159e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.3172712897357997e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002761268406175077\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.6376e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.638758375425823e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.6036e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.602769569028169e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.6243e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.625427391147241e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.0914e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.090121602639556e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.8766e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.875525236362591e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.1939e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.1934749383945018e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.3514e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.34952263883315e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.1710e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.169271364342421e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013209260941948742\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002399970981059596\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002069091278826818\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.2194e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.217317342001479e-05\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.0848104466276709e-05\n",
      "tensor([[3.6330e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.635949542513117e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020714759011752903\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.7604e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.75973306997912e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.8804e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.881486428435892e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.9485e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.9490908016450703e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.4108e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.4122483561513945e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.8068e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.80708180880174e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001401403424097225\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.1398e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.142608668189496e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.5958e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.594224835978821e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011933562200283632\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.1914e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.194539648480713e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.8624e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.860729627078399e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.8767e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.874376852763817e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011373213055776432\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.4335e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.433286474319175e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.7393e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.739873348735273e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.1200e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.117371670668945e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.8254e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.82496465742588e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.4905e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.4901272152201273e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.2267e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.228879647096619e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001459824270568788\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013543092063628137\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.3283e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.330213363980874e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002117977273883298\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.1438e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.1444157280493528e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.8071e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.808809196925722e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.9648e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.96378745790571e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.3896e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.3915617677848786e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.3063e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.306370050879195e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002226481301477179\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014580358401872218\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.145769485650817e-06\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004845243238378316\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015426872414536774\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.2743e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.276566273299977e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.5472e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.549346315092407e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001286351034650579\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.3625e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.361758354003541e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00025293449289165437\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.1563e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.1545299609424546e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011540124978637323\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004106411070097238\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.4365e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.4392473025945947e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00038285431219264865\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016344929463230073\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.0222e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.020617830567062e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.6417e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.6433095121756196e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.1962e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.194539648480713e-05\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 5.960466182841628e-07\n",
      "tensor([[4.0538e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.053198063047603e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.5496e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.550765101565048e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.2702e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.269189568934962e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.4563e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.4585267460206524e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.7469e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.749202369363047e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9.4436e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.441821748623624e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.7319e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.7299300199956633e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013596743519883603\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.6663e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.6689440599293448e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00035262363962829113\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001580840180395171\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.7613e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.7639354054117575e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010985739208990708\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010723450395744294\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003049838705919683\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.5699e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.5703818866750225e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010091575677506626\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.8186e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.819004192948341e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.8274e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.8252999982214533e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012344884453341365\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.4486e-06]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.4570753086882178e-06\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011742804781533778\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.8722e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.874376852763817e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.4708e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.4722455489390995e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.6786e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.6776742490474135e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.4335e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.4318991563632153e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.3782e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.376483386498876e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003025393234565854\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.0090e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.010079853993375e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.8412e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.83861297450494e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.9673e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.9666028391802683e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.4543e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.45539112156257e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.5747e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.573189628194086e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013900769408792257\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015635520685464144\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.0863e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.08896230510436e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.5461e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.546539301169105e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.2395e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.2391470388975e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000111645735159982\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.4961e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.4974657208076678e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.4432e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.440644624992274e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.9963e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.9949940148508176e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.7507e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.7478119591251016e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.3774e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.376250091008842e-05\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.6689314179529902e-06\n",
      "tensor([[7.0046e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.003790960879996e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.2435e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.242226274684072e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.0848e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.087568256887607e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.8312e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.830925849499181e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.1595e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.157349707791582e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.1013e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.103701889514923e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.4727e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.4736233172006905e-05\n",
      "counter: 20000\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.9706e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.971151065547019e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003299661329947412\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.1752e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.178091957233846e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011277834710199386\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013483480142895132\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.1697e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.172412081970833e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.0795e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.078444519313052e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.1517e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.153135548927821e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010109458526130766\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.9943e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.996775608859025e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.5066e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.505944424541667e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013906729873269796\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010282330185873434\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.2046e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.2053962311474606e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001836587762227282\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001286947081098333\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.1701e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.172412081970833e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.8908e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.890867108362727e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.7684e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.770273593137972e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.9873e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.990237595047802e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010127342102350667\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.9097e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.9073668227065355e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.5246e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.522696715663187e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.8766e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.877563954622019e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.4767e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.477587445871904e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.2727e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.275150033412501e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.0051e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.002367808832787e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021394396026153117\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015200339839793742\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011468591401353478\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.4020e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.4048803829355165e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001021675780066289\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.2540e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.2530810383614153e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.0712e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.069360435707495e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015295721823349595\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.8467e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.845720731187612e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.6331e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.633180545468349e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008753195870667696\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010139263758901507\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.6603e-06]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.689029189350549e-06\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.3044e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.306726310052909e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.2899e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.288844552822411e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.6227e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.625427391147241e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.0492e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.0504208805505186e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012911199883092195\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.8002e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.801457594614476e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.9894e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.987341086845845e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022634444758296013\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013125804252922535\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001263698359252885\n",
      "tensor([[0.9996]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.00044242391595616937\n",
      "tensor([[3.6880e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.689595541800372e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.2200e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.221503306529485e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.2482e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.249623351730406e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.8178e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.815915498416871e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.4963e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.498545164708048e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.1705e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.1696325347875245e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.9101e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.912743553286418e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.5018e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.500251816352829e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.4233e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.421365181566216e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.0636e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.0651193558005616e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.4355e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.434683796716854e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.7435e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.741512333974242e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.5416e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.540578472893685e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.5532e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.555693415866699e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.3403e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.340718780644238e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.0618e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.063399971229956e-05\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[5.6741e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.674523345078342e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00023200818395707756\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.2573e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.260109123308212e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.9294e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.926630804722663e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.7375e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.7358906663721427e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.6202e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.617763913003728e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.4363e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.434683796716854e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.9696e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.972563667455688e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002643219195306301\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.0396e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.042433521244675e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.4064e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.404607797274366e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.8016e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.804249692824669e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.5176e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.5167358873877674e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022950422135181725\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.9175e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.91891621297691e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001645223528612405\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.9686e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.968355329358019e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.0737e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.0728893357736524e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019349539070390165\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.0062e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.0073235898744315e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00033109899959526956\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.9092e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.9069949202239513e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.2503e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.248187466757372e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.0055e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.008328637108207e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.1088e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.109663081588224e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.6355e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.635646761627868e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.8165e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.816171349375509e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011128806363558397\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014267604565247893\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 7.152560215217818e-07\n",
      "tensor([[4.2736e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.273744343663566e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.5270e-06]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.510213436034974e-06\n",
      "tensor([[0.9996]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.000400921591790393\n",
      "tensor([[2.1623e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.1636720703099854e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.2578e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.25702988752164e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.2538e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.254466719226912e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.2685e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.270962795591913e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.0344e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.035315942019224e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.8979e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.8968277547392063e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.3696e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.3691157770808786e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002141228032996878\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013990188017487526\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.0477e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.048640741733834e-05\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[6.7271e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.729590677423403e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.0198e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.0192446097789798e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003279389056842774\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[2.0634e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.062341991404537e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012583332136273384\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013030423724558204\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.7312e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.7299300199956633e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.2083e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.210996798472479e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002182960306527093\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0078e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.0086967197130434e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001241045683855191\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.0194e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.0220011467463337e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.4278e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.430130477179773e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.9551e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.95326858072076e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001044327873387374\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018872611690312624\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.2374e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.236265810206532e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.2960e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.295874820556492e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001148647497757338\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.3157e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.3186476028058678e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.2928e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.2916264646919444e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010234641376882792\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.8596e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.862455560010858e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.5641e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.567003144184127e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001841357006924227\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001811549300327897\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017435879271943122\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.3632e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.361758354003541e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010395590652478859\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012106436042813584\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.2388e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.23978434203309e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.4920e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.48973839613609e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.3911e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.3884050430497155e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014949962496757507\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.9683e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.9666028391802683e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.3992e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.3975225960602984e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.9289e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.927732076495886e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.5302e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.532882252940908e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.6721e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.671713420771994e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.3557e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.3544111172668636e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.0186e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.020617830567062e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003529813839122653\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.5051e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.505944424541667e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016291276551783085\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.2017e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.2022158595500514e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.3869e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.3856013033073395e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.9103e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.9073668227065355e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015957436698954552\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.5493e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.5511113562970422e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.8672e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.8656428437680006e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001309599756496027\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011271873518126085\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.7709e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.769896233687177e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.8642e-06]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.88113117980538e-06\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.5885e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.589663149090484e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.4125e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.4126400856184773e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.1068e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.109663081588224e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014532668865285814\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.2603e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.261545008281246e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002824466791935265\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.7813e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.781817526440136e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.7657e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.768485450767912e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010020042827818543\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.0\n",
      "tensor([[3.1831e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.182938598911278e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.4344e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.433286474319175e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002160305593861267\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.0213e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.0233942854683846e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.5493e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.546539301169105e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.0737e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.076755329966545e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015915706171654165\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003997290332335979\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.9953e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.9949940148508176e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.4780e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.4778167395852506e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.0602e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.062341991404537e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.5828e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.584885992808267e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.4020e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.4007189747644588e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004703912709373981\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.2853e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.287071689963341e-05\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 7.152560215217818e-07\n",
      "tensor([[6.9810e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.979947647778317e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.2738e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.273744343663566e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001306619233218953\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.9607e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.95782662963029e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.6243e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.6268367188749835e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018872611690312624\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.0229e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.020617830567062e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.8529e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.8551032301038504e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.9416e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.9445127438521013e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.5596e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.561653880344238e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017924723215401173\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.1742e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.1742184142349288e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017686262435745448\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.5611e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.561041952110827e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.8261e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.8239186829305254e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001019291375996545\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.5151e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.51500013191253e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002689723332878202\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015903783787507564\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011128806363558397\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.6067e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.6047568098874763e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003217380144633353\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.7944e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.795160152250901e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.0108e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.012628288473934e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.9488e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.948720354354009e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.8843e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.8835246009984985e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.8393e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.83861297450494e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.2015e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.203380614053458e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.8906e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.8894850654760376e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.3487e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.349837061250582e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.8461e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.8431819373508915e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.7155e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.713438491104171e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011218223517062142\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.1896e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.188899427186698e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005021760007366538\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.6886e-06]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.689029189350549e-06\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.2900e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.2930325182387605e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.0972e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.098105505865533e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00030850162147544324\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.3205e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.319718133658171e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.5924e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.595359395258129e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001306619233218953\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.2156e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.21413715230301e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00024440890410915017\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.7650e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.767084490391426e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.4655e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.464487574296072e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.1611e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.161895387573168e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.8451e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.84284750605002e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.9255e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.923213317757472e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.6896e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.6909954107832164e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.4976e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.498853766359389e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.8402e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.83861297450494e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00044081389205530286\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.1990e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.2008207199396566e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.2044e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.20503633399494e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.0701e-06]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.07969241173123e-06\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.4741e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.476408867049031e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.9157e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.913327469083015e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.7132e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.7162487792083994e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.9563e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.954681182629429e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.6394e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.6391411918448284e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.7089e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.7106678569689393e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.5781e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.576342714950442e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.6483e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.646139081567526e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.4134e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.416530181420967e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.5036e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.50307847536169e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.4648e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.467662670824211e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012112397234886885\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.6921e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.6927862816373818e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.7378e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.738681673188694e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.9811e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.9816695789340883e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.6955e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.696956239058636e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.9983e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.999263470992446e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003010487707797438\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.4593e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.460324438085081e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.5656e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.5658201997866854e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.8968e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.898219802067615e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.7417e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.7400920923100784e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.4448e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.445207767072134e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.3329e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.330213363980874e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022252889175433666\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.4999e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.4974657208076678e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.8378e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.83861297450494e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.0004e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.999551699962467e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.3835e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.382211283082142e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.0305e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.027961793122813e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005903198034502566\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.6410e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.641910006990656e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012809858890250325\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.0961e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.094638178590685e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.9810e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.978524495731108e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.3422e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.342490006412845e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012815819354727864\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.7720e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.771654362732079e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010049848060589284\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.9388e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.941347287967801e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.4259e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.427325646043755e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.8786e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.8803376810392365e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.9395e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.941347287967801e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.8332e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.834053470403887e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.7388e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.7404707250534557e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.6300e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.628599395393394e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.6842e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.683334115426987e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.7328e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.734131264034659e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.8288e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.827837882563472e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.0557e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.056015263311565e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.2705e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.270962795591913e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014604204625356942\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.2097e-06]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.212187938421266e-06\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.0884e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.087568256887607e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00033312622690573335\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.5833e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.5823031794279814e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012780052202288061\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.2326e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.231740503106266e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.5555e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.558154902653769e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.2155e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.21695799054578e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.7971e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.8000764612224884e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.8180e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.817366662900895e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.5917e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.5910721130203456e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.5537e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.555307143367827e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011933562200283632\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[6.7845e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.783238495700061e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.1097e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.1086525773862377e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.7767e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.7762342395144515e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.6142e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.6152989701367915e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.4123e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.41509284125641e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.9918e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.991869304329157e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.5760e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.574608414666727e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.9882e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.988784247776493e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.0481e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.0458437322522514e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001600512769073248\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012124319619033486\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.4169e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.415404353290796e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.0587e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.059158527525142e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.6075e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.607281051808968e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.8419e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.841211431194097e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.2163e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.2159421203250531e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.6141e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.613506098394282e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.0479e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.048394713317975e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.9247e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.926299829501659e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.3703e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.368842100026086e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.6593e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.6584024453768507e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.3489e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.3484506527893245e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.5037e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.505944424541667e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.5395e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.5374246585415676e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017680300516076386\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00024786681751720607\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 9.179157132166438e-06\n",
      "tensor([[2.7346e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.7358906663721427e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000722251832485199\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.1345e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.1338690203265287e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.2217e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.222918455023319e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013000618491787463\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011337445903336629\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019039535254705697\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.7992e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.7982888645492494e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.7282e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.7285496141994372e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.8973e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.897927571320906e-05\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 6.437322554120328e-06\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002377315831836313\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.4224e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.421053305733949e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.3774e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.376483386498876e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.3604e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.3631553126033396e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.9215e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.923213317757472e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.5054e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.505944424541667e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011438785440986976\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.2089e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.209581649978645e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010562501120148227\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.4771e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.476139919366688e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011993173393420875\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017584915622137487\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.2344e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.232019273331389e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.0250e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.024797792430036e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012476030678953975\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00026301026809960604\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.6671e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.669982394669205e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.9123e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.9147096938686445e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.7744e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.77418860909529e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011558008554857224\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.9459e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.948720354354009e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000463116099126637\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.0534e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.0563813450280577e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.0388e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.039883085875772e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.3674e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.3663324100198224e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.0321e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.030511137098074e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.8456e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.845720731187612e-05\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.60018530045636e-05\n",
      "tensor([[2.5286e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.527268952690065e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.6979e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.69669602252543e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.2292e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.230304618133232e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.0211e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.0233942854683846e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015981282922439277\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.5812e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.581998215755448e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.6257e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.62722008099081e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.0673e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.0683026377810165e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.5320e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.532882252940908e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.5468e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.546232518507168e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.2588e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.25702988752164e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.2683e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.270962795591913e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.0625e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.060316369868815e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.4994e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.498545164708048e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.7846e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.7863675717962906e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017686262435745448\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.2828e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.282526737777516e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.0126e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.015712617430836e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014025956625118852\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010157147335121408\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.4496e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.450857810908929e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.2446e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.2439409298822284e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002506689343135804\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.7084e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.707477662828751e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.2959e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.2948051991988905e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.7340e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.7345102605759166e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.2478e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.248187466757372e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.7349e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.732720844913274e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.8013e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.802848732448183e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.0795e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.0788498912006617e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011039389210054651\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.8310e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.8312606445979327e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.1443e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.142608668189496e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010574422776699066\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021847488824278116\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.2577e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.2590415028389543e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.9087e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.910141094820574e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011170533980475739\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011259951133979484\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014872464817017317\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.9873e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.990237595047802e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.1789e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.178372910246253e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.3130e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.314073182991706e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.4935e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.49115681881085e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.3612e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.3589951777248643e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.2779e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.2755474926962052e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.0778e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.0756469641346484e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.7580e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.755163197638467e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012595254520419985\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.8926e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.894860704662278e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.0426e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.0444602341740392e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.3388e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.3379157684976235e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.5749e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.5749537599040195e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.2934e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.2930325182387605e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.7474e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.747473526047543e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.1958e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.1980605449934956e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.4695e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.4722455489390995e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.2320e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.2334245992824435e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.5419e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.541977250482887e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.3356e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.3351529560168274e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.1326e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.1324947081448045e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000188487654668279\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.1065e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.106560562737286e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.1611e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.1622973033809103e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.8629e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.862455560010858e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.5766e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.576037023682147e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.2171e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.2159421203250531e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.6178e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.617763913003728e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.7697e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.769678581506014e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.5823e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.580569606740028e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.7182e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.7166285033454187e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016672808851581067\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.5956e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.5928356990334578e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.9853e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.9862372684874572e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.2009e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.2036208217032254e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.7966e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.7997000112663954e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.9406e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.938221486052498e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.8390e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.838317833375186e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.8485e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.850534267257899e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.5226e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.524094765656628e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.9053e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.9073668227065355e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.2364e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.2379801016068086e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.2049e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.2036208217032254e-05\n",
      "tensor([[0.9968]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.003230838105082512\n",
      "tensor([[3.0212e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.0220011467463337e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.1518e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.150376283476362e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.9390e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.936587593983859e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.7103e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.712048080866225e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.9033e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.9010340919485316e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014079608081374317\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.3410e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.342490006412845e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.6579e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.65641884249635e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.5927e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.5928356990334578e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.5966e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.5974172129062936e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.0349e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.033595466054976e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011546086170710623\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010729411587817594\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.8482e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.84880797052756e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.4286e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.431547808460891e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.0058e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.005512528237887e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.6209e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.623724377481267e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.8703e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.8729851692332886e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.9003e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.9010340919485316e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.4688e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.4690503525780514e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.3880e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.3887978639104404e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.7997e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.8000764612224884e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.2461e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.245346255833283e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.3161e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.3172712897357997e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.5847e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.586530071217567e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017608761845622212\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011021506361430511\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.8776e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.875525236362591e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.9493e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.9490908016450703e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.3479e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.3484506527893245e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013727891200687736\n",
      "tensor([[0.9995]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0005223326734267175\n",
      "tensor([[3.4991e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.498853766359389e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.6382e-06]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.616137397941202e-06\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.7903e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.787778354715556e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.5425e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.5451507099205628e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016166086425073445\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.1727e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.171017306158319e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016488003893755376\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.8385e-06]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.854557796032168e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.2187024771701545e-05\n",
      "tensor([[3.8114e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.808809196925722e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011432824248913676\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002498938993085176\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.8819e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.882938320515677e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.4615e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.463089888100512e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.2809e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.2828839064459316e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.3954e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.396135460003279e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019057421013712883\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.4654e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.4662849935120903e-05\n",
      "counter: 21000\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.2722e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.2723484764574096e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001567128929309547\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.1966e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.194859891664237e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.6190e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.6180674214847386e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.3797e-06]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.357972885482013e-06\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015647443069610745\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.4026e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.4048803829355165e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.5433e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.546232518507168e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.0002e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.0009548431262374e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.4477e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.4480129620060325e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.2292e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.230304618133232e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011975289817200974\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003260309458710253\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.2491e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.251307084108703e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013692124048247933\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.7906e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.789199687773362e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.9796e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.9816695789340883e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.8721e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.871229950571433e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.0238e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.026211121934466e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.1837e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.182938598911278e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011099001130787656\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.3979e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.400326699600555e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.1771e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.175593181164004e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.7373e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.73842291184701e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.8791e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.881739732809365e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.3120e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.31091500306502e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011552047362783924\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.8388e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.840014298679307e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.1494e-06]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.179157132166438e-06\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.5003e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.501431486569345e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.7104e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.710068115964532e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.9301e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.93083786952775e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011933562200283632\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002415471972199157\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.7926e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.790630479576066e-05\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 7.152560215217818e-07\n",
      "tensor([[4.9931e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.9949940148508176e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.2725e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.2695870282186661e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.5093e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.510467548854649e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.8621e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.862455560010858e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015081111632753164\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.5650e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.565554162487388e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.8004e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.7997000112663954e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.2220e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.2219026757520624e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011599736899370328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.3720e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.371939525706694e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.0807e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.080223748635035e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.2251e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.222918455023319e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011546086170710623\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.1749e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.173575381282717e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.1598e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.1622973033809103e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00047868024557828903\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.8009e-06]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.775209946383256e-06\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.1214e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.123008981579915e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.7535e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.755163197638467e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.6065e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.607281051808968e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.7409e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.743241904885508e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.1373e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.139829666703008e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.4154e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.41509284125641e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.3812e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.383861182257533e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.4484e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.4511685953475535e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.4503e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.4511685953475535e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014580358401872218\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.8554e-06]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.841272468387615e-06\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.3228e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.3246082491823472e-05\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[1.0899e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.09077091110521e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.5361e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.536016422207467e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.2966e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.2948051991988905e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.2137e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.21413715230301e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.9490e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.951575389597565e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.2224e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.221263462677598e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.2800e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.2828839064459316e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.1969e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.1980605449934956e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.2121e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.2113566956249997e-05\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.576279254957626e-07\n",
      "tensor([[6.1320e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.133506394689903e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018264530808664858\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.6317e-06]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.629423635080457e-06\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.6232e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.6212594346143305e-05\n",
      "tensor([[0.9990]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0010290670907124877\n",
      "tensor([[5.3037e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.3049541747896e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00027535174740478396\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.7257e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.723969373619184e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.0769e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.0756469641346484e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.8510e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.8537215510150418e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.7036e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.7047073924914002e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.7650e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.7656937163555995e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.3653e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.365978333633393e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.8162e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.8133788873674348e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.1521e-06]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.172333774477011e-06\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.5019e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.5034263671841472e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.0063e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.003790960879996e-05\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[1.5867e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.585496102052275e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.3562e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.35719448432792e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.0340e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.0325391233200207e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.8111e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.811997572076507e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.4748e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.476139919366688e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.1568e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.15909601340536e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.2759e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.276566273299977e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.3789e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.381037433631718e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.1093e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.108248660690151e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.1722e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.171017306158319e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.1602e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.1604907892178744e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.0621e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.060562398284674e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.4210e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.4199778636102565e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.9167e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.916101923095994e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.6705e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.6703237381298095e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.1508e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.149974094820209e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.6329e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.633180545468349e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014914193889126182\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011832221935037524\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.9513e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.9490908016450703e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.3072e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.309508585720323e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.4567e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.4543638826580718e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.8048e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.8074182409909554e-05\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.768372718899627e-07\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001539706572657451\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.0400e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.0384995877975598e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.9373e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.935386459692381e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011248028749832883\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.5947e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.594224835978821e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.4038e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.404607797274366e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.5587e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.5570720026735216e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.3875e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.3901748136267997e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.1618e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.1604907892178744e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.4968e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.4960877706471365e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012684673129115254\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.4221e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.4245611964724958e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.2359e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.235199099231977e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.4738e-06]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.477183084527496e-06\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 6.711708556395024e-05\n",
      "tensor([[2.9230e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.9206701583461836e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.1289e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.1292929634219036e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.8439e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.844573438982479e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010520773503230885\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004706894396804273\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.4217e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.422762503963895e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.3513e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.3484506527893245e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.0506e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.0504208805505186e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.4471e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.449430657085031e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.5923e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.592491263290867e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.0712e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.0728893357736524e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.9633e-06]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.947197794535896e-06\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.5181e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.515347659937106e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.4522e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.452295878669247e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.5704e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.572964336257428e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.3926e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.3901748136267997e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.8528e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.8533474657451734e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.7916e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.794115814846009e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.8058e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.806037107598968e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.5074e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 8.505944424541667e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.6997e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.6987467461149208e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.5156e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.5167358873877674e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.7544e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.7537724236026406e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.6485e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.6464813345228322e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.1366e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.134930638130754e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015176493616309017\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.9242e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.9252485799370334e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.1434e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.144013266544789e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021227466640993953\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018395685765426606\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.6982e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.695556370075792e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.0844e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.0848104466276709e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.0165e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.018836964154616e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.0369e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.0381327784853056e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.3764e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.378253520873841e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.4516e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.449430657085031e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.2388e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.237700967583805e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.2210e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.2232779883779585e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.3690e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.3663324100198224e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.3413e-06]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.357972885482013e-06\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.9374e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.937169690791052e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.2236e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.224344153655693e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.6737e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.676284384506289e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.0315e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.0293551137438044e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.4441e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.4466054532676935e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.1671e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.1696325347875245e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013245029549580067\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.6367e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.6373490476980805e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.8446e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.8418004401610233e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.3177e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.3168758313404396e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011355329479556531\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.6842e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.685034946305677e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.5531e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.552499765646644e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.1538e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.154247916536406e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.1633e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.161895387573168e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.5109e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.512173472903669e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.8674e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.8684163882862777e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.2828e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.2842697692103684e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.9525e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.9550514480215497e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015677249757573009\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.8008e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.802848732448183e-05\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.3113030945532955e-06\n",
      "tensor([[6.0546e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.056015263311565e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.8445e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.8418004401610233e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.5172e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.515347659937106e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.4063e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.406055323779583e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.4994e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.498545164708048e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.2175e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.2187024771701545e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.7892e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.787778354715556e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.4450e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.4424427718040533e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.8656e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.865269122296013e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.5702e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.570076559204608e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.4697e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.470448402571492e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.0449e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.0444602341740392e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010276368993800133\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.9277e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.9252485799370334e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.6077e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.608954234048724e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.7318e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.73246171977371e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.9388e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.939653733046725e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.5659e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.5658201997866854e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.0839e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.0848104466276709e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.0765e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.078199218492955e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.1506e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.1485694964649156e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.7238e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.723630212945864e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016499926277901977\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.2653e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.2663880119798705e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.7004e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.6987467461149208e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.9194e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.91891621297691e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.5283e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.529788465239108e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013787504576612264\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.3787e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.381037433631718e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.6639e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.662380034569651e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.2497e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.2499013943597674e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.2978e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.30076584557537e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.1437e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.1444157280493528e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.3427e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.3411135114438366e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.6835e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.6836347135249525e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.1154e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.1159872630960308e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.1096e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.108248660690151e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.1826e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.1815538275404833e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001100958397728391\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.1982e-06]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.225474630307872e-06\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.0884e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.090366175863892e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.6673e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.665451266802847e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002549615746829659\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.4727e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.47327397018671e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.6719e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.6703237381298095e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.9675e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.9651902372715995e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.5854e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.585496102052275e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.1035e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.1026920219592284e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.7812e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.780406743520871e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.7823e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.780406743520871e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.9638e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.962064799154177e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002553192898631096\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.4856e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.484166659793118e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.0354e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.0339224394992925e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.8752e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.8771907788468525e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.2750e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.273744343663566e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.9287e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.9280232158489525e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.6799e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.6808651707833633e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.6599e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.6584024453768507e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.3415e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.339312363299541e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015677249757573009\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.2955e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.295874820556492e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.3149e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.315469413995743e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.8909e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.892258973792195e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.2289e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.226058445055969e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.7949e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.796887904172763e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.7060e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.708617679076269e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.9056e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.908416980877519e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.5578e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.556725566042587e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.6235e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.623724377481267e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.5206e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.522696715663187e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.2863e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.288844552822411e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.2644e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.2650021492154337e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.1343e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.136647839914076e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012154125579399988\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.5187e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.5213083063135855e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.5383e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.538843445014209e-05\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 5.960466182841628e-07\n",
      "tensor([[2.8778e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.878945815609768e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.1008e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.103701889514923e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.4552e-06]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.477183084527496e-06\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.1614e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.160209108609706e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.1877e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.1875144739169627e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.0604e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.060316369868815e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.7804e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.7821947039919905e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.9376e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.9399445086019114e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.2121e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.213857654482126e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.4329e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.433286474319175e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.7940e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.793739182990976e-05\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.5033982637978625e-06\n",
      "tensor([[6.4975e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.49711728328839e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.3201e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.320033647469245e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.4566e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.4585267460206524e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.3825e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.382444214774296e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.3470e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.3470740668708459e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.5775e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.577476546633989e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.4297e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.4318991563632153e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022175387130118906\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.0912e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.093529085163027e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010729411587817594\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.0126e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.014657366089523e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013137726637069136\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002557366096880287\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.4485e-06]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.450608336512232e-06\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.1525e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.154247916536406e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.7992e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.7982888645492494e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.7749e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.7730453186668456e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.2416e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.239385427557863e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014115376689005643\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.0285e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.0265784769435413e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.6585e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.6584024453768507e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.3303e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.33163979020901e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.8077e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.808809196925722e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.2199e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.218382961582392e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.0414e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.039883085875772e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.1214e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.1219479094725102e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.8703e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.872652011225e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.2803e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.2815080481232144e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.4211e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.422490645898506e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000477129768114537\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012446223990991712\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.5957e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.5928356990334578e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00031273486092686653\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.0045e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.0041192076168954e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.7146e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.713438491104171e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.3593e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.360371763643343e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.0309e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.0325391233200207e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.7895e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.790630479576066e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.0614e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.0637256713816896e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.9292e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.9312092263135128e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.1088e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.1100266167195514e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022062112111598253\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.4441e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.445207767072134e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.7348e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.734131264034659e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.9411e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.9399445086019114e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.7793e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.7821947039919905e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.8854e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.887447620509192e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.3121e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.314073182991706e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.4795e-06]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.483642325998517e-06\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.0977e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.0967314665322192e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.9706e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.972563667455688e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.7287e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.7299300199956633e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.1180e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.120573597290786e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011116883979411796\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.6139e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.6152989701367915e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.1350e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.137809684034437e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.9959e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.996775608859025e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.4259e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.4241696489043534e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.8023e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.802848732448183e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020053013577125967\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.5428e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.540272054029629e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.1720e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.1742184142349288e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.7459e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.746431371429935e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015104957856237888\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021173810819163918\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.7169e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.71939895558171e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.0716e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.073897748137824e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.1025e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.1026920219592284e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.8995e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.902455788804218e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.9139e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.916101923095994e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.0317e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.032171950209886e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.1725e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.1742184142349288e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001317349378950894\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.5617e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.5598593715112656e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.9974e-06]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.987054232216906e-06\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.9762e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.978893851628527e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.6092e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.608954234048724e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012601216440089047\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011152651131851599\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.0076e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.009752152953297e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.3562e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.3589951777248643e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.9437e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.945905336877331e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.1543e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.1545299609424546e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.4362e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.436091305455193e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.6251e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.6240282497601584e-05\n",
      "tensor([[0.9995]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.00047277656267397106\n",
      "tensor([[5.8624e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.865269122296013e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.2262e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.2232779883779585e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014484977873507887\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.3162e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.3186476028058678e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.2288e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.230623769923113e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.7661e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.7656937163555995e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012332962069194764\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014043840928934515\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.9372e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.937169690791052e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.6948e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.69669602252543e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.0993e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.1022878324147314e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.7491e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.746431371429935e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.6436e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.6451016563223675e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.0680e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.0679369198624045e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.4237e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.421365181566216e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.6284e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.6299887142376974e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.6657e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.664021202595904e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.3356e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.336174556054175e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.4297e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.431547808460891e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.8649e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.865269122296013e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.7294e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.7299300199956633e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.1808e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.182938598911278e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.7956e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.7954969482379965e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.8341e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.8358399756834842e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.0945e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.093204476404935e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.7164e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.7166285033454187e-05\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.768372718899627e-07\n",
      "tensor([[9.5225e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.525275527266786e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.8505e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.8477610865375027e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.5788e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.581998215755448e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.7945e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.794115814846009e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.4065e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.4080565708572976e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.7705e-06]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.775209946383256e-06\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.9589e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.95782662963029e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.7758e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.7730453186668456e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.6275e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.628599395393394e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.0553e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.057438779156655e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.6306e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.631388219422661e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.2589e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.2590415028389543e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.1044e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.105126132955775e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.7673e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.7656937163555995e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.0309e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.0325391233200207e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.4184e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.419626151910052e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.0254e-06]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.046659786486998e-06\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.1581e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.157711423933506e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.9314e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.9312092263135128e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.7642e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.7656937163555995e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.0261e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.0265784769435413e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.9455e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.947057358454913e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.0168e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.0174338209908456e-05\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[2.4478e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.4497809135937132e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001179049359052442\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.2957e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.2934291589772329e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.3171e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.3172712897357997e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.7496e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.752391835907474e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.6836e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.6836347135249525e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.6617e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.661191997001879e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.7583e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.7565637942170724e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.2554e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.2572679123841226e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.6572e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.6584024453768507e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.9942e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.996406980557367e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.6158e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.6152989701367915e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.5902e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.5928356990334578e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004940060898661613\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.2659e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.267783515388146e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.1228e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.121584738139063e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.7916e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.789536301861517e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012875432730652392\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.2624e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.2650021492154337e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.9134e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.915810419945046e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.4673e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.464487574296072e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.1893e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.190294202999212e-05\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[6.0479e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.050054435036145e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014335944433696568\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014866502897348255\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.1426e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.1444157280493528e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.3265e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.325994475744665e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.0227e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.025205165205989e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.2768e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.27491273614578e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.9368e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.937169690791052e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.7876e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.789199687773362e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.6873e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.687864515697584e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.7182e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.7180087272427045e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.5503e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.552499765646644e-05\n",
      "tensor([[0.9996]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0003512522380333394\n",
      "tensor([[4.5736e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.5717806642642245e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011468591401353478\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.9793e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.978524495731108e-05\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[2.2833e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.2828839064459316e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.9258e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.9252485799370334e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.1478e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.150376283476362e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.5968e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.599881064379588e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.0137e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.011185127543285e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.3335e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.3305687136598863e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.6076e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.609338323760312e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.8331e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.832357368897647e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.7478e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.745834540808573e-05\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.5139694369281642e-05\n",
      "tensor([[2.5557e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.5570720026735216e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.5568e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.5570720026735216e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.4936e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.49115681881085e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.5543e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.5570720026735216e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.1890e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.190294202999212e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.1342e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.1324947081448045e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.7951e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.794115814846009e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.9844e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.9816695789340883e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.3757e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.3736796467565e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.2084e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.209581649978645e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.0772e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.0742631022585556e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.1273e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.127545202616602e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.0360e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.035315942019224e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.2655e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.2650021492154337e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.1196e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.1219479094725102e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.6665e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.667152461479418e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.5068e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.5048145946348086e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.9815e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.981380622368306e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.9596e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.9623946829815395e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.2044e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.2040211004205048e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.9388e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.937169690791052e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.2358e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.237700967583805e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.0480e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.0458437322522514e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.6462e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.647568418178707e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.4290e-06]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.450608336512232e-06\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010592306352918968\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.0701e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.0679369198624045e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.0861e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.088677714113146e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.2106e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.2099815648980439e-05\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.0013630344474223e-05\n",
      "tensor([[5.2543e-06]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.245222382654902e-06\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.1170e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.115623546065763e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.3050e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.3053502698312514e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.0209e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.018589480547234e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.7347e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.7372810766100883e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.5063e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.5034263671841472e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.2767e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.275150033412501e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.8965e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.8968277547392063e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.4716e-06]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.477183084527496e-06\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.5783e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.577741492539644e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.4770e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.475011180853471e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.6427e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.641607226105407e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.1089e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.1086525773862377e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.7161e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.7162487792083994e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.8900e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.8894850654760376e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.7086e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.708877895609476e-05\n",
      "counter: 22000\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.3684e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.3691157770808786e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.4135e-06]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.3974704365391517e-06\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.1284e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.1265341527177952e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.4430e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.4438202672172338e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.3195e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.3214302422711626e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.7258e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.727950964588672e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.8428e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.84284750605002e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.8844e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.88457366777584e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.2457e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.2457448065106291e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.3306e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.3319553040200844e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00027761736419051886\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.3291e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.33163979020901e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.1540e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.1545299609424546e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.3762e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.3768767530564219e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.8233e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.8207308534765616e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.9246e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.923213317757472e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00023421407968271524\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.4451e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.4466054532676935e-05\n",
      "tensor([[0.9975]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0025344991590827703\n",
      "tensor([[6.1294e-06]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.139297056506621e-06\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.4974e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.4974657208076678e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.7666e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.76822741702199e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.4036e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.4007189747644588e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.0436e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.0426799134584144e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.0980e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.100883597857319e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.4506e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.4525659177452326e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.4656e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.465895446832292e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.8134e-06]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.814704541582614e-06\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.3387e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.336174556054175e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.8241e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.826396176824346e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.7818e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.7821947039919905e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.0047e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.0073235898744315e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.6843e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.6868256352609023e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.7346e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.735551869496703e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.1714e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.172412081970833e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.0525e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.054355177795514e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010920166823780164\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.5366e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.534310861956328e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.4110e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.4126400856184773e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003429643402341753\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.7695e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.768485450767912e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.3020e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.30076584557537e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.9589e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.962064799154177e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.5106e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.510775059112348e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.5728e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.572964336257428e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.2664e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.2650021492154337e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.4060e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.406679530191468e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.7938e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.7954969482379965e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.6539e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.6566408602520823e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000273324636509642\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.9966e-06]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.020087312208489e-06\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.5905e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.589398203184828e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.9175e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.915810419945046e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010294251842424273\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.0239e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.0220011467463337e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.2389e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.236265810206532e-05\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[1.7999e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.8000764612224884e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.0380e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.035315942019224e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.1597e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.1622973033809103e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.5847e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.584885992808267e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.4954e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.49569922441151e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.4655e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.463089888100512e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016034934378694743\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.0573e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.059158527525142e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.1187e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.117371670668945e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.1904e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.191699165152386e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.2702e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.270962795591913e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010437318269396201\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.861027041944908e-06\n",
      "tensor([[3.4285e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.427325646043755e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.6077e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.6107174562639557e-05\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[7.9383e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.939653733046725e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.9121e-06]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.89442014542874e-06\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.5350e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.5374246585415676e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.7963e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.794115814846009e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.1602e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.157711423933506e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.7889e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.78815535036847e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.7640e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.765356374671683e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.7341e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.7345102605759166e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.3963e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.3947584193374496e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.1081e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.1068444261327386e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.7821e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.7821947039919905e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.2648e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.2636264727916569e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014806889521423727\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.8633e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.865055471891537e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.2322e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.2334245992824435e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.9954e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.9949940148508176e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.0497e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.0490472959645558e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.8091e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.810210521100089e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001656550302868709\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.8686e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.8716033082455397e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013489440607372671\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001676223037065938\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.6637e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.6657529562944546e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.4894e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.4883305235998705e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.8125e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.8133788873674348e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.0412e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.0384995877975598e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.9056e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.904180266545154e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012470068759284914\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001347751822322607\n",
      "tensor([[0.4496]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.7994040250778198\n",
      "tensor([[0.0468]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.04795887693762779\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0089]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00890729296952486\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0279]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.028322026133537292\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0307]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.031173521652817726\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0069]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.006951236631721258\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0021]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002060209633782506\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0048]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004766660742461681\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00024923807359300554\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0057]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005749150179326534\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00161044264677912\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0041]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004112402908504009\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0057]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005695258267223835\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0035]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003516292432323098\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0035]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003530049929395318\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000674534821882844\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0082]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00823643896728754\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010381363099440932\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0021]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0021182068157941103\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0039]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003936877474188805\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0032]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0031997438054531813\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0018679649801924825\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0024]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0024407466407865286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0023]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002255657222121954\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012856042012572289\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017290152609348297\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0153]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.01544165425002575\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014107637107372284\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009169012191705406\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003561415651347488\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0078]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00787561759352684\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0039]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003914796747267246\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0067]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.006708543282002211\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001211005263030529\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014163066225592047\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010430290130898356\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0016452487325295806\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008116081007756293\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007283955346792936\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0021]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002143234247341752\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0028]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0027776157949119806\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002847122959792614\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003318144881632179\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0020]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0020338103640824556\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000344812695402652\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005691478727385402\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017019083024933934\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000512552447617054\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006703596445731819\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017122376011684537\n",
      "tensor([[0.9916]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.008427507244050503\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00047981328680180013\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009329497115686536\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000651154259685427\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00120909558609128\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000755655113607645\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006663634558208287\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00046305646537803113\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0031]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0030560651794075966\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002521594287827611\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009912395616993308\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012545703211799264\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009485211921855807\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008278338355012238\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005414163460955024\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001631039660423994\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007737887790426612\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007700307760387659\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008648200309835374\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0038]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0038217524997889996\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002631295064929873\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0025]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0025229663588106632\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008203174802474678\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001743524451740086\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000349284615367651\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0077]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.007733248174190521\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007935930043458939\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007205816800706089\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0021]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002115996554493904\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004996117204427719\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0032]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0031651228200644255\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001254756498383358\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015440576244145632\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004530383157543838\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005138047854416072\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0025]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0024693075101822615\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0022]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002230328042060137\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006918915896676481\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004949602298438549\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00025746561004780233\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005207225331105292\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007389532984234393\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005127909826114774\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0116]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.011698216199874878\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0018332705367356539\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0016428607050329447\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001191073446534574\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0020]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0019577222410589457\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000344812695402652\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006073173135519028\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008864754345268011\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001058542518876493\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014252486289478838\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004306170449126512\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008788393461145461\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0105]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.010599019937217236\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00023600264103151858\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016040896298363805\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020637256966438144\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0019279815023764968\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017736777663230896\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010310957441106439\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003765934379771352\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012518251314759254\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002982465084642172\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000538553751539439\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00039776129415258765\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00024578016018494964\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000489652797114104\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007152730831876397\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00025204019038937986\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008114887750707567\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00037075000000186265\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015009575872682035\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018544725026004016\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005381362861953676\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003232286253478378\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006433410453610122\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014535017544403672\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005085568991489708\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002472110209055245\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005890077445656061\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001296485133934766\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006861059810034931\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021728253341279924\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011796157341450453\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005931228515692055\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009001967846415937\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006461442681029439\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006622479995712638\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005206628702580929\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002212769177276641\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00026301026809960604\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013179455709178\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0040]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004048066213726997\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001358537352643907\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006917126593180001\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015968310181051493\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020386866526678205\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012736679054796696\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005328882834874094\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00031106540700420737\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022664254356641322\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007214167271740735\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006796045927330852\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0024]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0024302902165800333\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004791573155671358\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006119692698121071\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008700101170688868\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.0860e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.08896230510436e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021853450743947178\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008145311148837209\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002069687470793724\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0022]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0021863621659576893\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00047438667388632894\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.2649790025752736e-06\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0016466219676658511\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010335979459341615\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007628727471455932\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005268053500913084\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001861030177678913\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.3545e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.352406050311401e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006281918031163514\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000618708785623312\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007260096608661115\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.8290e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.829504152643494e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006132813869044185\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002636064891703427\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005490498733706772\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000380409590434283\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002641430764924735\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00023087543377187103\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015718766953796148\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009639139170758426\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022401934256777167\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000316014135023579\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016493965813424438\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0018397794337943196\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007287534535862505\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0028]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0028495818842202425\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007547603454440832\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016952997248154134\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.4370e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.434413030045107e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005076623638160527\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.4817e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.483548637945205e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001044923992594704\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00043610305874608457\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003622234216891229\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.2751e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.273466664832085e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00040891184471547604\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005423108814284205\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009634366142563522\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006131621194072068\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002866798313334584\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009425550815649331\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005404621479101479\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006342156557366252\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004318096616771072\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.6411e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.6433095121756196e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00026259294827468693\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019832431280519813\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020351096463855356\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000392454385291785\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00031893569394014776\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.2761e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.279427856905386e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.9803e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.978315210901201e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012708517897408456\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003248384746257216\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011934007052332163\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001593955239513889\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002178787108277902\n",
      "tensor([[0.2069]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.5753259658813477\n",
      "tensor([[0.2923]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.3456682860851288\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0347]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.03528112545609474\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0427]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.04368390142917633\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0137]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.013749577105045319\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0239]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.024169448763132095\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0172]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.017317064106464386\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0084]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.008443015627563\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0115]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.011582978069782257\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0022]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0021852271165698767\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.1091]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.11549510806798935\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0040]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003960215486586094\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0031]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003090084530413151\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0019279815023764968\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0023]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0023211934603750706\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001457799575291574\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0021]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002083802130073309\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002982465084642172\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0043]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004287959076464176\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007279780111275613\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007227290188893676\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0023]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0023484963458031416\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0051]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005074719432741404\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0023]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002283496083691716\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0021]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002065047388896346\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0068]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.006780913099646568\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014467566506937146\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0047]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004682759288698435\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0105]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.010571791790425777\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0043]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004353748168796301\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012511089444160461\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0026]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002648221096023917\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0039]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0038616019301116467\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008831346640363336\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002861432440113276\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001161057036370039\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0021]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0020869679283350706\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0022]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002199683105573058\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015758168883621693\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015370134497061372\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0025]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0025169907603412867\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008684590575285256\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000899480888620019\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00047796464059501886\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0021]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0021472962107509375\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00123786018230021\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005178003921173513\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011214939877390862\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011855235788971186\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00042316329199820757\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007292903028428555\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0040]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004021195229142904\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0021]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00214263703674078\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0031]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0030917588155716658\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0021]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002052444964647293\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003080842434428632\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010240551782771945\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009143358911387622\n",
      "tensor([[0.9997]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0003015257534570992\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001503643812611699\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0028]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0028410940431058407\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001905145909404382\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012498557334765792\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005396272172220051\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000629742513410747\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0034]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0034183210227638483\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010399860329926014\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021281122462823987\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0038]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003843651618808508\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0037]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0037460667081177235\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008510992047376931\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0020]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0020043659023940563\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022211157192941755\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007897753384895623\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0016067412216216326\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0026]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0026474441401660442\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008938133250921965\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005018778028897941\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0043]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004268564283847809\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0039]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003943998366594315\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0044]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004417207092046738\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000188487654668279\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0019014662830159068\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001184151042252779\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011212262324988842\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005978940753266215\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005340213538147509\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0025]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0024830505717545748\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008009302546270192\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008434632909484208\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005009236629121006\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006802607094869018\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0077]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.007753671146929264\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010533514432609081\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012588076060637832\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007776661077514291\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008728736429475248\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003402215661481023\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0020]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0019927197135984898\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020851878798566759\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0051]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005140320863574743\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015834582736715674\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011623580940067768\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00048231787513941526\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0054]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005365254823118448\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00037146551767364144\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005249566747806966\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022652330517303199\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015289759903680533\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010307973716408014\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014392356388270855\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0027]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002661309204995632\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0022]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0022281177807599306\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009597375756129622\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012490798253566027\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0020]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001973249949514866\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008026601863093674\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00033306662226095796\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006156670278869569\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006342753185890615\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017286841466557235\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017503909766674042\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008469829917885363\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.4570753086882178e-06\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003717636573128402\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004906665999442339\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000797351123765111\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010318096610717475\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001166009926237166\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003792170318774879\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007064453093335032\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005808967398479581\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008010495803318918\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0053]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0052990997210145\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005891270120628178\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005114193772897124\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007527919369749725\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021555362036451697\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007776064448989928\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00033706147223711014\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002743978111539036\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001809564302675426\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0016749214846640825\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.6476e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.65045765042305e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00045733180013485253\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021793833002448082\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0031]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0030571413226425648\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005219748709350824\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004822582413908094\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007255920791067183\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000934978190343827\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003199492930434644\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002167459751944989\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004527997807599604\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001211661729030311\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0075]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.007481717504560947\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005607388447970152\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000194031948922202\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022562903177458793\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002803599345497787\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002371353912167251\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016648962628096342\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001217797034769319\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006095240241847932\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006881935405544937\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011684566270560026\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020959188987035304\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020476292411331087\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009396913810633123\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00026074470952153206\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008645217167213559\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007959194481372833\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014413442113436759\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017608761845622212\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003817213873844594\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00026271218666806817\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021066500630695373\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003842853766400367\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.1635e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.166169573087245e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0023]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002338519087061286\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021239390480332077\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004770105006173253\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002481649280525744\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009046114864759147\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00023701615282334387\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006419096025638282\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.5962e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.596808376954868e-05\n",
      "counter: 23000\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0018431830685585737\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00023755272559355944\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017598250415176153\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006744751590304077\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000194151172763668\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019522426009643823\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00026551433256827295\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013984227553009987\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005935999797657132\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008235984132625163\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018520878802519292\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000440157949924469\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004381901235319674\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002641430764924735\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00026044659898616374\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002205615019192919\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018157223530579358\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005014007329009473\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008077903185039759\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003124367503914982\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.1466e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.1485694964649156e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0021]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0020866692066192627\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0025]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002489563776180148\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012660828360822052\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011510915355756879\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009912395616993308\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00028596437186934054\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001949600176885724\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00027624607901088893\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022545017418451607\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.576279254957626e-07\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001375769788865\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.9883e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.989033186575398e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015307644207496196\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015790517500136048\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001174961100332439\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002770211431197822\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012233584420755506\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008605844341218472\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000792698236182332\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006159652257338166\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004675289092119783\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00025090741110034287\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001467574038542807\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00034636296913959086\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007938912604004145\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013574630720540881\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002532326034270227\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003100518079008907\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00023552568745799363\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005195297999307513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012814265210181475\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00023326017253566533\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00031875682179816067\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007267850451171398\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012605383526533842\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006205576355569065\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015230146527756006\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011206301132915542\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020774376753252\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010973817552439868\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00033413985511288047\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003318144881632179\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005802406813018024\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015534176782239228\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004117740609217435\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019176652131136507\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003938258159905672\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008060603868216276\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0022]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002214378211647272\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00046162528451532125\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004186910518910736\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010180991375818849\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018890495994128287\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004768316284753382\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.576279254957626e-07\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011122845171485096\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006711946916766465\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00031452355324290693\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013316563854459673\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003124367503914982\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.6611e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.662601688527502e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.3587e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.360017869155854e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010878439206862822\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007053716690279543\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006140567129477859\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0022]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0022300295531749725\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004894142621196806\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019331654766574502\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00026968776364810765\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003259713121224195\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000293476739898324\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001759683946147561\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002970540663227439\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00028781263972632587\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000147591985296458\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00024381271214224398\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009268047288060188\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006813343497924507\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022497323516290635\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022652330517303199\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001276812981814146\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014732598792761564\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014944000577088445\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012017018161714077\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021227466640993953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.1547e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.155692532658577e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006418499397113919\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.2082e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.209581649978645e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001391865371260792\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002460782416164875\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002301003987668082\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000769672857131809\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017948569438885897\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.5315e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.532882252940908e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0034]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0034412278328090906\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017733954882714897\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003243614628445357\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010401551116956398\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00026092358166351914\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00035727446083910763\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012994656572118402\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.8073e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.808809196925722e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000547618605196476\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003012276429217309\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010015614097937942\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006058859289623797\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001680396089795977\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001946280972333625\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005850118468515575\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00037200216320343316\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013519247295334935\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014324022049549967\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014121337153483182\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010789490770548582\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.4064e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.406055323779583e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00028041956829838455\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002303984947502613\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.2445e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.243662159657106e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011689154052874073\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001365635689580813\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001827049272833392\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022968306438997388\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003331858606543392\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001657742541283369\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017739915347192436\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002525171439629048\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005865624989382923\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.0912e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.090121602639556e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022026342048775405\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00024172605480998755\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012929082731716335\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001765645429259166\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005315166199579835\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003167892573401332\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009600955527275801\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004972263122908771\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013818747829645872\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022723872098140419\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.8906e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.892258973792195e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018353953782934695\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00037945553776808083\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00046347390161827207\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008053445490077138\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011642196914181113\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002970540663227439\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013578860671259463\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015832246572244912\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011516280210344121\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002286695671500638\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018049914797302336\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.4059e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.40913164918311e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004246540484018624\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013847117952536792\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009938647272065282\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012332962069194764\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003282966499682516\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002502516144886613\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00031011144164949656\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007158098742365837\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0016918180044740438\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001890838029794395\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.3113030945532955e-06\n",
      "tensor([[8.0447e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.046950824791566e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005393886822275817\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00025991001166403294\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.1811e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.179536573356017e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00042608517105691135\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020142438006587327\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017948569438885897\n",
      "tensor([[0.9934]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.006641939282417297\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011850105511257425\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003934680426027626\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000398655713070184\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017555108934175223\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.1604e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.163310172269121e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014526706945616752\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018920304137282073\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003883996687363833\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.3139e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.312331242952496e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017191456572618335\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019128959684167057\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00034594559110701084\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.4914e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.489509830018505e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0021]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0021001084242016077\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00029168807668611407\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015975321002770215\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010860556358238682\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020988998585380614\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002708801766857505\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015361297118943185\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010938050400000066\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004716435505542904\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000899480888620019\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021430166088975966\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.3768e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.376483386498876e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011629542132141069\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00028852809919044375\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.0290e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.027634273981676e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000222946226131171\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003706307616084814\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.145769485650817e-06\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003051627427339554\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013495402527041733\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002867394359782338\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016857613809406757\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00030462612630799413\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003015853581018746\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.7449e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.744383376324549e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[2.4043e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 2.4020961063797586e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020088783639948815\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.8309e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.829289047047496e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015724942204542458\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018085684860125184\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.5886e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.588264007703401e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003254346956964582\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001747760979924351\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015999165771063417\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016297238471452147\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004523823445197195\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003116616571787745\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002324255183339119\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001119437874876894\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000820913992356509\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015385141887236387\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002197268622694537\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003228112473152578\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013143688556738198\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012726400746032596\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.9098e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.912743553286418e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.2933e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.291349513456225e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012189892731839791\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019796661217696965\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011158612323924899\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015730902669019997\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015581867774017155\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001190971743199043\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018037992413155735\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00042912628850899637\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015218224143609405\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.0835e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.084160410566255e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005185756599530578\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022878879099152982\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.0190e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.020250293659046e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00026891269953921437\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003046857600566\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014616127009503543\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001532552851131186\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00023600264103151858\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003205455432180315\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014717469457536936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.9215e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.924665937433019e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003319337556604296\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00024244147061835974\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.6965e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.6987467461149208e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019295884703751653\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003327088779769838\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000722609693184495\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020512062474153936\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.8792e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.8803376810392365e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.1496e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.151388515718281e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.6626e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.659792128019035e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00043115371954627335\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.6772e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.6776742490474135e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001812145346775651\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00036228305543772876\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006386292516253889\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014061723777558655\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007177782827056944\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004006234521511942\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017662416212260723\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003082631155848503\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021066500630695373\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013233107165433466\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.9195e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.920338637428358e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.0790e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.076755329966545e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00039633020060136914\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002884684654418379\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[6.0027e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.002367808832787e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002432761393720284\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019582042295951396\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.1611e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.1622973033809103e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008049866301007569\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.3455e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.346444858238101e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020231862436048687\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00039227548404596746\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017602801381144673\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00033753845491446555\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003061167080886662\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.1985e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.195974805857986e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00035131187178194523\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002987234911415726\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.2000e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.1976599934278056e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017435879271943122\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00031303297146223485\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00035453165764920413\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011116883979411796\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.1143e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.112521027214825e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.7667e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.769678581506014e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.0750e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.072484054835513e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006407763576135039\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.9011e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.901406176330056e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.5618e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.559592970414087e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00024715138715691864\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012762169353663921\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.4733e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.4718562751077116e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.8883e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.887447620509192e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.0752e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.076755329966545e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001837780000641942\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.0041e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.006667096400633e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00028167161508463323\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.7156e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.7166285033454187e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001285754842683673\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00023248513753060251\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001401403424097225\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00046430874499492347\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[1.0275e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 1.025205165205989e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005559081910178065\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.4548e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.456819002982229e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.9429e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.942548786057159e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.4807e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.483548637945205e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001401403424097225\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.9379e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.941096166381612e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.5248e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.522389205405489e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00037045186036266387\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002717148745432496\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00023159086413215846\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010306174226570874\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.9477e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.94850997813046e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00023838737979531288\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.7753e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.777278031222522e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011772610014304519\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.4809e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.482369695324451e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016273392247967422\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.5709e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.570076559204608e-05\n",
      "tensor([[1.4255e-09]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 20.36871910095215\n",
      "tensor([[0.3486]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.4286164343357086\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0327]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.03326403349637985\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0625]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.06457291543483734\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0089]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.008930927142500877\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0183]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.018487242981791496\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0709]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.07355522364377975\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0175]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.017657890915870667\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0108]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.010871220380067825\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0185]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.01869376190006733\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0086]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.008644765242934227\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0072]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.007253184914588928\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0073]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.007297254167497158\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0046]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004565452225506306\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0179]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.018089018762111664\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0029]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0028868820518255234\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0084]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.008479923941195011\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0109]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.01098414696753025\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0056]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005620449315756559\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0086]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.008592820726335049\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0147]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.014852365478873253\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0081]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.008136441931128502\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015294321347028017\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0036]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003633057465776801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0043]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004277663305401802\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0052]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005196459591388702\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0047]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004728691652417183\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0035]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003499364946037531\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0113]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.011315708048641682\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0047]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0047256373800337315\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0071]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.007081969641149044\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0061]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.006100389175117016\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0026]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0026344757061451674\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0089]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.008957689628005028\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0037]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0036618916783481836\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0028]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0027644664514809847\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0109]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.010984688997268677\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0043]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004346804227679968\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0045]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004534675739705563\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0036]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003649807535111904\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0047]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004694436676800251\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0066]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.006572939455509186\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001625188859179616\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014664549380540848\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0042]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004186859354376793\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0034]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0033974479883909225\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0040]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004053512122482061\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0034]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003452233038842678\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0055]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005556375253945589\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0030]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0029759537428617477\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0022]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002222801325842738\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001740718143992126\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0018679649801924825\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010913603473454714\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013524494133889675\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0025]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002454967238008976\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0018035931279882789\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0023]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0022665895521640778\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0101]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.010141242295503616\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0028]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002815929241478443\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008805693942122161\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0036]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003624742152169347\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0029]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002896625781431794\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015467440243810415\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0020]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002014041179791093\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001644412986934185\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0023]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0022655739448964596\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0048]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004855480510741472\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0020]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002031720010563731\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0058]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005809400230646133\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0050]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005049798171967268\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0035]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003514737356454134\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013787710340693593\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0018110572127625346\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0049]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004901660606265068\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0026]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0025895358994603157\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010672539938241243\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0042]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0042087663896381855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001581906108185649\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 7.867844033171423e-06\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011602812446653843\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.4305124977909145e-06\n",
      "tensor([[0.0061]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.006098410114645958\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0029]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00286733522079885\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0031]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0030635984148830175\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013115658657625318\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013842025073245168\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0023]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0023444935213774443\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0018022197764366865\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010482200887054205\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0050]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004965752828866243\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015690709697082639\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0026]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0025618677027523518\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013423627242445946\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0039]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0039384332485497\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0035]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003547874977812171\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0023]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002333261538296938\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006339174578897655\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0025]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0025166920386254787\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001098819077014923\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0038]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003793691284954548\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005607388447970152\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009078330476768315\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013524494133889675\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014415038749575615\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0022]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0021680237259715796\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009892109083011746\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014868698781356215\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010495923925191164\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00048541880096308887\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012390537885949016\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001719820313155651\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 8.344653679159819e-07\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0019275634549558163\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009563965140841901\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000833143072668463\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012182858772575855\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0022]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002216170309111476\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0016352784587070346\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0021]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0020750819239765406\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010483990190550685\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0030]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0029562257695943117\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0037]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0036720018833875656\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003867897321470082\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014826912665739655\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.768372718899627e-07\n",
      "tensor([[0.0040]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003964763134717941\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013526285765692592\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001194832962937653\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00107065518386662\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0040]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003989837132394314\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011424985714256763\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011250742245465517\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003416525723878294\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005302046192809939\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011507931631058455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0030]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003046678612008691\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0034]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003400916699320078\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014999427367001772\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009754883940331638\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001922845607623458\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014631717931479216\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004496989422477782\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007200448890216649\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0034]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003361324779689312\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001479826052673161\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00038738601142540574\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004979419172741473\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0040]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003967994824051857\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001728119677864015\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004716435505542904\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001643875613808632\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009783522691577673\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017291944241151214\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007821398903615773\n",
      "counter: 24000\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006967229419387877\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005854293121956289\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005517335375770926\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008787796832621098\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008898759260773659\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007203430868685246\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009007336921058595\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007177186198532581\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008267600787803531\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001446398557163775\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001177885220386088\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014899738598614931\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008991825743578374\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010411792900413275\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005412970203906298\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013501814100891352\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001381576294079423\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014036607462912798\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003243614628445357\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007299464195966721\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004406349908094853\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014595903921872377\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006186491227708757\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007150940946303308\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009429130586795509\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011615343391895294\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005817913333885372\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0027]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0026649548672139645\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005534630618058145\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001570026041008532\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010559768415987492\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005547750624828041\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00033646522206254303\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005567431217059493\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005998025881126523\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006821693386882544\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008190647349692881\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003686630807351321\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003810058697126806\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0020]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0019966615363955498\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002973521768581122\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009879580466076732\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007884033257141709\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001382710412144661\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000776174827478826\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008710242691449821\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003178624901920557\n",
      "tensor([[0.8140]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.20585033297538757\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000547618605196476\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0016806531930342317\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0031]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0031322967261075974\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 5.960466182841628e-07\n",
      "tensor([[0.0039]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003887570695951581\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014721256447955966\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0099]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.009988555684685707\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0022]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0022068514954298735\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0054]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005393300671130419\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0079]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00795882660895586\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0053]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005277648102492094\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0065]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.006491106003522873\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0047]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004723121877759695\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0022]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002168143168091774\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0048]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00481313606724143\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0026]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0025999341160058975\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015300887171179056\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0028]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0028353556990623474\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009093245607800782\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0020]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0019617832731455564\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011953701032325625\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0036]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00364352623000741\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0038]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0038566954899579287\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0021]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0020744248759001493\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006351103074848652\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007367462967522442\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007008981774561107\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0025]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0025280455593019724\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001746808411553502\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0090]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00906270183622837\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015459679998457432\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013574630720540881\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0049]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00493268808349967\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0038]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003785135457292199\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009738774970173836\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.145769485650817e-06\n",
      "tensor([[0.0033]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0033488254994153976\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001335200504399836\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007059681229293346\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0043]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004352670628577471\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00044379543396644294\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0018855216912925243\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0028]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0028128211852163076\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00062777433777228\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.0\n",
      "tensor([[0.0021]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0021354095079004765\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0020]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0019793417304754257\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005409392178989947\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0027]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0027516160625964403\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0033]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003274790244176984\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000352683273376897\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00046263905824162066\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0029]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002869068644940853\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013773385435342789\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0020]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00202425429597497\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007690167403779924\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0023]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0023057200014591217\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0028]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0027921400032937527\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00144502567127347\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0033]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.003329508937895298\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0022]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0022022516932338476\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009091455722227693\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0029]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0028657810762524605\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002258675085613504\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001320757088251412\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007264868472702801\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010137927019968629\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003460052248556167\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017014903714880347\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001016954891383648\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005896041402593255\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017958305543288589\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011576556134968996\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006581922061741352\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00042316329199820757\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0018558426527306437\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004989557201042771\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012852461077272892\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011153478408232331\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0025]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002489205216988921\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007822592160664499\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008484743302688003\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001841451390646398\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00035954025224782526\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0017]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017094910144805908\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008463864214718342\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001400795765221119\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003675301559269428\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009521605097688735\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0016347412019968033\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006633216398768127\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006227047415450215\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0055]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.005465275142341852\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004937078920193017\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00045601988676935434\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012416199315339327\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009203614899888635\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00047367106890305877\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0017715281574055552\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007642447017133236\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012545703211799264\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011046667350456119\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006784713477827609\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008239563321694732\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011444081319496036\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004928134148940444\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0027]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0027394231874495745\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007097855559550226\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008345150854438543\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007175396895036101\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015390431508421898\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006497824797406793\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009562175255268812\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0030]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00303041678853333\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007048944826237857\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010671346681192517\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002549615746829659\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008012881735339761\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0041]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.004069730639457703\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00037510276888497174\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0020]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001999826868996024\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00023952014453243464\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003368229663465172\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006955896387808025\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006946353241801262\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000444988050730899\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010561557719483972\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000495496962685138\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0019335354445502162\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009053870453499258\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008842084789648652\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004554832121357322\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00030027367756702006\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000457152898889035\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004164251440670341\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006473371176980436\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007095469627529383\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006787099409848452\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000836722319945693\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009923730976879597\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005240024765953422\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007700307760387659\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000730662199202925\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005485131405293941\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019283962319605052\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005556099931709468\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006563432980328798\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00037760709528811276\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005172039964236319\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013606264255940914\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000715511676389724\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006852709339000285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0016]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0015758168883621693\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0027]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002672066679224372\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010680296691134572\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014139271806925535\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011454821797087789\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009164836374111474\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014681702305097133\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006704193074256182\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001495592441642657\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007749818032607436\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004161269753240049\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012992712436243892\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002032128832070157\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000998160568997264\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0025]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002546390751376748\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006413728115148842\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00027427857276052237\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00036478735273703933\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000797351123765111\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006725068669766188\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000715690606739372\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00025370955700054765\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004895931924693286\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010056186001747847\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006656477344222367\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006707771681249142\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00026008888380602\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000778381887357682\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008975717937573791\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005564449238590896\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001495592441642657\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003876841510646045\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004520842048805207\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000700957840308547\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007638271199539304\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0022]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0022105551324784756\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.7163e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.71457887114957e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0011729322141036391\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006289671291597188\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008949468610808253\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.576279254957626e-07\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000521378533449024\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005268053500913084\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006427445914596319\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006314721540547907\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003006910264957696\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002896609075833112\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00026533546042628586\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001422164379619062\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00036914009251631796\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00040980629273690283\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005277595482766628\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00024238185142166913\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012547493679448962\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010107497218996286\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019957625772804022\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00045673546264879405\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001243171631358564\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00034850946394726634\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008728139800950885\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000489294994622469\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007131854072213173\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013717280235141516\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020249748195055872\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000499790592584759\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012585688382387161\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00028322177240625024\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006932634278200567\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010296637192368507\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005056944210082293\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010231004562228918\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006795449880883098\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001450935029424727\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005980133428238332\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00032984689460135996\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00036043464206159115\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005103459698148072\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009265660773962736\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012970630777999759\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006934424163773656\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0021]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.002131049055606127\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004289474163670093\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007278587436303496\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012499875447247177\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014444884145632386\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006711350288242102\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00046985456719994545\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011444746633060277\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005554311210289598\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003390886995475739\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005380766815505922\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.3113030945532955e-06\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000537003215868026\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0018]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001809325534850359\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002482245326973498\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012992116389796138\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008364837267436087\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00025049009127542377\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008383329841308296\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00034683995181694627\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017048382142093033\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008930974290706217\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002447666192892939\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0014]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001402586349286139\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005769009003415704\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001443728688172996\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007624551653862\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004773683031089604\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005373013555072248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022610597079619765\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000667138840071857\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.768372718899627e-07\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010318714193999767\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009152904385700822\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005284155486151576\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00038118474185466766\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005335442838259041\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0014512335183098912\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008736491436138749\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.2209e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.218382961582392e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005402832175604999\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019021650950890034\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000351192633388564\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005778551567345858\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00047981328680180013\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00024458777625113726\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017829338321462274\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006586693925783038\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003334839711897075\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0012]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001189163769595325\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000933427014388144\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002868587034754455\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003378961991984397\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007566691492684186\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009305633138865232\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001382327318424359\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006500210729427636\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018264530808664858\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022950422135181725\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001600512769073248\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005606792401522398\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00039746315451338887\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002575252146925777\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004108796129003167\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00041135665378533304\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006705982377752662\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006080926395952702\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00041028333362191916\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018091646779794246\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00029794839792884886\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004445706435944885\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005511372000910342\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001546860148664564\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002985446189995855\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007947264239192009\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002722514618653804\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003520869940984994\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002497150271665305\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020774376753252\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000822047411929816\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020863802637904882\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005942560383118689\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006089872331358492\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008719787583686411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00027916752151213586\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005587111809290946\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00047277656267397106\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008287883247248828\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004467770049814135\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00035697632119990885\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002233635459560901\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003194127057213336\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018592417472973466\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001581436226842925\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021233428560663015\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022735795937478542\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001268833875656128\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001403191708959639\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002445281425025314\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005476782098412514\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016547618724871427\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020386866526678205\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017877030768431723\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005160709260962903\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004180351388640702\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016285314632114023\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002684357459656894\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00025227866717614233\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00027415933436714113\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014985731104388833\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003772493510041386\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004431394918356091\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000262354442384094\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009386174497194588\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00036335631739348173\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000413205154472962\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004569143638946116\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002749343984760344\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006837201653979719\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019731084466911852\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012428341142367572\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00025132476002909243\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019146845443174243\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005254337447695434\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00043562601786106825\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005773780285380781\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005104652373120189\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000492395949549973\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017602801381144673\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0012617915635928512\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000543265079613775\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002307561953784898\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007682412397116423\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020875725022051483\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.3424e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.340484393760562e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003878630232065916\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003954357816837728\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000509630364831537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005485728033818305\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0010]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009912395616993308\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005562063888646662\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00033795583294704556\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008334413287229836\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008735895389690995\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002628314250614494\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00026718369917944074\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006116710719652474\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000256034720223397\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007277990807779133\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0021]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0020857732743024826\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016714539378881454\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[3.7811e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 3.7790057831443846e-05\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002963982115034014\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002980676363222301\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.8816e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.882938320515677e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011319562327116728\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00035435278550721705\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011635503324214369\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014830735744908452\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018872611690312624\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002808369172271341\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006052895332686603\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007776064448989928\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.6678e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.665451266802847e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003229901194572449\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002532326034270227\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002367776760365814\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0011]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0010867657838389277\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00028131387080065906\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004821389738935977\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.4305124977909145e-06\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00028870697133243084\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.1848e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.185497765429318e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003344379656482488\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020673027029260993\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021132078836672008\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003390886995475739\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015933590475469828\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005569220520555973\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00030540121952071786\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012386612070258707\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009449415374547243\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005788690177723765\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00037438725121319294\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00039334880420938134\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017447801656089723\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00031452355324290693\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002683761122170836\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002709994441829622\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000371644418919459\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004135032941121608\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002746959216892719\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 6.318112355074845e-06\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00040211417945101857\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.0\n",
      "tensor([[0.0015]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001524775754660368\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006875970866531134\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021966724307276309\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.4172e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.416530181420967e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007683009025640786\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005528070614673197\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003991327539552003\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.7353e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.732720844913274e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020047051657456905\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001606474252184853\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001618993264855817\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003290717722848058\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 6.318112355074845e-06\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003036721609532833\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021346702123992145\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001435978920198977\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00028978014597669244\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020148399926256388\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002547230978962034\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00028298329561948776\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.7688e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.769678581506014e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001790683891158551\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004313326207920909\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016684731235727668\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021000920969527215\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00044021758367307484\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002994985843542963\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005056944210082293\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.8564e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.856200681999326e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020118591783102602\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.576279254957626e-07\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010401551116956398\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022998116037342697\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00042536959517747164\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.576279254957626e-07\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006895654369145632\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.2737e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.272030779859051e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00037343319854699075\n",
      "counter: 25000\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.9879e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.985908112255856e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00047051053843460977\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002002916589844972\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012011056969640777\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014711508993059397\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001806183863664046\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005241814069449902\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004227458848617971\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002787501725833863\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005607985076494515\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.576279254957626e-07\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002348102570977062\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004424835497047752\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017507416487205774\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003243018581997603\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00041696178959682584\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00028954166918992996\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019468771643005311\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00033694220473989844\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002823870745487511\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015790517500136048\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005730840493924916\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002331409341422841\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015695134061388671\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00027916752151213586\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002983657468575984\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015379181422758847\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000557041319552809\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.7105e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.713138620601967e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001365635689580813\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017012612079270184\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0009]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0009213160374201834\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001263698359252885\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021996532450430095\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00027159563614986837\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019874163263011724\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015140726463869214\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019695314404089004\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016768192290328443\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002262848283862695\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010073692101286724\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018836841627489775\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000365562504157424\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003761164261959493\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001607666490599513\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010532695159781724\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003311586333438754\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017221264715772122\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002803599345497787\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022777529375161976\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.9728e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.9723540188279e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00029377485043369234\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022217119112610817\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012350844917818904\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005399254150688648\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004907858674414456\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[5.1275e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 5.12613078171853e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016225701256189495\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00015689173596911132\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003377769608050585\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001273832458537072\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.7583e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.759395182598382e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005923475255258381\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002843545807991177\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019075305317528546\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000217044071177952\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0013209361350163817\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017060304526239634\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010741333971964195\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002893627970479429\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.3244e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.32260081754066e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002602677559480071\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021304968686308712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0019]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0018664720701053739\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.5655e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.567003144184127e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00042686035158112645\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00037140591302886605\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000244468537857756\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016941074864007533\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001533149043098092\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000194031948922202\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003240633523091674\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00024011633649934083\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.4591e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.456819002982229e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003013468813151121\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.2311e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.228879647096619e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016619155940134078\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003790381597355008\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001490227150497958\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007650201441720128\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011450707825133577\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000771164137404412\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.4850e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.488061575917527e-05\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 5.483642325998517e-06\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014645933697465807\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016398582374677062\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018687802366912365\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016917228640522808\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002667067456059158\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.000232723614317365\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00024285880499519408\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006698228535242379\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00025108628324232996\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.2728e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.272030779859051e-05\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00028954166918992996\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013018501340411603\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 1.1920930376163597e-07\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002243770577479154\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005150571232661605\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016857613809406757\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[6.0937e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 6.0917802329640836e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017638569988775998\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0008250897517427802\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006804396398365498\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002538287953939289\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017447801656089723\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001936146290972829\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00026169861666858196\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014622088929172605\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019605889974627644\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00020559754921123385\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005119561101309955\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021924992324784398\n",
      "tensor([[0.9993]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0007115749176591635\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005882324185222387\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0008]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0007907893741503358\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018830879707820714\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018634148000273854\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012607176904566586\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018598377937451005\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001759683946147561\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00035638007102534175\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00028071767883375287\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00024899959680624306\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0006]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0006213926244527102\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002935363445430994\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.814704541582614e-06\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018872611690312624\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011420901864767075\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016148202121257782\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014413442113436759\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019206460274290293\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 4.768372718899627e-07\n",
      "tensor([[7.8861e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.886005187174305e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011498397361719981\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012619099288713187\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018443378212396055\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002454820496495813\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002571078948676586\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018103569163940847\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011760687630157918\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002724303340073675\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010330018267268315\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00012231621076352894\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018497032579034567\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013042346108704805\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002774981257971376\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019737044931389391\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[4.5176e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 4.5181343011790887e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004901298671029508\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00028876657597720623\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003183990775141865\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002536499232519418\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002529941266402602\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00022676178195979446\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[8.2176e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 8.219818118959665e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00021430166088975966\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 3.576279254957626e-07\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016988767310976982\n",
      "tensor([[1.0000]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 2.3841860752327193e-07\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001276812981814146\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001686357572907582\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.8655e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.868122338550165e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00013388099614530802\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001307811471633613\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002156132395612076\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00027415933436714113\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[7.9836e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 7.981380622368306e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00026873385650105774\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.7302e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.727950964588672e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0001555206108605489\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0013]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.001346839009784162\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[9.9852e-05]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 9.984276402974501e-05\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016422428598161787\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00027076093829236925\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00016404544294346124\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003093959530815482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004959740326739848\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010687683970900252\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003067725629080087\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005311588174663484\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00027320539811626077\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002002916589844972\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00018836841627489775\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019802623137366027\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0004071229777764529\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0002822678361553699\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00037218103534542024\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003640122013166547\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00011528202594490722\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00010908245167229325\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0003]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00029866385739296675\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0001]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00014580358401872218\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00019033574790228158\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0005]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0005414759507402778\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.0003685438132379204\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward>) tensor([[1.]])\n",
      "loss: 0.0\n",
      "tensor([[0.0002]], grad_fn=<SigmoidBackward>) tensor([[0.]])\n",
      "loss: 0.00017722031043376774\n"
     ]
    }
   ],
   "source": [
    "class BasicConv(nn.Module):\n",
    "    def __init__(self,inchanel,outchanel,kernersize,stride):\n",
    "        super(BasicConv,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inchanel,outchanel,kernel_size=kernersize,stride=stride,padding=kernersize//2)\n",
    "        self.batchnorm = nn.BatchNorm2d(outchanel)\n",
    "        self.relu = nn.LeakyReLU(0.2)\n",
    "    def forward(self,x):\n",
    "        x = self.batchnorm(self.conv1(x))\n",
    "        return self.relu(x)\n",
    "    \n",
    "class BasicLinear(nn.Module):\n",
    "    def __init__(self,inchanel,outchannel,act):\n",
    "        super(BasicLinear,self).__init__()\n",
    "        self.linear = nn.Linear(inchanel,outchannel)\n",
    "        self.layernorm = nn.LayerNorm(outchannel)\n",
    "        self.relu = nn.Sigmoid() if act=='sig' else nn.LeakyReLU(0.2)\n",
    "    def forward(self,x):\n",
    "        x = self.layernorm(self.linear(x))\n",
    "        return self.relu(x)\n",
    "    \n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator,self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            # expect input of shape (1,3,128,128)\n",
    "            nn.Conv2d(3, 256, kernel_size=8, stride=2),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.Conv2d(256, 256, kernel_size=8, stride=2),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.Conv2d(256, 3, kernel_size=8, stride=2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            View([1,3*10*10]),\n",
    "            nn.Linear(3*10*10, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.Linear = BasicLinear(16*16*3,1,'sig')\n",
    "        self.loss = nn.BCELoss()\n",
    "        self.optimizer = torch.optim.SGD(self.parameters(),lr=0.001)\n",
    "        self.counter = 0\n",
    "        self.process = []\n",
    "    def forward(self,x):\n",
    "        x = x.permute(0,3,1,2)\n",
    "#         print(x.shape)\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "    def train(self,input,target):\n",
    "        output = self.forward(input)\n",
    "        print(output,target)\n",
    "        loss = self.loss(output,target)\n",
    "        print('loss:',loss.item())\n",
    "        self.counter += 1\n",
    "        if self.counter % 100 == 0:\n",
    "            self.process.append(loss.item())\n",
    "        if self.counter % 1000 == 0:\n",
    "            print('counter:',self.counter)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "    def plot_loss(self):\n",
    "        plt.plot(self.process)\n",
    "        plt.show()\n",
    "        \n",
    "def generate_random(size):\n",
    "    arr = np.random.randn(*size)\n",
    "    return torch.cuda.FloatTensor(arr)\n",
    "    \n",
    "if torch.cuda.is_available():\n",
    "    torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
    "    \n",
    "D = Discriminator().cuda()\n",
    "dataset = DataLoader(dataset,batch_size=1 ,shuffle=True,drop_last=True)\n",
    "for i,img in enumerate(dataset):\n",
    "    D.train(img.cuda(),torch.cuda.FloatTensor([[1.0]]))\n",
    "    D.train(generate_random([1,128,128,3]).cuda(),torch.cuda.FloatTensor([[0.0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "39f22932",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "D.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e0bcf20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0001]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1.]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x25896853088>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "D(generate_random([1,128,128,3]))\n",
    "D(img)\n",
    "plt.imshow(img.cpu().detach().numpy()[0,:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cab250c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 128, 128])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "class View(nn.Module):\n",
    "    def __init__(self,shape):\n",
    "        super(View,self).__init__()\n",
    "        self.shape = shape\n",
    "    def forward(self,x):\n",
    "        return x.reshape(*self.shape)\n",
    "\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self,img_file):\n",
    "        self.img_file = os.listdir(img_file)\n",
    "        self.img_path = img_file\n",
    "    def __len__(self):\n",
    "        return len(self.img_file)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        if index>len(self.img_file):\n",
    "            raise IndexError()\n",
    "        img = plt.imread(self.img_path+'\\\\'+self.img_file[index])\n",
    "        img = self.crop_center(img)\n",
    "        return torch.cuda.FloatTensor(img)/255.\n",
    "    def crop_center(self,img,new_height=128,new_width=128):\n",
    "        height,width,_ = img.shape\n",
    "        dis_height = (height-new_height)//2\n",
    "        dis_width = (width-new_width)//2\n",
    "        return img[dis_height:dis_height+new_height,dis_width:dis_width+new_width,:]\n",
    "    \n",
    "    def plot_img(self,index):\n",
    "        if index>len(self.img_file):\n",
    "            raise IndexError()\n",
    "        img = plt.imread(self.img_path+'\\\\'+self.img_file[index])\n",
    "        img = self.crop_center(img)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "def generate_random(size):\n",
    "    arr = np.random.randn(*size)\n",
    "    return torch.cuda.FloatTensor(arr)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator,self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            # expect input of shape (1,3,128,128)\n",
    "            nn.Conv2d(3, 256, kernel_size=8, stride=2),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.Conv2d(256, 256, kernel_size=8, stride=2),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.Conv2d(256, 3, kernel_size=8, stride=2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            View([1,3*10*10]),\n",
    "            nn.Linear(3*10*10, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.loss = nn.BCELoss()\n",
    "        self.optimizer = torch.optim.SGD(self.parameters(),lr=0.001)\n",
    "        self.counter = 0\n",
    "        self.process = []\n",
    "    def forward(self,x):\n",
    "#         x = x.permute(0,3,1,2)\n",
    "#         print(x.shape)\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "    def train(self,input,target):\n",
    "        output = self.forward(input)\n",
    "#         print(output,target)\n",
    "        loss = self.loss(output,target)\n",
    "#         print('loss:',loss.item())\n",
    "        self.counter += 1\n",
    "        if self.counter % 100 == 0:\n",
    "            self.process.append(loss.item())\n",
    "        if self.counter % 1000 == 0:\n",
    "            print('counter:',self.counter)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "    def plot_loss(self):\n",
    "        plt.plot(self.process)\n",
    "        plt.show()\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator,self).__init__()\n",
    "        self.model = nn.Sequential(nn.Linear(100,3*11*11),\n",
    "                                   nn.LeakyReLU(0.1),\n",
    "                                   View([1,3,11,11]),\n",
    "                                   nn.ConvTranspose2d(3,256,kernel_size=8,stride=2),\n",
    "                                   nn.BatchNorm2d(256),\n",
    "                                   nn.LeakyReLU(0.1),\n",
    "                                   nn.ConvTranspose2d(256,256,kernel_size=8,stride=2),\n",
    "                                   nn.BatchNorm2d(256),\n",
    "                                   nn.LeakyReLU(0.1),\n",
    "                                   nn.ConvTranspose2d(256,3,kernel_size=8,stride=2,padding=1),\n",
    "                                   nn.BatchNorm2d(3),\n",
    "                                   nn.Sigmoid()\n",
    "                                  )\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(),lr=0.01)\n",
    "        self.counter = 0\n",
    "        self.process = []\n",
    "    def forward(self,x):\n",
    "        return self.model(x)\n",
    "    def train(self,D,inputs,targets):\n",
    "        outputs = self.forward(inputs)\n",
    "        outputs = D(outputs)\n",
    "        loss = D.loss(outputs,targets)\n",
    "        \n",
    "        if self.counter%100 == 0:\n",
    "            self.process.append(loss.item())\n",
    "        if self.counter & 1000 == 0:   \n",
    "            print(self.counter)\n",
    "        \n",
    "        self.counter += 1\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def plot_loss(self):\n",
    "        plt.plot(self.process)\n",
    "        plt.show()\n",
    "        \n",
    "G = Generator().cuda()\n",
    "G(generate_random([100])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03f7139f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:27: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:141.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "counter: 1000\n",
      "counter: 2000\n",
      "1024\n",
      "1025\n",
      "1026\n",
      "1027\n",
      "1028\n",
      "1029\n",
      "1030\n",
      "1031\n",
      "1040\n",
      "1041\n",
      "1042\n",
      "1043\n",
      "1044\n",
      "1045\n",
      "1046\n",
      "1047\n",
      "counter: 3000\n",
      "counter: 4000\n",
      "2048\n",
      "2049\n",
      "2050\n",
      "2051\n",
      "2052\n",
      "2053\n",
      "2054\n",
      "2055\n",
      "2064\n",
      "2065\n",
      "2066\n",
      "2067\n",
      "2068\n",
      "2069\n",
      "2070\n",
      "2071\n",
      "counter: 5000\n",
      "counter: 6000\n",
      "3072\n",
      "3073\n",
      "3074\n",
      "3075\n",
      "3076\n",
      "3077\n",
      "3078\n",
      "3079\n",
      "3088\n",
      "3089\n",
      "3090\n",
      "3091\n",
      "3092\n",
      "3093\n",
      "3094\n",
      "3095\n",
      "counter: 7000\n",
      "counter: 8000\n",
      "4096\n",
      "4097\n",
      "4098\n",
      "4099\n",
      "4100\n",
      "4101\n",
      "4102\n",
      "4103\n",
      "4112\n",
      "4113\n",
      "4114\n",
      "4115\n",
      "4116\n",
      "4117\n",
      "4118\n",
      "4119\n",
      "counter: 9000\n",
      "counter: 10000\n",
      "5120\n",
      "5121\n",
      "5122\n",
      "5123\n",
      "5124\n",
      "5125\n",
      "5126\n",
      "5127\n",
      "5136\n",
      "5137\n",
      "5138\n",
      "5139\n",
      "5140\n",
      "5141\n",
      "5142\n",
      "5143\n",
      "counter: 11000\n",
      "counter: 12000\n",
      "6144\n",
      "6145\n",
      "6146\n",
      "6147\n",
      "6148\n",
      "6149\n",
      "6150\n",
      "6151\n",
      "6160\n",
      "6161\n",
      "6162\n",
      "6163\n",
      "6164\n",
      "6165\n",
      "6166\n",
      "6167\n",
      "counter: 13000\n",
      "counter: 14000\n",
      "7168\n",
      "7169\n",
      "7170\n",
      "7171\n",
      "7172\n",
      "7173\n",
      "7174\n",
      "7175\n",
      "7184\n",
      "7185\n",
      "7186\n",
      "7187\n",
      "7188\n",
      "7189\n",
      "7190\n",
      "7191\n",
      "counter: 15000\n",
      "counter: 16000\n",
      "8192\n",
      "8193\n",
      "8194\n",
      "8195\n",
      "8196\n",
      "8197\n",
      "8198\n",
      "8199\n",
      "8208\n",
      "8209\n",
      "8210\n",
      "8211\n",
      "8212\n",
      "8213\n",
      "8214\n",
      "8215\n",
      "counter: 17000\n",
      "counter: 18000\n",
      "9216\n",
      "9217\n",
      "9218\n",
      "9219\n",
      "9220\n",
      "9221\n",
      "9222\n",
      "9223\n",
      "9232\n",
      "9233\n",
      "9234\n",
      "9235\n",
      "9236\n",
      "9237\n",
      "9238\n",
      "9239\n",
      "counter: 19000\n",
      "counter: 20000\n",
      "10240\n",
      "10241\n",
      "10242\n",
      "10243\n",
      "10244\n",
      "10245\n",
      "10246\n",
      "10247\n",
      "10256\n",
      "10257\n",
      "10258\n",
      "10259\n",
      "10260\n",
      "10261\n",
      "10262\n",
      "10263\n",
      "counter: 21000\n",
      "counter: 22000\n",
      "11264\n",
      "11265\n",
      "11266\n",
      "11267\n",
      "11268\n",
      "11269\n",
      "11270\n",
      "11271\n",
      "11280\n",
      "11281\n",
      "11282\n",
      "11283\n",
      "11284\n",
      "11285\n",
      "11286\n",
      "11287\n",
      "counter: 23000\n",
      "counter: 24000\n",
      "12288\n",
      "12289\n",
      "12290\n",
      "12291\n",
      "12292\n",
      "12293\n",
      "12294\n",
      "12295\n",
      "12304\n",
      "12305\n",
      "12306\n",
      "12307\n",
      "12308\n",
      "12309\n",
      "12310\n",
      "12311\n",
      "counter: 25000\n"
     ]
    }
   ],
   "source": [
    "D = Discriminator().cuda()\n",
    "G = Generator().cuda()\n",
    "\n",
    "dataset = Dataset(r'E:\\\\img_align_celeba')\n",
    "dataset = DataLoader(dataset,batch_size=1,shuffle=True,drop_last=True)\n",
    "for i,img in enumerate(dataset):\n",
    "    D.train(img.permute(0,3,1,2).cuda(),torch.cuda.FloatTensor([[1.0]]))\n",
    "    D.train(G(generate_random([100])).detach(),torch.cuda.FloatTensor([[0.0]]))\n",
    "    G.train(D,generate_random([100]),torch.cuda.FloatTensor([[1.0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04534e47",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAABbtklEQVR4nO29aZgcV3k2fD9VvU3PpmVGu6zFlo33BWEb2ywBE2zjYMKHE0jC9pLLcQK5Ql4CH4SEQN68CVk+koABhcWJzRoCxBhj4xi84UW2JVmSJUuydmmkkTT71j3dXVXn+1F1Tp2qruqu3qa7R+e+Ll3q6a7uOrXd9dT93M9ziDEGBQUFBYX2h9bsASgoKCgo1AeK0BUUFBTmCRShKygoKMwTKEJXUFBQmCdQhK6goKAwTxBr1or7+vrY2rVrm7V6BQUFhbbE1q1bhxlj/UGfNY3Q165diy1btjRr9QoKCgptCSI6GvaZklwUFBQU5gkiEzoR6UT0IhE9EPAZEdEXiegAEe0koqvqO0wFBQUFhXKoJEL/EwB7Qj67GcAG598dAL5a47gUFBQUFCpEJEInolUA3gbgGyGL3AbgXmZjM4AFRLS8TmNUUFBQUIiAqBH6vwD4BAAr5POVAI5Lfw8473lARHcQ0RYi2jI0NFTJOBUUFBQUyqAsoRPRrQDOMMa2llos4L2irl+Msa8xxjYyxjb29we6bhQUFBQUqkSUCP16AG8noiMAvg/gTUT0bd8yAwBWS3+vAnCyLiNUUFBQUIiEsoTOGPsUY2wVY2wtgHcDeJQx9nu+xe4H8D7H7XItgAnG2GD9h6swl3hpYAI7B8abPQwFBYWIqLqwiIjuBADG2CYADwK4BcABABkAH6zL6BSais//fA8sC/jeHdc2eygKCgoRUBGhM8YeB/C483qT9D4D8OF6Dkyh+SgYDKw4FaKgoNCiUJWiCqFgYFATWikotA+a1stFofVhMUBNUaig0D5QhK4QCoupCF1BoZ2gCF0hFJYicwWFtoIidIVwMJUSVVBoJyhCVwiFxaBcLgoKbQRF6AqhsJSArqDQVlCErhAKxgIa8igoKLQsFKErhEJF6AoK7QVF6AqhUHyuoNBeUISuEApVKaqg0F5QhK4QClUpqqDQXlCErhAKpaErKLQXFKErhELxuYJCe0ERukIomKoUVVBoKyhCVwiF6uWioNBeiDJJdIqInieiHUS0m4g+F7DMG4logoi2O/8+05jhKswlLMaUjq6g0EaIEqHnALyJMTZNRHEATxHRQ4yxzb7lfsUYu7X+Q1RoFhSXKyi0F8oSujO93LTzZ9z5py71swDKsqig0F6INAUdEelEtB3AGQCPMMaeC1jstY4s8xARXRzyO3cQ0RYi2jI0NFT9qBXmBBZTOrqCQjshEqEzxkzG2BUAVgG4mogu8S2yDcAaxtjlAL4E4L6Q3/kaY2wjY2xjf39/9aNWmBMwKA1dQaGdUNEk0YyxcQCPA7jJ9/4kY2zaef0ggDgR9dVpjApNgqW6LSootBWiuFz6iWiB87oDwI0A9vqWWUZE5Ly+2vndkbqPVmFOwRhTOrqCQhshistlOYB7iEiHTdQ/YIw9QER3AgBjbBOAdwH4QyIyAGQBvJspJmh7KP1cQaG9EMXlshPAlQHvb5Je3wXgrvoOTaHZUPdkBYX2gqoUVQiFitAVFNoLitAVQqEcLgoK7QVF6ArhUHyuoNBWUISuEAoVoSsotBcUoSuEQmnoCgrtBUXoCqFQ3dAVFNoLitAVQqEidAWF9oIidIVQKB+6gkJ7QRG6QigsBlCzB6GgoBAZitAVQqHmFFVQaC8oQlcIhdLQFRTaCxW1z1U4eyDr50pLV1BoDyhCVwiEzOGKzxUU2gOK0BUCIVeJqopRBYX2gCJ0hUDI+rmicwWF9oAidIVAyP4WFaErKLQHokxBlyKi54loBxHtJqLPBSxDRPRFIjpARDuJ6KrGDFdhrqA0dAWF9kMU22IOwJsYY9NEFAfwFBE9xBjbLC1zM4ANzr9rAHzV+V+hTWF5XC5NHIiCgkJklI3QmY1p58+4889/id8G4F5n2c0AFhDR8voOVWEuIWvoSnJRUGgPRNLQiUgnou0AzgB4hDH2nG+RlQCOS38POO8ptCmYcrkoKLQdIhE6Y8xkjF0BYBWAq4noEt8iQS0/iliAiO4goi1EtGVoaKjiwSrMHZTLRUGh/VCRy4UxNg7gcQA3+T4aALBa+nsVgJMB3/8aY2wjY2xjf39/ZSNVmFN4KkWtJg5EQUEhMqK4XPqJaIHzugPAjQD2+ha7H8D7HLfLtQAmGGOD9R6swtyBKQ1dQaHtEMXlshzAPUSkw74B/IAx9gAR3QkAjLFNAB4EcAuAAwAyAD7YoPG2HfKGBY2AmN5eln+Py6WJ41BQUIiOsoTOGNsJ4MqA9zdJrxmAD9d3aPMD7/3mc7hsVS8+/baLmj2UiqBcLgoK7QfVPrfBODU5i/6JZLOHUTHkSlHF5woK7YH20gHaEKbF2pIQvZWibbgBCgpnIRShNxiMtadk4e222MSBKCgoRIYi9AbDYgxmGzKi14fefuNXUDgboQi9wbAYa8sIl6kIXUGh7aAIvcGwWHUa9OhMHv/6i/2wmsSmHh+6YnQFhbaAIvQGgzEGswpCf3zfGfzzL17B0dFMA0ZVHu2o+ysonO1QhN5gWKw6yYLr7s0iVlUpqqDQflCE3mBYjFUlufCvNMsyqFwuCgrtB0XoDYZpsaoiXP6dZpGpx+WiInQFhbaAIvQGgzFUZVvkunvzLI8qQldQaDcoQm8wqrUt8u80S79WEbqCQvtBEXqDUb2Gzpz/6z2iaFDdFhUU2g+K0BsMq1rJpckuF0ua1EK5XBQU2gOK0BsMVqPk0iwNXS73tyx7O/71F/sxPJ1ryngUFBTKQxF6g1FtpShrssvF020RDANjWfzzL17Bo3vPNGdACgoKZaEIvcGwqqwU5ZF5K/jQZaeOSpAqKLQuoswpupqIHiOiPUS0m4j+JGCZNxLRBBFtd/59pjHDbS8wZvdCt6qYZLnpkouvUtQSNsqmDEdBQSECosxYZAD4GGNsGxF1A9hKRI8wxl72Lfcrxtit9R9i+6IW62HzC4u8Ebo7HhWhKyi0KspG6IyxQcbYNuf1FIA9AFY2emDzAVYN1kPXtth8H7rspVeSi4JC66IiDZ2I1sKeMPq5gI9fS0Q7iOghIro45Pt3ENEWItoyNDRU+WjbDEKmqEpD579RzxFVAm+lqCu5KEJXUGhVRCZ0IuoC8CMAH2WMTfo+3gZgDWPscgBfAnBf0G8wxr7GGNvIGNvY399f5ZDbB6wOkks1N4N6wMvbTPLFN2U4CgoKERCJ0IkoDpvMv8MY+7H/c8bYJGNs2nn9IIA4EfXVdaRtiHpILs0rLPJG6LXcnBQUFOYGUVwuBOCbAPYwxr4QsswyZzkQ0dXO747Uc6DtiFqcKmaTNXR5rZbFml65qqCgUB5RXC7XA3gvgJeIaLvz3p8DOAcAGGObALwLwB8SkQEgC+DdTGXPanKGCIdMk2yC/l4uyraooND6KEvojLGnAFCZZe4CcFe9BjVfYFnVSy7N1tCLfejuawUFhdaEqhRtIGqRXKwWqxQVTxsqK6qg0LJQhN5A1EVyaYVeLswlcsXnCgqtC0XoDUQt1Z7Nrsz0zimqJBcFhXaAIvQGopaJnlkNck09ENbLRRG6gkLrQhF6A1FbpWhzZyyS+6HLLhdF6AoKrQtF6A2Eaz2spTlX82cskifpULZFBYXWRRQfukKVqM22aP/fLMnFo6FbAEH1Q1dQaHWoCL2BqEVyqeVmICObN/G5n+7GTM6o6HvM91o151JQaH0oQm8g6tMPvTYC3X58HP/+9BFsPz5e0feYz+WimnMpKLQ+lOTSQNRmW/T+Xy04EVcaWcuLM8bAnGJhlRRVUGhdKEJvIETHxBqSorWW/htOdrPS3/FXinIRRhG6gkLrQhF6A1EPyaXWJKSQSiq8qXh96K6NURG6gkLr4qzQ0I+PZpqy3rpILjVqLtVq32GVosq2qKDQupj3hL775ARe9w+PYc+gf5KlxsPv5a7ou3WaJLpaDd3TywXNn+NUQUGhPOY9oY/O5AEAY87/cwk5yq04KVmnCSWMKn/HUykquVyUbVFBoXUx7wmdE1oz+op7ZYvqvlsroVftcpGeLrzNuWoajoKCQgMRZQq61UT0GBHtIaLdRPQnAcsQEX2RiA4Q0U4iuqoxw60cVhMjS3mVlRJzvW2Lla/f63JpdisCBQWF8ojicjEAfIwxto2IugFsJaJHGGMvS8vcDGCD8+8aAF91/m86mikV+BOLFX23TuOultDlpS1WPwlIQUGhcSgboTPGBhlj25zXUwD2AFjpW+w2APcyG5sBLCCi5XUfbRVoJqGzOkgutSYhheRUoTvFXymqJBcFhdZHRRo6Ea0FcCWA53wfrQRwXPp7AMWkDyK6g4i2ENGWoaGhCodaHcwmSgWtIbnYTF6p/dGzOJP2o2J0BYWWRWRCJ6IuAD8C8FHGmN8DGDSJdNGVzxj7GmNsI2NsY39/f2UjrRJmlRFqPSCTX+WEWuekaA0ausWYW/WqJBcFhZZFJEInojhsMv8OY+zHAYsMAFgt/b0KwMnah1c7qiU0ANhyZBTbjo1VvW5vhF7pd+sTEVdtW/SNvZnJZQUFhWiI4nIhAN8EsIcx9oWQxe4H8D7H7XItgAnG2GAdx1k1qi19B4B/+Pk+fOF/Xql63X4duhJw22DdXC4VFxZJLhcwmEpDnzNsOzaGux7d3+xhKLQhokTo1wN4L4A3EdF2598tRHQnEd3pLPMggEMADgD4OoA/asxwK0ctSdG8aSFfg1Zj1kLodZI4jCq33/90oSSXucODOwfxxUcPNHsYCm2IsrZFxthTCNbI5WUYgA/Xa1D1BCfVagjdYqwmycNDihXeF+pe+l/h73gidDVJ9JzCsGo77xTOXsz7SlGrBg3dtFhNFaY1+dCFxFEnH3pN/dDdpLLS0BuPWs87hbMX857Qq5Uc+HdqIbCaNPQ6JUXrUSlqSRG64pnGw7AsuzpX3TwVKsS8J/RqCY1/txZC9/RDqVRyqdOUb/XoZaMqRecWhln7MVM4OzHvCb2WyY1NViOht4TkUm1hkV9D57+nSKZWvHxyElOzhdDPVWdLhWox7wm9FsnFslhNhFpbpWi9Cou8/0dfv/tabs6lgsbawBjD//PVZ/DtzcdCl6nlnFU4uzHvCb0WqcBkTFxc1aAWDb0Wqcj7O7XPKSpr6EoGqA1500K2YGI6Fx6hVzsPrILCvCd0TsjVELNp1mYf8/rQK/su/2q9NPRKm3z5ZyxStsX6IG84ZF3iiUlo6JV6TRXOesx7QhcRerUaerMllzq5XCqfgs4foaMu4znbUTDL3xhraVehcHZj3hO6W1hUxXetyt0pMjykWHVhUZ0IvcqkLACPhU7xeW3gEbpRIvo2aghCFM5uzHtCr8W2Z7WEy6Xq1QOopZeL/FpVitYLnNBL7UeuodeSv1E4OzHvCb0mycWqLSnqn5ezou/WvZdLZd/zz4eqbIv1Qd40AbikHQShoat93RKYLZi455kjbfHENO8JXdj2qiwsqs22WH1StNmVot5eLu7vqAC9NuQiJEWVD7218OzBEfzV/bvx0omJZg+lLM4CQq+usMb+bq2l/97fqgR1b59bxZyiRHwM7gQXKlFXG1yXS4kIvcWSoi+fnMRln30YZyZnmz2UpoDfhGcLZpNHUh7zhtAZY3hpoPgOyi+KqmyLNXdblKPcZksula9fJwKRPfZmTuU3n1AQckr4MkYNQUgjcHRkBpOzBgYnzk5C59dOLa205wrzhtA3HxrFb9z1FF45PeV5v5bHV6vGrne1+NDrVZkpnlCqSMpqRCBwH7rzfouQTLsiUoRuVh+ENAKcyFplPHMNfoMtKEKfO0w6vTFGpvOe92tqzlVjpaj81UpvKPVqV1tLhA6ySd07p2hNwznrwZOipWqGWk1DP9uTtCJCN+YBoRPR3UR0hoh2hXz+RiKakGYz+kz9h1kefKdnC4bvfe/nUWFZrOYWpv5JIqr5bt36oVf6MwzQBKG3Hsm0K4RtscR+bLVeLgURobc+oTUC/Hjk2oDQy85YBOA/ANwF4N4Sy/yKMXZrXUZUJfhOz+a9O71aycGsQxJQvmirdrk0yYduMWaTOTk3Nufrld6YFLzgpFDStthivVwK4hxq8kCahHkVoTPGngQwOgdjqQmcuLO+TDR/tK1c8nA17GpJrLbS/+q+50ctlaIEO0r3FhbVNJyzHlF6ufAeLq2SryhEuAnNZ/Bg8WxKir6WiHYQ0UNEdHHYQkR0BxFtIaItQ0NDdVq1Da7zZfNeyUVMQVdDYU21j76e36iUUOvUbbHax3fmJEW5hl7LVH4KLjgpRLEttkoSUjwxtMh45hqmc8wK8yFCj4BtANYwxi4H8CUA94UtyBj7GmNsI2NsY39/fx1W7cLV0L0RunsyVnYw5JO32gvLXz5fCeouuVSxfiI7SleSS/3ASaFUUrTVerkUWsx1M9c4qyJ0xtgkY2zaef0ggDgR9dU8sgrBd3om75NcRKVoZb8nE3q1UbKnUrTKCSZqJVBXQ6/se4wxaBqJpGgtMz8puIgUoZstpqGbrRuhz+QMjM3kyy9YA+aVhl4ORLSMyK4pJKKrnd8cqfV3K0VYhF5tCb188lZ7Ips1SC5mnQi0Wg2dwY7OibwTXLTgNd1WcDX08B1ptpjkUmhhH/rfPrgHH/yPFxq6DqONCL2sy4WIvgfgjQD6iGgAwF8BiAMAY2wTgHcB+EMiMgBkAbybNeG53HW5+CWX6ojRrCG65qhFcqmXbbHaqkPX5eL8zVsRtOBF3U5wbYvhy7Sa5OL60FuP0IanczjV4ApWzh25NpBcyhI6Y+w9ZT6/C7atsakQLhcfoVebzJPP3WoffWuzLVb3PT9qcrkQQSM7Olel//VBLoKnu9WSokJDb8EZlAyTIeMzQtR9Hc5xKBitt/1+zJtKUSNEcqk2QpUJsFq7Vm2Votw2WaPkUqV0w5ykqL9StFV03XZFvkxSlEk9+FslQm9lDd2wWFHerN7gwSKv8m1lzBtCN4Vt0R+hO59XqmFLV1y1T5rVTnAhk3jNGrrJbwyVfc+2LcJpzlW/J4azHeV6uXhyNy1y84wy4can//sl3PPMkTkakQvDsmBYrKH6djtp6POG0MMi9Goj1FoSmhz+nuJR4S1IqmrVArX0ctGIQL7Sf2VbrA1utBv8uUyarRIR543y59CT+4fw/JG5rz/kclAjZRceFClCn0Pwk83/+FV1UlTWv6suLJJfR/+NWtru+lHvStFWIZl2RbkIvRUJPUphkWEyYbecS/B1NlJ2ERp6C+YQ/Jg3hM53ur8JvVVDhOr/7UpRbbWp1wMPfPT7L+JnOwerGoNIZlZRKUpEIJDT/sAdTzlMZAr44++9iIlModLhznvky+jRstTXOknR8oReMFlTCM+tP2lchM5vaO3QnGveEDqPeIoLi6pzZ9Tbh17J6v0zHT28+zSeO1ydtV/0BanCNqlpdoTunyy73M1h54lx/HTHSWwfGK94vPMd5XzocgK+dZKi5V03pmU1pV+4ISSXxkXoaoKLJiBUQ6+H5FK1hu6+rlZysRiDYVnIFao7mdzpzCr7nl9DryTByy+y6dnG2snaEWJO0ZB96JFcWiRfUYhU3coaamv8+a5T+MELx4vX64xpJtdAyUVo6MrlMmfg0cysP0Jn1RFaPSJ0q8qL0/SRZ8FkyFV5MlVrgWNwK0UZmPepocy28EhmalZJLn6UKyxqSQ09QoResKyGdmP8zxeO4e6nD4eOzT8PQj3RTqX/UfqhtwWEllYwHQ+1XeJYLaGZVerfgK0hf+6nuxHTSbxXyU8w6bwR0UGVj3vVunwsxrst2k8alchHIkLPqQjdj3yZwiJZQ28VQi+n+wP2MW+khl4wg2cPK8xFhK6SonMPWVopBFwUlSaYrBoipRePj+HHL57ASycmxXuVuFVkSYM/olcjuXiKVKrttujr5RLltwoiQleE7kd526L7QasQulGmlwtjNtk2MkIvmFagi8acC9tiG0Xo84bQ5ZNNLi6qOkKvQcssBGhuFblcggi9ipOppjwA46X/VDQVX7ltUYQejna0LfLzOdSZw4OmBkawhhX8BMBnU2qsbZFXiipCnzPIJ5ucGK3Wh+1tzlX83ZdPTmLTEwcDv8sJLW9aiGm27FLJtSmTL78pVKOh+8nBCIlywsageSJ0+bPS3y0IyUVp6H6UdbmY1QcSjYLothhC2K4k0eAIPeAmOBc+dBWhNwFGGKE3qH3uAztP4vMP7Q2UUgShGxZ0h9ArkVzkRXlUUHuEDnzwP17A/3ng5UjfFS4X8NL/0jc4GfzCUxF6McoRuue8axHNtpzLZS6aiRVCXDTGHEgu822S6LaAfLLJB7fabovlJBe52EJOfgLugc8bdoSeQ/W2xVo0dH9i98RYNrIf3pIlF7CK5BtOWiopWgyRYAzZhwVZQ2+RCL0cYRtlIvi6jMEM9rnPRVLUnIMnkHph/kTo0skkV4vW0suEI+i7pYotClJUHdM15zeir1teHx9GNfqd3zFRsCxMRrQSMsZL/wmWhYpsi3yf1BqhZ/Mmfvcbm/HK6amafqeVkCtjW6yHXbbe4NPmhd3I+bXQeMmleP1iYptGauiql8vcQz75ZT1NTLZcseQS/Nsc/OQNjBp8ETpQabdF+/+4FPnnCrVp6IwxFAyGyWxUQrcrRYNcLuU2hW9/rT70kxNZPH1gBNuOjtX0O60ETgphjhCjBW2LPPEYrqE3fkajIMmFMTdROjMXLpf5EKET0d1EdIaIdoV8TkT0RSI6QEQ7ieiq+g+zPAyLCfKU79bV6nvlJAZDRCWlo3cux1RjW4xp7uGpVUM3nYrTyYhRs1wpylCZL58TQK2SCye/WhJe2byJv/rJrpYpcuIBgMWCz4lWbJ9brpeLMQcRumFZyJtWaGvphkboUnOyVrnJhiFKhP4fAG4q8fnNADY4/+4A8NXah1U5TIuhO2WnBAKTojX1cin+3M38F38o38k5KVcjufAbFFAloTvbHNcJpmUT5GS2EOnmwuDttlhJG4N62Rb57/jbOVSCnQPjuOfZo9hypPlRvmFasBiQEDKcvR8nsgX8+j8/gb2nJr0aeoskRctVihplIvh6IMg6KY9nLiJ0exyVXYdff/JQYMuCRqEsoTPGngRQqtHxbQDuZTY2A1hARMvrNcCoMCwLXZzQAySXmvqhB1aohT+GyVqbXoXkwleny5JLFbZFTghxXRMtBAyLRSJInhS1JZdgXT8MspWsloimIH6n+os1b9Ye5dcLfCwdCR2Ae44NjGXwyulp7Bmc9OY92iZCLy0j1XMMRgi5NjZCLzYpRMWPtg3gpztP1ntIoaiHhr4SgHwLGnDeKwIR3UFEW4hoy9DQUB1W7cK0GDoTNqHXJSlaJjnFdeKgqEQ+0XSNnFl/KtHQiyWXgln54x6/wBIxzfahO39PZssTJHN86HZhEROVo0CUwiL381pkFz6xQi1k7Mo2zXfc8LGk4t4IPS85mVqzsKg0YRck+bFRE6AEyTqmJ0JvvMsFqDwxmjcszMyh26sehE4B7wUeVcbY1xhjGxljG/v7++uwaheGxUTkw0+wWiZpLqdluomggKSodNLxeTmrac4V99khKz2Z+DbEdQ2WVGk3ESExyhg83RYZA+LODSaq5ALUlhgVkksdCL0W2aZe4GPpiDsRus/fPFswfVJf8wndtNyiskje+QaNOai4SQ4cMg0kTfkmW2liNGdYc/p0WA9CHwCwWvp7FYC5e8ZwYFpMXChBXt9aJJcgh0xeWJnCk6KATYq6Q4pRwe8RulYjoTvbkNA1Mds8gEjWRdHLBW4/9KjykUzotUTohTrIJa0kuXDi7nCeJIsidMNbDVlt2+Z6Qj6WYRq6rPs3wunCe8UUr8t5AtU1ZBp4w64pQjfbj9DvB/A+x+1yLYAJxlh10+vUAMN0CT1I86upsKiU5BIQocs6m1w+HxWWiNC9h6dSHZ1HM8mY5jkRo1gXLadjJb+nWMx17JS7ZuUoqpbEaD0IPVcHp0y9wLenI0RymS1Ynn3XyCRjVESRgLxRc/11dHkMQfunpyOOTEO7LVpIxuxjVun25Q1rTuW+KLbF7wF4FsAFRDRARB8iojuJ6E5nkQcBHAJwAMDXAfxRw0ZbAqbFENc1xDQqKq9O6FqkCD2bN/HY3jOe7wKlJZdAH7opE7rb4Coq+LL+CL3ShIwsuciIFqFDaOi8l0vc584Ig/xYWsskF/wpqJZe165s0wIaujOWNI/QmTexnjNcyYWoNZKiBemcK+dDL7VMTWMwg3+fv9/TEUM+pJK0HjBNhrQj51anoc9dMFG29J8x9p4ynzMAH67biKqEYVnQdUJc19wInUsOMS3So/+DLw3iY/+1A5s/9eaylaJ5KRHkh1dDJ+gaVdVtMVZE6BVG6JzQY97fiZIU5d0WiRgsy4nQI0ouhmkva1gMU7VILnWIrltRQ0/5NHRZcuGSQjKmtcQUdB4bZZQIvQFOF/kaywckRXtScQD2MfYHL/WA4ci5YyhUHFTlTUv41/0BWiMwrypFYxohEdNcz6qw7TlEVOYC4aQ/kzfKSi6l+ld4XS7lJRfDtPCNXx0S2XBRWORLis5W2M/FkjR0GVElF7tS1O7lYjEmLpZy12zBtLAgnQDQOknRVpBc+HbweoliycWN0JMxvSUmiZbJNLS6tcERuuHR8d3XfGxdSXt/Nqo0XzZchK3j8PAMzkzOet6TC5GyBRPZvImPfHcbBieyDRknMI8I3XDugHFdK5q3MeHoX+UeYXkEnDesspWipUr/5URpFMnl25uP4m9+tgf3PnsUQLBt0R5fZScsv7j49nNEcbnwSlHN8aFbFSRF86aFhWk7aqpFcqlLUtSo/aZQL/DEHScg1+XCWyRbnrxHKyRFZTIN4+pCg3X/sN/n5M7lkEquj5mcgfd+8zkcGZ4pu6xhWkImC3O5fPg72/D5n+/1vCeTfyZn4ODQNB7YOYgXGljkNm8IXUToOhUlRaNqv7yjoZ/QgyN0LrmUrhSNIrm84PQq6e2IO+uz3/dLLtXaFhMx3fN+FA2dwZFcwH3okJKi5SWXzjpETVzWaiWXy3++cAwvHqvuguQ3la6Un9CLI/RETGuJpKjX9x18LD2VlA2RXKyQ194IvZJ+R0dHMvjV/mHsGBgvu6zsoCuEnM8T2QJGZ/Ke9+RzfyZveo5zozBvCN2O0DXEY1oRoXPJoRwRiba3plXW8pgXEXq4AwbgicXgdZsWw0zOwO4TEwBQpFEXt+WtVEPntq7KNXSLOaX/mtsPvRIfekdcB1FtDY3c6LqW4qT6Ruh///N9+P7z1ZVyC0L3Rehyz/uCVAzWChF6lOg7LGlZL3hcLh7Hi71eHjxUEqGLp6IIMqZHcgk5n4P85jnT/XsmZ0gFZI0j9HnUD51H6AGEHosYoUuSS7lKUbe/RXmXC4X40P/mZy/j358+UrT+cNvi3Llc5EpR7kOPalssWAzppIaErtUUoQvJxTfxdyUQtsU6zQqfzZsV31h3nZjAwFhGSC5CQ+cuFzlCF5KL3hKFRfwYJGPhTrFG2xbDInQ+nnSycslFkGuE8ZpWeZdLzjCLggaP5JI3xc2gkQn6+ROhm5bQ0Itsi5EJXZZc3PdLTXARdIC9hO5E6AHrfvrAcOD6621blDV0XaOIhG6TOcA7A7pPEGVL/w0LCd1OUNcyywvfj4xVP1tMPSUXxhhmDbPisdz99GF85ie7xZNGUYTuKSxyNfRWSoqm4uE3GKPBhUWhtkVnXbzlRyWRby5itMyLmsolRXMBfnP5PJnJG9KNuzHJW2AeETqP0OMxTWivpi/Sjaqh5wyrbKVoULMg8TvSgSRRKVq83HlLugK/53Zb9EXoFd7Z/ZITACxMxyMnRXnbAgZeWBRNcjEsCzFNQzKm10To8uNttYTM5a9SksuXHzuAb/zqUKTxMOa9qE2L4aPffxG7T06Efm+2YGI6ZyCbt79XnBSVzjsn+axrwefMXMMthgp33QTJIPVEUCJUXlc1kgs/huUkQb5pXEMPiugZY8gbVukIPWd6nsQahXlD6IbFoOuEpK65M6z4CK2cy4UfXNs7Kl+0xcuKnudlC4sQKrnkChYuXN6Dx//sjc7ffsmltgjd9aG7h7mnIy6IpRQsBlEpyhiDKfvQy9oWGeIxrahCtVIUJLdQtdV2USL0h3YN4qc7ynermJVu+Bwj0znct/0knj04UvJ7mbyJmbyBREwTx6O4OZeJgmUJQm+FpKghIvSokktjI3TZQeYmRSuXXKJO7chvIN2O13024DxyZb1wQp/JG8g7mrqK0CPAjdBJXMR+Qtt2dAwHh6ZDf8NrW5T7VhcfAH6g80FJUV8vFzuxGJxY7YhrWNvXiWTM7bdSd8lFitC7k7FIGjDX0Ml5umDMfdKJkhSNOzUBtSRF69EeNUph0WzBwpmpXNnf4pGVvP/4jaLUseHfG57KIZ3QoZNXupIlF9NkiGvhT3VzDX4MUnG9RLdFWXJphIYeEqFb3gi9kuCBk2u585MfI573CAoMwtpLeJ4wpaTobBWtsKNiXhA617l0TfNWigpCsy+gj/1gB/7lF/tDf0e2LfLCGi2gBFue+qpchK47F2fQ00GuYCHpWAqTMU2sPyyZWb1t0f2dzmQs0o2Ba+gaSTbKiLbFgmkhrttJ0Voy+oU6SC7i0dpnRZUxWzAxNJUrW3jmEnrxuEptJ7+ZDE3n0BHXi3IR/MKfLZiiniKmV1Zd3CjIhB6uoZd3wtSCsMIlPh5Xcol+juQjR+huTiMZ0wKfFMPqV/y2RSW5RATfh7EySdGZvFmyN7GbFLX9wDoRYppWJLmENdkPeo+cwqJAycUwkXQaNSXjrt7sTkFnX/i8MVCl7oqgCL0rGUPesMr2rRbdFonETSsW0bZomLYjJhmvLUKvh4bu/Y3gY8+JdLxMbmFWuuFzZANIPux7Q1M5dCR0aH5C93VbjOma3XK5JQjdHkMpDb2WGX2ijSHY5cLHJpKiVUguebP0ecVdR7pGSCf04Ai9EHyOeV0uhvKhRwW/g+u+x3x/UhQovTPFndZ0k1OaVkxgQSeV93f8GnowCeYMS5BtQtck26L9OY+IEzENcZ2q1tDlCL0rYgKJd1skyE8M0TT0vBSh12ZbdPdZtQ265O0Mk2044Z6Zmg383F2umLyzAZLLyHQO77v7efF7PHofnrYlFxGhM54U5dqq6UmKtkJzLn5tpeJa6JR4UVrs1gKv5FKcgBWVohUQZaURekwjpBOxwKnuws4xj8slZ0pPYkpDLwl5Dk7Zh+5PigLlCN37eC7kEstP6MGanvu5TOhOhB7ihkk62fNkXCuK0LmGHtcdx0iFJwLX/uUbGq9SLEfoTBq76RtPOaIxTIa4Y1ssR+jPHhzB9Z9/NLB5WsGwRPveWiWXUr/Bz4kzk6V1dEHo0jnEIzL56enlwUk8+coQNh8a9XxvZCaPjnhxhC67XAyuobdIUlRE6Ak99Lg3vH2uWUzigGtbjBqkyJCLCEuBHyNd05BO6IFBQVBOxf/bGY9tURUWlYRhuYQT10m4I4Ii1FJ3R0/pP+MRehChl47QvUlRhFrQcgVTyCkyYbuSi+b8T6CYVrXkIrtlujx6Yzz0u4y5laL8guI3hnJyjdDQY1rZfuhbj47ixHgWpyZmi2ycBdNCd8q2WVZK6N9//hgOnJkuS+gF0/V+l0uMzkrEy8ElF/m84u1Sjzp9Qvj3GLMntyjS0CWJMGfYXUNbLinqNAvbdmwMS7qTWLUwLZZptIYedr3VVilamcslVkpyCTnH+HGN66Q09ErAHwXdboveCN0juZQgRVEObNqVohoRYgGEXioikTusAbwFbbCGnjctidCLJRdOxHaEXnmRjpzQ4RARepkT2eIuF5DnpLa3Mfx7PEEd06PZFgcnbFkiKLeRNy0scJp8Vepy+dlLg3hg5yAKpoVO55E8SLaRL67aJJfiqP3ISKZoHem4Lgq2/ElRwN4PMU2DXiIpmjcs/M7XN2NblT1lKgEnzVTCTop+5Dvb8OXHDgQuAzTK5RL8+3z/pOIadI0qCnhkebUUxNO/bksugUnRgnyDL9bQezsSyORkDV1JLiUhInRd8yZFWXGEXorIAiWXAC0zLEkT9HepStEil4uI5NzHPABOglGvWI/m0QyXdYDoj6funKLekxoonRTl67QrRfWyFwwn9CDJJW9YomFZuQj91MQs3vmVp3Hbl5/GtmNjODGWtftnSK18g35DvriiSi5BUX+u4CVlADg6MmNXl0qE3pHQXenKF6ED9n4Ik/rEOKdm8czBEWw72nhC57bclNOKYCJbKOoF5DUJNMLlEvz7BRHIaR6XWBTI7qco6y6ZFA2TXJz3F6bj3ghd2RZLw6+h+33oVWnoXHIJ0L9lkvI/YvoJTKPwqj9bQ3ddLn53Do+IYxp5IvioyOYNELlJI8AvuYTDnYKOpMRQeZeLiOYjJkVPjtu9oQM1dNMS/t9yDbpeOjGBbcfGseP4OB7edQoD41nM5A3kCqVvCvL5MFROcim4UR0/J4JcLnwG+iMjGRRM5nk66whIivoLUGJlkqL8qWAuWgLz6JufpzN5syhKLZgWeJudRlSKFnxPMHc/dRiGabnuK92+PipxVLnOojIuF+ncTydjlUkuzngWphNeDb2Bxy0SoRPRTUS0j4gOENEnAz5/IxFNENF2599n6j/UcMgul3hQUjQWjdD9nmURoZeUXHzRu4/ANC1YcrEshrzpulzkCN3vcuF6dKWSSyZvenzPgKs3lnvsYwyObVGStKJE6IYrc0UZs4jQA7T2gsmQjOnoiAdHRjL4pB2JmIbnDo86tQR2W9NSsk1lkou7LW4FanFSlM9APzydw8iM9yaRDkmKiuRvzkRMd867kGiX3zDmYhYmufTfv34Oub1so10uT7wyhL9+4GVsPz4ukqJcag2L0LcfHy8KynJRI3TZthjXAyUXj401QENfkLbnPBUulxqcX+UQZU5RHcCXAdwM4CIA7yGiiwIW/RVj7Arn31/XeZwlYfoOrMXsSCEwKVpiZxbZFrnLo2RS1C+5eJ8K3EkigiN5EaF7NHSv9q/zCL1C7S1bsAmda7ZAZRE677YoKm4jzFhUkFr2lnuqyOQN0VcmyA5mJ1cJnUm97KzuvOHYlasXYKfU4zpbMAWhf/Q/t+Pfnjjo+R4n6Y64Xj4pKo2BHwveRsGTFJUu6ldOeyuTPbZFSXLhx8WWXLTQYjTAvYnMxSxMQraTriH/jbHgmaC9AZKLdI2NZ+zjPJ0zYJiWbRggcvoGFe+PbcfG8I4vP42nfI3wZO9/Kcjckk5WlxRdkI77mnM1N0K/GsABxtghxlgewPcB3NawEVUBr8uFz87NApOipsUCHwtNy63+dCtFnYq9Ehq6PwnEP+MtPeUWtDI4IXANXY4wLGl7AFu+SMX1iiOybN60C1kcQtcIomtcZA0dxRJQKdtiQTwGl0+K8ugcQKAbhvvZO5OxkgVhgNvj/YrVC4qehno7EuL1P/3PPs9nXM9ctbADI9PeCQr8kEmbkwdPggUlRQFg36lJz2+kpOPBz9u8aaHHkYXspCg5SdHgcfAZ7htJDBy8ViImXUP+m69hWWKe1EZKLomYhvGMfYwyedezDyDUNLDVmR3IP+2bXN1ZCuLpX3c1dL/LK+zY50wLiZh9/mZy3gkuyjnFqkUUQl8JQO7oP+C858driWgHET1ERBcH/RAR3UFEW4hoy9DQUBXDDYasc3FniDxJhX+Ch6AoXT6wQnKh8j50uVkQXy9gP1oDjoZOVBTV8ub3HtuiX3LhPnQnIeOPjD7145dw34sniraFg0foXj+7U3UaweUiKkV9LpdSJ6NsceRFXmHLD467hB5E2AVHkkonIhD6bAGdCR3nL+0u+oxH6ADwqmU9ns84KS7rTWE6551LdnAiixPjLhHIySx//w5/EUlfl30TKYrQ47orXUkROm/+NJO3raz2eRd8jGbmMELP5A10Jr2ynf88NEwmnjQLDZRcUjFNPP3M5Ay7CRyXLOPBhM5nJBqd8VYBVxWhJ2IwHalUhsfl4ovQk7qGTqcgiX/PYo15kgGiEXrQrAL+0WwDsIYxdjmALwG4L+iHGGNfY4xtZIxt7O/vr2igpSDrXJyw5L4K/jk1gyIb+S5rSy4QPnR/dF3KpsVPFB4Jcx06PEIvL7nEdBInhYwHdpzEk/vDb4yZvIm0VGrOC5T82xsEi7ndFl1NnzcrC/8eP2njup2gZixcVz0pRU3BhUX2BduV1IW3OwyT2QJ6OuJY399Z9Fk6ruOJj78Rt1y6rKgXPL8A+7uT9jikJ4Wb//VXuP7zj4rzyCO5+AndV/69pDuFuE44NeHV5dOJmNucS0qK9qTckpBVCztKTluYnUMNfSZnojMZ8zSK899cDctCXNMQ0yhyhH7gzBQOR5jPk/8+l1M5sgXTaZPAI/RgycUldK+clotI6H6XC+A+Ifl/C/B2XMwbdoSeTuqwmHdy9kY5XaIQ+gCA1dLfqwB4eo0yxiYZY9PO6wcBxImor26jLAP5LupKLi6h+5tcBRO65Xlt8jamARE6Jy2iYpeLkFyc/hKiUrToMY1r6NEqRf36HWMMM3mj5CTM2YKJVNzt7hfXCak47wtT7sLj3Rbdd6IkReUIPVlmXZzslnQnQ10u8RjZkksZl8vkbAE9qTjW99vFSTIBJWIa1izuxOLOZFEveP60xgldJnyu1z6+74y9rIfQvbKHTCbTOQNdyRh6UnGcdmaC5+NJ+Xq58OQ4l1wA4JxF6ZKEPuNzuWTyBl4+ORm4bK3I5A10SsVQQPGNhPfuiekUOSn6iR/uxGfv3x1p2QL/fWl+gJmcab+vSQGR76lzZDqH46N20BAeoZcmVkOyRgpCL/gJ3f67OxUritATMU30muFyEdA4uSwKob8AYAMRrSOiBIB3A7hfXoCIlpEzPxgRXe387ki9BxsGWeeKRujFBCOfDLZt0XaolHK5pOPFPmuX0F0N3bYt+tZnFEsuvGmWq/27tsVOn+wwW7BdHEFEyJEVETrEfnAj9HKSi3sz4ogyp6jQ0B0LKRCuU56anMWizgQWdSYCb0xCQ48guUxkC+jpiKG3I47+7iTOlSJ1Htn1dMQwmS14JCB+YS3pTonf4Vi5oAMAxDSBXg09XHLJ5E2kkzp6O1xCX9RpSzBpX7dFfv70pFxCX80JPdS2aO8LTqzffe4Y3n7XUx7CqBemcwbSSd1zgyyYzHNMC04hWVzTikwCYTg1MVs0qXIY7HbMGuIxdwyZvJ0UjYsIvVhy2enM1atrVBShi7kPomrojuQCuC4mjpxhj6Mr6S08yjsaOueCsYwUoUeYk6AalCV0xpgB4CMAHgawB8APGGO7iehOIrrTWexdAHYR0Q4AXwTwbtYo1T8Angg9VkzoFUsuhu0z1h0yDnO5dCRiRY+YXFN3CZ2CJRfDTfQALrHnTUuQv1tYZOvIdq8P+3ucyEsSesGbFPVEzWUiBIsxEOCZx9Ov/fqRNywR4cZjGhKx0tN2cUthV0gEzjX0zkiSiyFI8V9++wr85a2uEYvv496OeNFNkO+HpT3FETq/OJ85OFxUIMQDAE7ocqJrJmdHtT0dcUw6N6rFnNB9ETonlm5JcjlnUdqpfwje1hkRmdv/D4xlYVgMewanSu6japDJm3aE7stDyZGo6UgiMT1a/xnGGIZn8piKMBUi4PQGimkioADsCF1Oigb1DTrg5C8uWdGD0Yx3XfLsZKWoyquhOxG6fzJop0CwI6F7HE55g5+/9rGVz61mSi5gjD3IGDufMXYuY+z/Ou9tYoxtcl7fxRi7mDF2OWPsWsbYMw0ZbQhknYtHhfI0cn5CD3rM4gQb18nXbZHgP0d54ied0IuSG3m/5BJSnBSkofNx+Gcs4tY9wH3cm4lC6HkTHfGYJN2QZz2lwBhvW+C+JzT0gPOfMYYP3fMCfufrz9nr0jSx38MIfTJryySdyVioDz2yy2W2IGSL68/rw6vXLBSf8XOCE/6ktK6sL0LnbhvGGKZmDcR1++lqJm96CF3uYQ7YTzT8POS5C1lG6euybxgpn22R7xu/5BLTKLSMnkeIfN1D03b0uWewctnlnV95Gv+15Xjo5zM5w56UwzcdonwDtqUPQkzXIpX+TzmTPZTr8+P+vnvD4MjkDRQsKSkaoKGfnpxFR1zH2r7O0Aidj388k8euE8XTCHo19OBJLnKGncj2GxeEhu7cCBhzbcPNlFxaFi8cGcXJ8azH5ZJwHssKpttTJREiuTDG8IF/fx6P7T0j6WBxT6WoHlC2z4uHbEK3Qj8DwiUX4UPnpf98zsKCS+hucy5Netyzxyki9BIXRSZvoCOhiQidV28C0drncg89RymXy4+3ncCv9rteX95t0V5X8Mk7OWugpyOOrlSs6MbEe+LEJZfAnsFJ3PXo/kBt2b45uFGu7b+3X8sROl+Wg58LQkN3Ppst2HUMXHaZzBYwW7CKWrV6bGrOPp3JGehMxsT6AGBxlxShS7ZFQejO2DsTOhZ1JpxkfPC+zuS96+YVrpUSet6w7OpaybcftK7OpFdDl8cAwJE+NMQ1iuTe4PbQ6IRunwceDT1vCh86ECy5nJqcxdKeJBZ1JjA244/QvfmQf3vyEN7ztc1F6/b2cuERuoFDQ9P48mMHwJjdUC0Z05COB0suPEIH3OPcqH4ubU3of/CtrfjK4wdCfOjlJZfxTAGP7xvCMweHxcnQnYqVrxS1XCeLPwlUrKEHFxbxE0pE6LpLfq7kwonYjdB5ZBQpQi+YtqtCSq7aRRjlZxLilaKyhi7mFA0gGX/DJu5Dt7cp+OSdcki4K+El9CdeGcJnfrILAMQFYTHgi7/cj3/6n1fwjw97veSWxTCVMzxRLhGJZFRSaOj25xMeQjcR0wiLnH4vPHrncsAKTuizBcwapiBpWUPn+zfnyC42Cero7XAvZB6hp+MuOVoWE7/Dnx5WL0qDnKZwAIoCAb5OwJU9hnmEfqoyQuf7YSJbfA598kc78dBLg8jkeYTuJ3T3O3YzNidCj6Ch8/HmTStSpGpYtkYtdw3NOLZF/tQYZFs8M5nD0p4UFncmMJ0zipxs4rVhYXA8i6mcUTQeuR+6eErOm7j76cP4x4f3YTxTEG2wO3wROvfwy603+DmoInQfDNPC6Ewew1N54deVJRfZtijsf85Jye+O/MQamc57CD3nzFjE+7AUu1xkycWnoYvCItflElT6z9eXirsnJH+/SHLR3Ew5j9BlL3JQxGpZDLMFy/ahSy4XILwIw/N90W3RBd+PQdfsaSca4kjokuQScpFzmaQrFfNo5D/fdQrfee6YGLN8IQHApicOYv9pVy+ezhtgzJtYBNw2B3zcQRE6dwLxLpScyDmx8wh9atbArNQXhu+/bN7EwrT7Xt6pUE4nYp7xrFmcRlwn9HbEPX3leYTemYxBI1tuAdybeZCEkfElRXmE/srp6YoKeyayeed/b/RqWgz/tXUAj+49I2yLpSN0JiSRKD70YakiN0qUbjjE7S9u4nZGgLef9kkuU7NY2pPCok77vJSj9Jz8tGVYGHEStOMZ/77g3KKhQ0guBp4/PArA7nHP22D7m3e5koscoStCDwQ/Cccyecla5CZF86ZM6PZBX+gkpvza4/BMXmja3cm4qBQN67bIL5qOeCzAtug6YAAe5RbLNsK2KHVbBBzJRTzmuT70tC9Cn5YIMChK50mXjoQudHC3CCPYs8vBHKJJxLTgpKi/jYFhYSZv4sLlPZ5lk9LNNWgdPJHZmbQjdL7dsltDtn0NjGXEsdxzyiV0TtA9UkQMuK2ChcslFRSh21WOuma7FHjFaVGEni1gtmCK38gbluilzrs5zhZMcWPqTOgeyeW3Nq7G//zpG9CbjovchKyhJ2Ia+ruTeNUyuzCKPxkFSdI88VYwGaZzBqZmDZy3pAt5w8Ihn7fbshh+tHUg0H3iRuheEhudycO0GEZn8sgW7KSoVjJCt+yiPi1ihC65W6IkRvNcQ5fGwAOZWEiQwhjDqQkuucTFdnHkpMZvecMSMpB/X8jcwtswnxjLioKxkemckFz6u5M4NprBMadtct55v9MToTvN5hShe8EtQBPZgjSriBuhFwxL7DR+h+SP1Zzshp2DODqT83hJo/ZySSeKbYv+wqKwbotBtkX+flGlqC5F6D7JBQgmdB4pyI/LHr2xhIY3mTVgWAyLOpM+yYX3cvFuC78ILpIIPS5F6EFPAzya7emIoduJpDOSFCb/Do+0j49lceXqhSACDg25FZichMMi9IQ/Qp81MJEp4Lc2PYtXTk+Jp6SeVEw4EXjkuHKhJLkUTPHInDNMsY/lCJ0fl3QyJpaNaYRUXMe6PtdKGdMIP9o6gDu/vdUeY0zD/R+5AX/0a+eJz4HgCF1+rD8+apPHNesWAYAgE44XjoziY/+1A7/cc6bod/h+nvDZHXnEz6tk/ZWiQECErkd3uVQeodvBhSyd2pWilnC+JJ0JOPi1Opm1+4/LETondB6w8OrcvGmJJmpBTyuAzS38mn7iFbeYb2Qm7yRFddz5hnOR0DV8/Ic77HVw26KsofPzR2noXvAobiyT9+hcPArNm/Zdd0E6LkhzYSd/3LF3Jj9xvZJLXCRUuQ+9eE5RV3LxRyTFhUVwJJdgl4vftihLLjIRCw2dSy4yoQdcFPyiT0ml/3xdKWlC6iDwk3txZ8LrcgnRdfmxkMvubUdNuG2REyeP0OXtGJMIJu7YFvnvLF+QworeDhwensGxkQwmMgX3tzq8hN6VdPvkAPbNmsi+aHcPTuD5I6PYenRM9CHp6YiLaJ8TzSqP5OI2+soZltjHPELPGZYgus6EmxRNSZ0KOTQinJyYFaSZ0DUs7UmJZXlEHByhu8f7qEPgl67sBQAMTnorUwfG7N8/MlJclRkWofOuk7y1sZyH4eepXC1ZsOykaEzX8Ni+M3j7XU8VPQE+8coQrvnbX2BytuDpQBmJ0C0WGKHzGwkAj6NqdCaPA84N3yZ0J0J3ziu/VXS2YIoI3e/ll7klodsTaewYmBDXxch0zo7E4xpWLOjAH7/5PDx3eBQDY1nkDBMJXfN0qhSSSzNti60IHl2MZQpFk0QDNrEOT+fQ15UUJyMv7siJCN0hdEcHA7yPRPwkCqsGTcX1EpWiclI0XEN3XS7uCRnUnIvfIIKSodO54sdW9+nEdVV4HQHhJxSPZBZ1JjwuF10Pbs417hAC37+AN0IPJHQeVTsaurxN3gidPC6BhekE1vd3Yv/pabzzq0/jHx7e60ouKT+heyUXTcgqBU/vcx6hd6digmCCJRdLrCNXsMTTkojQC6YgW15YJP++DH/E60/c+xuhffe5Y9jkdIrM5Ezx1MEj9Fct74GuEU77Wg1wUj7qi9wBdz9PzhoeNw3fNzyPYEfo9vp4cleWXEzTlifjjjNn58CEqNDkeO7QCE5P5rB3cArDU3lxXsmSyzu/8jS+9eyRonHmDatYQ88ZoqAJkAMiEx/8jxdwx71bAMAbofNkrBS8ATYPcOIuFaGT9LR+yyXLne/mheQCAJeuXADAvoFy2VLXSJB6r0qKBoNHcXnDEnqy3JyrYDAMT+ewuDMhCG1B2o44RVJ0yj3APCnCD3I2b7oe8oBui7xvTFGjHr9tUSIRGbJ2CsiSi11YxCfXABxS44SeL47Qp2YNTM4W8H8eeLlo8gN/cy57XaWToiMSofNx2fvX1n79VjpODAvTLqHHJNti3iw+ed0IPSYiaUHoWUlD1zVBzHxM6/s68fLgJIan8zg6khHE49fQ+Y1Abv3a60Thw1JnRX6x9aTiRZLL4q4EUnENk7MGZg1TyA85wxQ3TU+EnnMjdE7+8j7k8GvSSR+h8895sPL9F47h3meOALDJlNsgjzmEvqwnhSXdSZzyRei8X86x0fAI3bSYJ0DguSUOOULnTcfkAhrbD+71ictNzQCIvi0HzkxjZCYnpCy+n2cLJrYdGxcTawP2zerdX3sW246NeVwuvD++x7bo3DRnCxYOnJ4S5/CynpRIRPPtkg0Q9ljdfVakoUuWaBl/fdvFWJiOY3SGE7p9jLmsdmQkIwgdgHjK7Fa2xWDIURwnZl2XXC6O5NLX7Ubo6bjusewNSycuj2S4TzRbMO1uiyGl//wE9tsWB8ayWOhUPwK25LK0J4mRmbwnMZUzTMSdiQwAb4Th9iK3l41pmtDvMiJCdy+omZyJJ/YN4ZtPHcYLR+wLgj/625Wi9nKeIowSJxSP0Bd3JbBEcq6E9aXhj6lyV0PZ5RK0LjeRGUdX0m0dO1swPSd73Gf7WtiZEP1aALsjYtkIXfc+8k7OFjzHXpZcJrIF/GT7CZyZmgWRTczdqTiGp3NgzF6WtwXOCg1dSoryCD1RWYTuJ3TdlxQ9PprByYlZZPO2du8n9MVdCSztSRU1AzvpkFVQhC6Tl/zaPxVfp1QMtSBtP7VlPRq6nRSVSe/EWDihD0/nsa7PPob8BsrXeXzMHudEpoD3//vz2HxotMiHvqQnCcNiyOZNj8sFsK9p+WazpMe+/lcu6MAx56nB7/0flG4+xRG621YEADb93lW4/yPXY3FXEou7khhx8m/8+C3pTiIV13Bk2InQnXOPP2Gnnb44KkL3QdZZ+cUpd2TLGxaGpnPo70qKiyOd0JGK62JnDk/nxV2f9+bulCq53ErR4mrQuGbPX8obLHEcHJrGuf1d4lFQI8LSHrsKUX7Ml+/qADzk53Y6dH3oiZhdFCRH6Pw707mCiIh432fep1ue4ELYFuPRJRc+dsB27NjNyrzL84ugNx0XN1S58+WOgXH8cs9pz3dEVJ2KC8li+/HxIttYPOaL0B3JhWNwYhZD0znENPKUzwPFkgtgR+gTPsmFH4eeVAwDY1n8yfe347vPHUNX0nZ39KRiYvlUXHdcQpY4FnJSlEsRcmFRkIYuu4eA4ohdTopOzRaECWD/mSkYFsNiR0Y4NprBwnQccV3Dsp5UcYTunBcnx7PIGxbu33ESn/7vlwB4yUve70URutRtsTOpO5Mlm9gzOIkL//LnGMsUENPJM+PTiXH3BmJZTGj4B4amMTyVw9rFtj1zctbAwaFpMW5+g/rm04dxeHhGSDwxTRNPAP3OexPZguepE4Bw+RDZx5rv+7V9nTjifJbzSS4nSxC6rKEDwE2XLMdlqxYAsHNMw9O2Q44/IWgaYe3iThwdmRFJUcB9Yk/ENHzqlgvxhvPr121WRtsS+ri04zmhy4VF3M61uDOBREyDRnYElorpHh/6uU60d3I8i2TMLYYpmEzqZR4Qocc0t4hJylwdGprGeUu6xMlHRMKffVq62OS7OuCXXJh4OgBcIrY7LnL/uSF+d2rWEBERj8j4TDry43IsQHIJqkQcmc6jKxlDMqZjmUToOoVLLrpG6E7G8OM/ug4fuG4tupIxcTJ/7/nj+N8/sDP//OJxo+oYzlvShbdevBT//Mgr+OVeL/HHdfL4eBd2xrFhSTc0sj3imbyJlwYm7HJ5X0Ww60N3ydJu0GX4InSuobsR/kzetSh2p+IigkzF7RtrzjCFfs0fs3OS/NeZcL3tQYTOteOPveV8fP6dl2K95IABvElRWY/edcIuHuIR+sBYRrQtWNab8mjofH/zHjYnxrP48bYBfPf5Y8jmTU8CUJYEh3wRelfS7eHeEY+hI2Gfhy8cGRWyU0wj7D/jOo9OjGXx7c1HcWwkg1OTs5gt2DLl5oMjmMoZuHhFDzoTOv7zhWO48QtPYMtR+8lyPFPAyHQO333uKN50wRL82gU28cV1Eo4W/tQ4kS14bIsAcHjIJu0/+/UL8ImbLhDjWbc4jSPDM8LhAkC4q046+yyd0IsjdNObz5LR15WUbIvuMV6zOI39Z6ZRMJkkubjBxYduWIdr1i8u+r16oH0JPeMmVniGWna58Dt+X3cSHQkd3/7QNfit16xGKq5h1rAr+oanc7jA8f0OTswi6bNGiaRogG3RXpcTRZmuf3p4Oo9z+7vEyaeR2yfktHSh5AqWZ10eycXytq7lj5p210G39J//7nTOKIrQOfEHa+j2U4ppMdzw94/htzY9i4OSDXB0JicSnMt6XUIP6z45lsmjt8P2V1+yshefffvFICJPy4WJbAGP7TuD6z7/KJ4+MOxxphAR/un2y9GR0PGNXx0G4Jbic+mG/9biziSW9aZw/0duwJ+99XwAtjVvnY8QAeA1axfhhvP6PH7wBR0JjMzkMTSVE/uXa+hy8hVw9c6ejriQDJZ0p0RV4sGhaaQTOtYs5oRu4sykLdV0p2zdtjsVC5RcuFNqbV8n3n31OUURO991JmMiagWAXScnALjJyYLJxLYv7UlhKmdgOme30/3vF09gJm/i2vW2pfHoyAz2Dk6BMeCV01OYyBZE07CJbAEnx7P4y/t24ejojKdIjMsE9j7S0ekU0OyXJu+IOb3vAdtx89zhUfzFfbvwhUf2icj46rWLkDctpOIabr50ObpTcZyetKWsJ/a5VsBNTxzE8HQeH7h+LS5dZbt3RqbzotsiP+8Ni2GJc57w1hmHh+0x3f7qVfjda9aI31zb14mpnIGRmbwgdH7D5UHGmsWdRU+I0851pFMxoS/uSki2RfcYr+3rFBIX7ykkInS9sZTbdoQ+NJXDD7Ycx/B0HqudqjpvhO5IKM5B4if+def1oScVF2Q2kS2gYDJhtcvkbY+xvMM1R3Ip6uXi0/S4Ns5J8dwlnSJykCUX+ZFUzowD/kpRb+taEaEndMmHbqInFUM6oWNaitC5dMRlJbnbYkKKZnKGhWOjGZwYz+L5I6P4xA93irGMzOQFofP9x7clyLEzLk3ELMN/8n7r2aMAgGcPjmAyayAhtQfoTsXxqmXdgjjPX9rlbLs3qcStp5es7MXqhWmxz4II/ep1i/Dt37/GE7mft6QLw9M5HBmewYYl9jp4BD3ikxoEoTu1CQBwwdJu4eM/NDSD9f2d4oaQK1h4cv8wLl+1QOQ8ep2nwjDwSNsP3gzLtCwMOLpyVzKG3U7f8z7pe+c527HcufmempjFZ3+6G//7BzsAAK91osEdxydEoLP31CTGswVxDU1kC/jR1gF8a/NRnJ7MeSyonYmYGE9HQkdHwu5Z8opUrRvXCK9Za5PXhqVd4jx8ePdpcRO68aKlAICbLl6GnlTck8R+8di4eP3tzcewamEHbjivDxev6BXj5dcbv9kD9nkAuDflvaemENfJc94CwFrnpnt0ZMbTtwmwr5kF6Tj6uhKeCN20GB7YMYhr1y8qksQAW5Icz9juJw+hO+ta19eJ153XJ/YhUJwrqTfajtA3HxrBJ364E9uPjYsdNywidE1Ehlx68F8wqbiG2YIlbgKrnGw7APzmlSs9UbOuIXCy3oKjjQlHjRNtHTxjk9G5/V2CiDSytTZdI4/kkvdr6M7yL5+cxM93DaKvOwm5qRZga5myht6ZjKEraVvtTkha6WzBFBp1WkqK+vte8AvyDef348VjY+IRfGQ6LyI3+VGTPzX4k6ITmQIWdBQTuv8i4AUZ24+PO2X/MU9ket4Sl0Q2OK9dQrcvCNlJIz89rAuYqSgIF62wi59m8iauOscmIB5Bv/+6tXjXq1fhL952IQD3guf/d8R1rFrYYfeuNy2RL+E348GJLHYOjOONF7j66NsuXY43XBCul/b7iIeD6/Inx2dxfDSD7mQMl6zswW6nIyDX0AE7gAAgAofdJydEchwALl+9AP3dSXz3+aPivT2DU5jMFrBmsUvomw+7UxhwogdsEhcReiImIvQDksSiaYRv//412PW5twrvfkLXkC2Y+MavDiMV13DLpcuwrCeF91231rNfATsvxbc5WzDxlouWgohEsdqk0/nSv8+4//7iFT2I64S9p6awvLej6Nxb69zwDw9nJNuifU6ZFsPizoSnDgEAfrnnNE6MZ/EBZ7x+LJbGkZRkNS6fvffaNWIcaV9NRKPQdoR+w3l9ILJPgP7uJDoTurBcyZozlx78F0zSSYo+sHMQgEscgH0APIROTlLUlwTkPSTkRmDjmTxeODKKhK5h1cK0OPnI+Y0l3Umv5GKYgggAm2wXpuO4f8dJZAomvvSeK8Vjt3sx6ZLLxZ4VpysVw8mJLKZzhrPds3jHl58WDaxSAZJLyul7wfuhfOiGdbAYxMzoo1KELoNXvcoe5NmCifFsXlj3SoFH9nbyM1/kSuERM+BGXvyis90mMc9kJUu6U0I2CYrQgyC3Jzh/aTdevWahWNfqRWn80+2X40qH6F3Jxf5/w9IuaE6ydzyTx4nxLNb3dYmb8SMvnwZjwBsvWCLW8albLsT7Xrs2dDyLQwj9qnMW2przoREcG81g9aI01vd3CflPDlR4Hojf4O599igYsyNzInu7brlkmTj/Vi3swJ7BSYxnClje24GYRhiaymHr0TER7fMWBFzyEk6xhN2EamAsi5GZvEhunhjLIhnT0ZWMCUvirZcvx9rFaYxl8vjjN23A8t4ObP7zN4sbqT+JvWFpt3jvLU4035HQ8eFfOxd3f2CjCEjkCJ0HdZ3JGF6z1paWVixIwQ8+rd+R4Zki2yJg3wx5wpzj3mePYnlvCjdeuDTwGPVJ14gcGL5m7SLc9TtX4veudSUfHqE3mtBj5RdpLSzsTOCylb3YMTCBhek4FqQTmMnb5C1K5WOa0MKKI3Qd+09PYefABN526XJctKIH73/tGhARlvSkhG0KsKOOGLMj0pcGJrDpiYP46I0bkHfmuuQnWM6wcPumZ3FiPItLV/ZC10h8xi8Em9DDJRcAePhPX499p6awYUk3lvWmRBk3lwTSiRieOXgaN37hCZyZyokIfZ/T1+TSlb3Ydmwce52/eS8aTbrRAW6Evv/MNFYu6MB15y5Gb0ccT+wbwtsuXV5E6Fw31zXCxjWL8MOtA3jN2kVgDPir+3cjWzDxzivdG2MQLlreg5cHJ3Heki4cODONF4+NY0mP98LjZJKMaXjHFSuwamGHiBQ7k3rRTSYR09DXlcTQVA7r+7oQBYs6E1jem8LgxCz6u5P40R9eV7QMz6u4kot94+EyRCKmYcfxcTBmR8eaU0V4ZCSDRc75GRVBTzaATVCXrerF5kMjmMgWcN6SLrzjipV48KVBjGcKWLUwLeZ75TbO5b12Z8GtR8ewZnEad3/gNdgxMI6+riRuvXwF7nn2KBZ3JvC6Df344dbjTh+aOHo74nhy/xBmCxY+/tYLsGZxGuf1d+HvH9oreiN1ObmAlQs6sGZxWrRKvuXS5fjK4wfxihStcynshvP68P/e9CoQUHSs7f1rb/uFy3uwZ3ASy3pSmJ41MDCWEeQMAB9/66sAANsdWUYmdDkSf8P5/Xjm4AhW9LrkyhHXNaxa2IF9p6fEDVwOKG67YgWOjmQw7sxodXBoBk8dGMbH33pBUbKd4+IVvejvTuKD16/F2y9f4RnTrZet8CwrIvQGa+iRCJ2IbgLwrwB0AN9gjH3e9zk5n98CIAPgA4yxbXUeq8DrNvRjx8AEFqQT4m576cpecXD5TksndI9Dwv6MRAL0kzfbJ8rnbrtE+txbSMOYHbG+4ytPw7TsWWtmHQ85J0j+aPbJm1+Fd161EoCtKQKul3xJj03QB85MYX1fF3KG5SkJBuyIkyd8AOCcxWl88T1XimiF68j8UZcT+hnHUveatYuwTdIiRZUbeSN03vdi7+AUNiy1LZY3bOjDo3vPYHg6j7xpechzSXcSgxN2su9L77kSd3xrCz7+w52I627/694ADV3G1esW4eXBSbz/urX4y/t2YXBiVhA4xwZHN1+YTiCma7hWcgIs7UkJXVrG8l6bBOQkXjlctLxHEHoQupIxfPTGDbjuXFv/5H7lCxxCT0sz0/DomGvst162PFBvDUOpZV+7fjG++sRBMAbcvnE1rl63CM//+Y04MZ7Fuj5bu+9OuTUPqbiOn3zkenzl8YO4Zt0idCR0sQ9ffc5CrOhN4dwlXbh0ZS++9/wxAK6175XT0yCye8Lwpy35qasnFcezn3wzejviuHrdIjy8+zSGpnK4feNqPLl/CJ9yriUAuGb9YvzDuy7DrZetKBmRdqfsDpNvu3QZ9gxOYmlPEq/b0OdM6Vb8vYtW9ODyVb3i3Oz2JbHfeMES/N1De7E8IEIHgBsvXIq7nz4somnZfXTbFStxzzNHYFoMx0ez2PTEQSR0Db/9mtWh4z9ncRovfPrG0M9ltEyETkQ6gC8DeAvsCaNfIKL7GWMvS4vdDGCD8+8aAF91/m8IXrehD3c9dgAL0wmRPf6Xd18hPl/f34kzU7mixAjgVoX9zTsu8eiEHKsWdngevbi88GsXLMG5Szrxb08cAgB88Pq14qT77vPHsLgzgQ/dsM5t1Sv50AG7uOiRl0/jxi88ibdctBQDYxmR8CkF+c7v75iXjGm48pwFeOagrX3yjPr6vk5P172YU8DE9xV/Mth3egqvP98mrfe/di1+tnMQf3Gf7VEOIvTZgoWOhI5vvv81+NxPd+PZgyPYsLQLD+8+jQD3owev29CHR14+jXdcsQKbD44gZ1h4/3VrPMss60mhKxkLTLD+3TsvDWwTfOGyHqQTepFLpBQuWtGDX+49E3h+cHz0xvPFa94j5nwncr/zDedi14lJTM4WiqSeUgRQKa5dvxhfefwgLljajQ9evxaATQh8nR2JmNDPOVYtTONvf/PSot/SNMI9/+tqpOI6+ruTeGzfGTzy8mmsWZQW0t8fvuFcD4kvSMc9bRt4t9LFXUl89XevwgM7B7F2cRoP/PHrPOvSNcJvbSy/H25/9Sqc298lNOelPSncXuJ7N12yHDddshymxXD7q1fhg9ev83x+/tIufOKmC3DTxcsCv//RGzfggZ0nxRyxPR0xrOhN4c0XLkUq7haCvf4fHwMA/PbG1SXPkUog+9AbiSgR+tUADjDGDgEAEX0fwG0AZEK/DcC9zjyim4loAREtZ4wN1n3EsCPRz739Ytx8yTJcvW4RsnlTREoAcO//ugb3vXgikBj+6jcuwt7BydATZ2FnAo/86evx7c1Hcctly8GY/aj9wevXIZM38KOtA7hgWTc+dfOF2Hp0DABwaGjGQ+aAHX28alm3iEJ5NeRFy3vwyz2n0ZWM4dbLlle03bzR0h+8fj3+7clDyOQNfOwtF2BRZxJ7BydFwcOvX7wMP942ICL3ZMy2bfKEoCz1bHCizqvXLcJbLlqKh3efxjmL0p5E3iWOxCVPGPJ/HdJ4aWACD+8+7akolfEHb1iPpd32RfNmR4v88u9eFbgsT4Lxx1MZYRr9X7/j4tC5N8Pw9stXYGAsKxKC5XDt+sV455UrsdG5YW5cuwi//NgbMDqTL/KYR7lJR8XV6xbh9levwodety6wfcCNFy4Rtr4o2CA5V77+vo04NTGLZb0pfPl3rkImb+DVaxZ5ll/akwqd+m/j2kXYuHZR4GdRceU5C3HlOQtxZmoW6YQupJBy0DXCP95+edH7RIQ/euN5od/rTsXxpfdchUf3nsFbL16KJd0pPP3JN4nPL17RiwXpOH7jshW48aKluO7c+nnFeVDQmWiwys0YK/kP9gTQ35D+fi+Au3zLPADgBunvXwLYGPBbdwDYAmDLOeecw9oRE9k8M01L/L3t6Cj7+pMH2fDUbMnvHTwzxf7+oT1stmCwI8PTbCZXqHjdu06Ms02PH2CmabHvPXeUjU7nipb5yfYTbGwmxzI5g41n8oG/c3R4hv3p919kf/qfL7IhadwnxzPsnx7ey8ZmvL87WzDYfS8OMMuy/D/FGGNs/+kpliuYFW9PEE6OZ9ipiWxdfmsu8eje02zXifHIyx8dnmEnxjINHFHtGBjLsGMjM3OyrrBza75gJldgD+8arMtvAdjCQviaWJlnZSK6HcBbGWO/7/z9XgBXM8b+WFrmZwD+jjH2lPP3LwF8gjG2Nex3N27cyLZs2VL5HUhBQUHhLAYRbWWMbQz6LIqgMwBA1idWAThZxTIKCgoKCg1EFEJ/AcAGIlpHRAkA7wZwv2+Z+wG8j2xcC2CCNUg/V1BQUFAIRlmFnjFmENFHADwM27Z4N2NsNxHd6Xy+CcCDsC2LB2DbFj/YuCErKCgoKAQhUsqVMfYgbNKW39skvWYAPlzfoSkoKCgoVIK2K/1XUFBQUAiGInQFBQWFeQJF6AoKCgrzBIrQFRQUFOYJyhYWNWzFREMAjpZdMBh9AIbrOJx2wNm2zWfb9gJn3zar7a0OaxhjgU32m0botYCItoRVSs1XnG3bfLZtL3D2bbPa3vpDSS4KCgoK8wSK0BUUFBTmCdqV0L/W7AE0AWfbNp9t2wucfdustrfOaEsNXUFBQUGhGO0aoSsoKCgo+KAIXUFBQWGeoO0InYhuIqJ9RHSAiD7Z7PE0AkR0hIheIqLtRLTFeW8RET1CRPud/xc2e5y1gIjuJqIzRLRLei90G4noU84x30dEb23OqKtHyPZ+lohOOMd5OxHdIn3W7tu7mogeI6I9RLSbiP7EeX8+H+OwbZ674xw2lVEr/oPdvvcggPUAEgB2ALio2eNqwHYeAdDne+8fAHzSef1JAH/f7HHWuI2vB3AVgF3lthHARc6xTgJY55wDerO3oQ7b+1kAfxaw7HzY3uUArnJedwN4xdmu+XyMw7Z5zo5zu0XoYsJqxlgeAJ+w+mzAbQDucV7fA+AdzRtK7WCMPQlg1Pd22DbeBuD7jLEcY+ww7L77V8/FOOuFkO0Nw3zY3kHG2Dbn9RSAPQBWYn4f47BtDkPdt7ndCH0lgOPS3wMovcPaFQzA/xDRViK6w3lvKXNmgXL+X9K00TUOYds4n4/7R4hopyPJcPlhXm0vEa0FcCWA53CWHGPfNgNzdJzbjdAp4L356Lu8njF2FYCbAXyYiF7f7AE1GfP1uH8VwLkArgAwCOD/c96fN9tLRF0AfgTgo4yxyVKLBrw3X7Z5zo5zuxH6WTEZNWPspPP/GQD/Dfsx7DQRLQcA5/8zzRthwxC2jfPyuDPGTjPGTMaYBeDrcB+358X2ElEcNrF9hzH2Y+fteX2Mg7Z5Lo9zuxF6lAmr2xpE1ElE3fw1gF8HsAv2dr7fWez9AH7SnBE2FGHbeD+AdxNRkojWAdgA4PkmjK+u4MTm4DdhH2dgHmwvERGAbwLYwxj7gvTRvD3GYds8p8e52ZnhKjLJt8DOHh8E8Olmj6cB27ceduZ7B4DdfBsBLAbwSwD7nf8XNXusNW7n92A/fhZgRyofKrWNAD7tHPN9AG5u9vjrtL3fAvASgJ3Oxb18Hm3vDbDlg50Atjv/bpnnxzhsm+fsOKvSfwUFBYV5gnaTXBQUFBQUQqAIXUFBQWGeQBG6goKCwjyBInQFBQWFeQJF6AoKCgrzBIrQFRQUFOYJFKErKCgozBP8/+IXcEmm8OBNAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "D.plot_loss()\n",
    "G.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08d0f6b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(G(generate_random([100])).permute(0,2,3,1).cpu().detach().numpy()[0,:])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

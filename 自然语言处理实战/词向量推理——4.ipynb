{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cook', 0.6973531246185303),\n",
       " ('sweet_potatoes', 0.6600280404090881),\n",
       " ('vegetables', 0.6513738632202148),\n",
       " ('onions', 0.6512383818626404),\n",
       " ('baking', 0.6481685042381287)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[('AP_TECHNOLOGY_NEWS', 0.15106438100337982), ('ATLAS', 0.14635424315929413)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\24132\\AppData\\Roaming\\Python\\Python38\\site-packages\\gensim\\models\\keyedvectors.py:877: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'computer'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7118192315101624), ('monarch', 0.6189672946929932)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.70705307"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "word_vectors = KeyedVectors.load_word2vec_format(r'D:\\迅雷下载\\GoogleNews-vectors-negative300.bin.gz',binary=True,limit=200000)\n",
    "word_vectors.most_similar(positive=['cooking','potatoes'],topn=5)     #cooking和potatoes两个词向量相加\n",
    "word_vectors.most_similar(negative=['cooking','potatoes'],topn=2)     #cooking和potatoes两个词向量相减\n",
    "word_vectors.doesnt_match('potatoes milk cake computer'.split())      #检测不相关的词向量\n",
    "word_vectors.most_similar(positive=['king','woman'],negative=['man'],topn=2)   #king+woman-man\n",
    "word_vectors.similarity('princess','queen')     #计算两个词项之间的相似度\n",
    "# word_vectors['phone']        #查看phone的向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练word2vec模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mul\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "#将文章分隔成转换为词向量需要的数据格式\n",
    "file = r'C:\\Users\\24132\\Desktop\\1.txt'\n",
    "with open(file,'r',encoding='utf-8')as f:\n",
    "    a = ''.join(f.read().split('\\n'))\n",
    "    b = ''.join(a.split('\\u3000\\u3000')).split('.')\n",
    "token_list = []\n",
    "from nltk.tokenize import TreebankWordTokenizer    #内置多种英语常见的分词规则\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "for i in b:\n",
    "    token_list.append(tokenizer.tokenize(i))\n",
    "\n",
    "#训练Word2vec模型\n",
    "num_features = 300\n",
    "min_word_count = 1\n",
    "num_workers = mul.cpu_count()   #动态设置cpu核数\n",
    "window_size = 6\n",
    "subsampling = 1e-3\n",
    "\n",
    "model = Word2Vec(\n",
    "    token_list,\n",
    "    workers = num_workers,\n",
    "    size = num_features,\n",
    "    min_count = min_word_count,\n",
    "    window = window_size,\n",
    "    sample = subsampling\n",
    ")\n",
    "\n",
    "# model.init_sims(replace=True)    #模型训练完毕时使用，存储隐藏层的权重并丢弃用于预测共现词的输出权重\n",
    "\n",
    "model_name = r'D:\\深度学习模型\\word2vec_model_mine_1'\n",
    "model.save(model_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-47-5ddf45db7c4d>:4: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  model.most_similar('forever')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('of', 0.17407584190368652),\n",
       " ('an', 0.17077642679214478),\n",
       " ('neither', 0.1651541292667389),\n",
       " ('never', 0.16309814155101776),\n",
       " ('Then', 0.1622140109539032),\n",
       " ('discipline', 0.14105772972106934),\n",
       " ('arise', 0.1328985095024109),\n",
       " ('act', 0.12999625504016876),\n",
       " ('oath', 0.12474499642848969),\n",
       " ('difference', 0.12166089564561844)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "model_name = r'D:\\深度学习模型\\word2vec_model_mine_1'\n",
    "model = Word2Vec.load(model_name)\n",
    "model.most_similar('forever')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 词关系可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "distinctiveness    Vocab(count:200000, index:100000)\n",
       "Namco_Bandai       Vocab(count:199999, index:100001)\n",
       "ramparts           Vocab(count:199998, index:100002)\n",
       "Linden_Lab         Vocab(count:199997, index:100003)\n",
       "Revolutions        Vocab(count:199996, index:100004)\n",
       "Henderson_Nev.     Vocab(count:199995, index:100005)\n",
       "dtype: object"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "wv = KeyedVectors.load_word2vec_format(r'D:\\迅雷下载\\GoogleNews-vectors-negative300.bin.gz',binary=True,limit=300000)\n",
    "len(wv.vocab)\n",
    "vocab = pd.Series(wv.vocab)\n",
    "vocab.iloc[100000:100006]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, numpy.ndarray)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "5.167152"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.9201079830527306"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "len(wv['ramparts']),type(wv['ramparts'])    #词ramparts的向量\n",
    "np.linalg.norm(wv['ramparts']-wv['Revolutions'])    #向量模长——欧基米德距离\n",
    "\n",
    "cos_similarity = np.dot(wv['ramparts'],wv['Revolutions'])/np.linalg.norm(wv['ramparts'])/np.linalg.norm(wv['Revolutions'])\n",
    "1-cos_similarity      #余弦距离"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nlpia.futil:Reading CSV with `read_csv(*('C:\\\\Users\\\\24132\\\\AppData\\\\Roaming\\\\Python\\\\Python38\\\\site-packages\\\\nlpia\\\\data\\\\cities_us.csv.gz',), **{'nrows': None, 'low_memory': False})`...\n"
     ]
    }
   ],
   "source": [
    "#美国城市的数据\n",
    "from nlpia.data.loaders import get_data\n",
    "cities = get_data('cities_us')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 利用Doc2vec计算文档相似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\24132\\AppData\\Roaming\\Python\\Python38\\site-packages\\gensim\\models\\doc2vec.py:315: UserWarning: The parameter `iter` is deprecated, will be removed in 4.0.0, use `epochs` instead.\n",
      "  warnings.warn(\"The parameter `iter` is deprecated, will be removed in 4.0.0, use `epochs` instead.\")\n",
      "C:\\Users\\24132\\AppData\\Roaming\\Python\\Python38\\site-packages\\gensim\\models\\doc2vec.py:319: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n",
      "INFO:gensim.models.doc2vec:collecting all words and their counts\n",
      "INFO:gensim.models.doc2vec:PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "INFO:gensim.models.doc2vec:collected 488 word types and 2 unique tags from a corpus of 2 examples and 2696 words\n",
      "INFO:gensim.models.word2vec:Loading a fresh vocabulary\n",
      "INFO:gensim.models.word2vec:effective_min_count=2 retains 488 unique words (100% of original 488, drops 0)\n",
      "INFO:gensim.models.word2vec:effective_min_count=2 leaves 2696 word corpus (100% of original 2696, drops 0)\n",
      "INFO:gensim.models.word2vec:deleting the raw counts dictionary of 488 items\n",
      "INFO:gensim.models.word2vec:sample=0.001 downsamples 70 most-common words\n",
      "INFO:gensim.models.word2vec:downsampling leaves estimated 1772 word corpus (65.7% of prior 2696)\n",
      "INFO:gensim.models.base_any2vec:estimated required memory for 488 words and 100 dimensions: 635200 bytes\n",
      "INFO:gensim.models.word2vec:resetting layer weights\n",
      "<ipython-input-36-b0a73cca55aa>:28: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  model.train(train_corpus,total_examples=model.corpus_count,epochs=model.iter)\n",
      "INFO:gensim.models.base_any2vec:training model with 12 workers on 488 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 11 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 10 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 9 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 8 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 7 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 6 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.base_any2vec:EPOCH - 1 : training on 2696 raw words (1766 effective words) took 0.0s, 216098 effective words/s\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 11 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 10 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 9 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 8 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 7 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 6 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.base_any2vec:EPOCH - 2 : training on 2696 raw words (1763 effective words) took 0.0s, 216340 effective words/s\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 11 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 10 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 9 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 8 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 7 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 6 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.base_any2vec:EPOCH - 3 : training on 2696 raw words (1765 effective words) took 0.0s, 185061 effective words/s\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 11 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 10 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 9 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 8 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 7 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 6 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.base_any2vec:EPOCH - 4 : training on 2696 raw words (1780 effective words) took 0.0s, 177824 effective words/s\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 11 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 10 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 9 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 8 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 7 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 6 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.base_any2vec:EPOCH - 5 : training on 2696 raw words (1774 effective words) took 0.0s, 239422 effective words/s\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 11 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 10 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 9 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 8 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 7 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 6 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.base_any2vec:EPOCH - 6 : training on 2696 raw words (1806 effective words) took 0.0s, 203470 effective words/s\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 11 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 10 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 9 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 8 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 7 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 6 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.base_any2vec:EPOCH - 7 : training on 2696 raw words (1764 effective words) took 0.0s, 221753 effective words/s\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 11 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 10 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 9 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 8 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 7 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 6 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.base_any2vec:EPOCH - 8 : training on 2696 raw words (1776 effective words) took 0.0s, 252970 effective words/s\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 11 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 10 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 9 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 8 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 7 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 6 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.base_any2vec:EPOCH - 9 : training on 2696 raw words (1790 effective words) took 0.0s, 219918 effective words/s\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 11 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 10 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 9 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 8 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 7 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 6 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.base_any2vec:EPOCH - 10 : training on 2696 raw words (1804 effective words) took 0.0s, 203835 effective words/s\n",
      "INFO:gensim.models.base_any2vec:training on a 26960 raw words (17788 effective words) took 0.1s, 123869 effective words/s\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "from gensim.models.doc2vec import TaggedDocument,Doc2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "file = r'C:\\Users\\24132\\Desktop\\1.txt'\n",
    "# with open(file,'r',encoding='utf-8')as f:\n",
    "#     a = ''.join(f.read().split('\\n'))\n",
    "#     b = ''.join(a.split('\\u3000\\u3000')).split('.')\n",
    "with open(file,'r',encoding='utf-8')as f:\n",
    "    a = f.read()\n",
    "with open(file,'r',encoding='utf-8')as f:\n",
    "    b = f.read()\n",
    "with open(file,'r',encoding='utf-8')as f:\n",
    "    c = f.read()\n",
    "    \n",
    "corpus = [a,b]\n",
    "\n",
    "train_corpus = []\n",
    "\n",
    "for i,text in enumerate(corpus):\n",
    "    tagged_doc = TaggedDocument(simple_preprocess(text),[i])\n",
    "    train_corpus.append(tagged_doc)\n",
    "\n",
    "model = Doc2Vec(size=100,min_count=2,workers=num_cores,iter=10)    #滑动窗口大小为10\n",
    "# train_corpus\n",
    "model.build_vocab(train_corpus)\n",
    "model.train(train_corpus,total_examples=model.corpus_count,epochs=model.iter)\n",
    "\n",
    "\n",
    "#将从未见过的文档来更新向量\n",
    "vec_3 = model.infer_vector(simple_preprocess(c),steps=10)  #迭代10步来更新\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'下面分别事原本两篇文章的距离和原文章分别与新增文章的距离：'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.5909461"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2.8172793"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2.2278397"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "def l2(a,b):\n",
    "    return np.linalg.norm(a-b)\n",
    "def cos(a,b):\n",
    "    return 1-a.dot(b)/np.linalg.norm(a)/np.linalg.norm(b)\n",
    "'下面分别事原本两篇文章的距离和原文章分别与新增文章的距离：'\n",
    "l2(model[0],model[1])\n",
    "l2(model[0],vec_3)\n",
    "l2(model[1],vec_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
